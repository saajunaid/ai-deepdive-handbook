<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI DeepDive Handbook - Complete Technical Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            color: #1a1a2e;
            background: linear-gradient(135deg, #0f2027 0%, #203a43 50%, #2c5364 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: rgba(255, 255, 255, 0.98);
            backdrop-filter: blur(20px);
            padding: 40px;
            border-radius: 16px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.15);
            margin-bottom: 30px;
            text-align: center;
            animation: slideDown 0.6s ease-out;
        }

        @keyframes slideDown {
            from {
                opacity: 0;
                transform: translateY(-30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h1 {
            font-size: 2.8em;
            background: linear-gradient(135deg, #1e3a8a 0%, #3b82f6 50%, #06b6d4 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 12px;
            font-weight: 700;
            animation: fadeIn 0.8s ease-out;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .subtitle {
            color: #4a5568;
            font-size: 1.2em;
            margin-bottom: 8px;
            font-weight: 500;
        }

        .nav-pills {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
            justify-content: center;
            margin-top: 24px;
        }

        .nav-pill {
            padding: 12px 24px;
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
            border: none;
            border-radius: 10px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 600;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3);
        }

        .nav-pill:hover {
            transform: translateY(-2px) scale(1.05);
            box-shadow: 0 8px 20px rgba(30, 64, 175, 0.5);
        }

        .nav-pill.active {
            background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
        }

        .content-wrapper {
            display: grid;
            grid-template-columns: 280px 1fr;
            gap: 24px;
            animation: fadeIn 0.8s ease-out 0.2s both;
        }

        .sidebar {
            background: rgba(255, 255, 255, 0.98);
            backdrop-filter: blur(20px);
            padding: 24px;
            border-radius: 16px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.12);
            height: fit-content;
            position: sticky;
            top: 20px;
        }

        .sidebar h3 {
            color: #2d3748;
            margin-bottom: 20px;
            font-size: 1.1em;
            font-weight: 700;
        }

        .sidebar-link {
            display: block;
            padding: 12px 16px;
            color: #4a5568;
            text-decoration: none;
            border-radius: 8px;
            margin-bottom: 6px;
            transition: all 0.25s ease;
            font-size: 14px;
            font-weight: 500;
            border-left: 3px solid transparent;
            cursor: pointer;
        }

        .sidebar-link:hover {
            background: linear-gradient(135deg, #edf2f7 0%, #e2e8f0 100%);
            color: #2d3748;
            border-left-color: #2563eb;
            transform: translateX(4px);
        }

        .sidebar-link.active {
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
        }

        .content {
            background: rgba(255, 255, 255, 0.98);
            backdrop-filter: blur(20px);
            padding: 48px;
            border-radius: 16px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.12);
        }

        .section {
            display: none;
            animation: fadeInUp 0.5s ease-out;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .section.active {
            display: block;
        }

        h2 {
            color: #1a202c;
            font-size: 2.2em;
            margin-bottom: 24px;
            border-bottom: 3px solid #2563eb;
            padding-bottom: 12px;
            font-weight: 700;
        }

        h3 {
            color: #2d3748;
            font-size: 1.6em;
            margin-top: 36px;
            margin-bottom: 16px;
            font-weight: 700;
        }

        h4 {
            color: #4a5568;
            font-size: 1.25em;
            margin-top: 24px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .must-know {
            background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
            border-left: 5px solid #f59e0b;
            padding: 20px;
            border-radius: 12px;
            margin: 24px 0;
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.15);
            animation: pulse 2s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.01); }
        }

        .must-know::before {
            content: "â­ MUST KNOW";
            display: block;
            font-weight: 700;
            color: #92400e;
            margin-bottom: 12px;
            font-size: 1.05em;
        }

        .concept-card {
            background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%);
            padding: 28px;
            border-radius: 12px;
            margin: 24px 0;
            border-left: 4px solid #2563eb;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06);
            transition: all 0.3s ease;
        }

        .concept-card:hover {
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .mental-model {
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #6366f1;
        }

        .mental-model::before {
            content: "ğŸ’¡ Mental Model";
            display: block;
            font-weight: 700;
            color: #4338ca;
            margin-bottom: 12px;
        }

        .tools-box {
            background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #3b82f6;
        }

        .tools-box::before {
            content: "ğŸ› ï¸ Tools & Technologies";
            display: block;
            font-weight: 700;
            color: #1e40af;
            margin-bottom: 12px;
        }

        .example-box {
            background: linear-gradient(135deg, #d1fae5 0%, #a7f3d0 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #10b981;
        }

        .example-box::before {
            content: "ğŸ“ Example";
            display: block;
            font-weight: 700;
            color: #065f46;
            margin-bottom: 12px;
        }

        .faq-box {
            background: linear-gradient(135deg, #fce7f3 0%, #fbcfe8 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #ec4899;
        }

        .faq-box::before {
            content: "â“ FAQ";
            display: block;
            font-weight: 700;
            color: #9f1239;
            margin-bottom: 12px;
        }

        code {
            background: #2d3748;
            color: #e2e8f0;
            padding: 3px 8px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 24px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 20px 0;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }

        pre code {
            background: none;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
        }

        th {
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
            padding: 16px;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 14px 16px;
            border-bottom: 1px solid #e2e8f0;
        }

        tr:hover {
            background: #f7fafc;
        }

        ul, ol {
            margin-left: 24px;
            margin-top: 12px;
            margin-bottom: 12px;
        }

        li {
            margin-bottom: 10px;
            color: #4a5568;
        }

        .diagram {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border: 2px solid #e2e8f0;
            font-family: monospace;
            overflow-x: auto;
        }

        .animated-diagram {
            position: relative;
            min-height: 200px;
        }

        .back-to-top {
            position: fixed;
            bottom: 32px;
            right: 32px;
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
            width: 56px;
            height: 56px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 6px 20px rgba(30, 64, 175, 0.4);
            transition: all 0.3s ease;
            opacity: 0;
            pointer-events: none;
            font-size: 24px;
            z-index: 1000;
        }

        .back-to-top.visible {
            opacity: 1;
            pointer-events: auto;
        }

        .back-to-top:hover {
            transform: translateY(-5px) scale(1.1);
            box-shadow: 0 10px 28px rgba(30, 64, 175, 0.6);
        }

        .progress-bar {
            position: fixed;
            top: 0;
            left: 0;
            height: 4px;
            background: linear-gradient(90deg, #1e40af 0%, #3b82f6 50%, #06b6d4 100%);
            z-index: 1000;
            transition: width 0.3s ease;
        }

        @media (max-width: 768px) {
            .content-wrapper {
                grid-template-columns: 1fr;
            }
            .sidebar {
                position: static;
            }
            h1 {
                font-size: 2em;
            }
            .content {
                padding: 24px;
            }
        }

        .collapsible {
            cursor: pointer;
            user-select: none;
        }

        .collapsible::before {
            content: "â–¶ ";
            transition: transform 0.3s ease;
            display: inline-block;
        }

        .collapsible.expanded::before {
            transform: rotate(90deg);
        }

        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }

        .collapsible-content.expanded {
            max-height: 5000px;
        }

        /* Tooltip Styles */
        .tooltip {
            position: relative;
            cursor: help;
            border-bottom: 1px dotted #2563eb;
            color: #1e40af;
            font-weight: 500;
        }

        .tooltip .tooltiptext {
            visibility: hidden;
            width: max-content;
            max-width: 300px;
            background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
            color: #f8fafc;
            text-align: center;
            border-radius: 8px;
            padding: 8px 12px;
            position: absolute;
            z-index: 1000;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            opacity: 0;
            transition: opacity 0.3s, visibility 0.3s;
            font-size: 0.85em;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
            white-space: nowrap;
        }

        .tooltip .tooltiptext::after {
            content: "";
            position: absolute;
            top: 100%;
            left: 50%;
            margin-left: -5px;
            border-width: 5px;
            border-style: solid;
            border-color: #0f172a transparent transparent transparent;
        }

        .tooltip:hover .tooltiptext {
            visibility: visible;
            opacity: 1;
        }

        /* Animation Styles */
        @keyframes pulse {
            0%, 100% { opacity: 0.3; }
            50% { opacity: 1; }
        }

        @keyframes flowData {
            0% { transform: translateX(-100%); opacity: 0; }
            50% { opacity: 1; }
            100% { transform: translateX(100%); opacity: 0; }
        }

        @keyframes processingCore {
            0%, 100% { background-color: #1e40af; }
            50% { background-color: #06b6d4; }
        }

        .gpu-core-animated {
            display: inline-block;
            width: 10px;
            height: 10px;
            background: #1e40af;
            margin: 2px;
            animation: processingCore 2s ease-in-out infinite;
        }

        .cpu-core-animated {
            display: inline-block;
            padding: 8px 12px;
            background: linear-gradient(135deg, #1e40af, #3b82f6);
            border-radius: 4px;
            animation: pulse 3s ease-in-out infinite;
            margin: 4px;
        }

        .data-flow {
            position: relative;
            overflow: hidden;
            padding: 20px;
            background: linear-gradient(90deg, transparent 0%, rgba(30, 64, 175, 0.1) 50%, transparent 100%);
        }

        .data-flow::before {
            content: "â†’ DATA â†’";
            position: absolute;
            left: 0;
            top: 50%;
            transform: translateY(-50%);
            animation: flowData 3s linear infinite;
            color: #06b6d4;
            font-weight: bold;
        }

        .attention-visual {
            position: relative;
            display: inline-block;
            padding: 10px 20px;
            border: 2px solid #1e40af;
            border-radius: 8px;
            background: linear-gradient(135deg, rgba(30, 64, 175, 0.1), rgba(59, 130, 246, 0.1));
            animation: pulse 2s ease-in-out infinite;
        }

        /* Interactive GPU Cores Visualization */
        .gpu-cores-container {
            display: grid;
            grid-template-columns: repeat(16, 1fr);
            gap: 3px;
            max-width: 400px;
            margin: 20px auto;
            padding: 10px;
            background: rgba(15, 32, 39, 0.3);
            border-radius: 8px;
        }

        .gpu-core {
            width: 100%;
            aspect-ratio: 1;
            background: #1e40af;
            border-radius: 2px;
            transition: all 0.3s ease;
        }

        .gpu-core.active {
            background: #06b6d4;
            box-shadow: 0 0 8px #06b6d4;
        }

        .gpu-cores-container:hover .gpu-core {
            animation: processingCore 1.5s ease-in-out infinite;
        }

        .gpu-cores-container:hover .gpu-core:nth-child(even) {
            animation-delay: 0.1s;
        }

        .gpu-cores-container:hover .gpu-core:nth-child(3n) {
            animation-delay: 0.2s;
        }

        .transformer-flow {
            display: flex;
            align-items: center;
            justify-content: space-around;
            padding: 20px;
            position: relative;
        }

        .transformer-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 15px;
            background: linear-gradient(135deg, rgba(30, 64, 175, 0.1), rgba(59, 130, 246, 0.1));
            border-radius: 8px;
            border: 2px solid #1e40af;
            min-width: 120px;
            transition: all 0.3s ease;
        }

        .transformer-step:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 16px rgba(30, 64, 175, 0.3);
            border-color: #06b6d4;
        }

        .transformer-arrow {
            font-size: 24px;
            color: #1e40af;
            animation: pulse 2s ease-in-out infinite;
        }

        /* Animation Styles for All Visualizations */
        .matrix-cell {
            background: white;
            border: 2px solid #1e40af;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            color: #1e40af;
            height: 50px;
            transition: all 0.3s ease;
        }

        .matrix-cell.highlight {
            background: linear-gradient(135deg, #fde047, #fbbf24);
            border-color: #f59e0b;
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.4);
        }

        .matrix-cell.result-cell {
            background: rgba(6, 182, 212, 0.1);
            border-color: #06b6d4;
            color: #06b6d4;
        }

        .matrix-cell.result-cell.computed {
            background: linear-gradient(135deg, #06b6d4, #0891b2);
            color: white;
            animation: resultPop 0.5s ease;
        }

        @keyframes resultPop {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.2); }
        }

        /* Embedding Space Styles */
        #embeddingCanvas {
            border: 2px solid #1e40af;
            border-radius: 8px;
            background: white;
            cursor: grab;
        }

        #embeddingCanvas:active {
            cursor: grabbing;
        }

        /* Attention Heatmap Styles */
        .attention-grid {
            display: grid;
            gap: 3px;
            padding: 10px;
            background: rgba(30, 64, 175, 0.05);
            border-radius: 8px;
        }

        .attention-cell {
            aspect-ratio: 1;
            border-radius: 3px;
            transition: all 0.5s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7em;
            font-weight: 600;
            color: white;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
        }

        /* LoRA Injection Styles */
        .lora-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 20px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .model-box {
            background: linear-gradient(135deg, #e2e8f0, #cbd5e1);
            border: 3px solid #64748b;
            border-radius: 12px;
            padding: 30px;
            font-weight: 600;
            color: #1e293b;
            position: relative;
            transition: all 0.5s ease;
        }

        .lora-adapter {
            background: linear-gradient(135deg, #fbbf24, #f59e0b);
            border: 3px solid #d97706;
            border-radius: 8px;
            padding: 15px 20px;
            font-weight: 600;
            color: white;
            opacity: 0;
            transform: scale(0.5);
            transition: all 0.5s ease;
        }

        .lora-adapter.active {
            opacity: 1;
            transform: scale(1);
        }

        /* RAG Pipeline Styles */
        .rag-pipeline {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .rag-step {
            background: white;
            border: 2px solid #1e40af;
            border-radius: 8px;
            padding: 15px 20px;
            font-weight: 600;
            color: #1e40af;
            min-width: 120px;
            text-align: center;
            opacity: 0.3;
            transition: all 0.5s ease;
        }

        .rag-step.active {
            opacity: 1;
            background: linear-gradient(135deg, #1e40af, #3b82f6);
            color: white;
            transform: scale(1.1);
            box-shadow: 0 6px 16px rgba(30, 64, 175, 0.4);
        }

        .rag-arrow {
            font-size: 24px;
            color: #64748b;
            opacity: 0.3;
            transition: all 0.5s ease;
        }

        .rag-arrow.active {
            opacity: 1;
            color: #06b6d4;
            animation: arrowPulse 1s ease;
        }

        @keyframes arrowPulse {
            0%, 100% { transform: translateX(0); }
            50% { transform: translateX(5px); }
        }

        /* Quantization Comparison Styles */
        .quant-comparison {
            display: flex;
            justify-content: center;
            gap: 30px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .quant-box {
            border-radius: 12px;
            padding: 20px;
            text-align: center;
            transition: all 0.5s ease;
        }

        .quant-fp32 {
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            color: white;
            border: 3px solid #1e40af;
        }

        .quant-int4 {
            background: linear-gradient(135deg, #06b6d4, #0891b2);
            color: white;
            border: 3px solid #0e7490;
            transform: scale(0.6);
        }

        .quant-box.animate {
            animation: quantTransform 2s ease-in-out;
        }

        @keyframes quantTransform {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(0.6); }
        }

        /* Token Embedding Lookup Styles */
        .token-lookup-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .token-box, .embedding-vector {
            background: white;
            border: 2px solid #1e40af;
            border-radius: 8px;
            padding: 15px;
            font-weight: 600;
            transition: all 0.5s ease;
        }

        .token-box.highlight, .embedding-vector.highlight {
            background: linear-gradient(135deg, #fde047, #fbbf24);
            border-color: #f59e0b;
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.4);
        }

        /* Neural Network Flow Canvas */
        #neuralFlowCanvas {
            border: 2px solid #1e40af;
            border-radius: 8px;
            background: linear-gradient(135deg, #f8fafc, #f1f5f9);
        }

        .animation-toggle {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            padding: 8px 16px;
            background: rgba(30, 64, 175, 0.1);
            border-radius: 8px;
            cursor: pointer;
            user-select: none;
            transition: all 0.3s ease;
        }

        .animation-toggle:hover {
            background: rgba(30, 64, 175, 0.2);
        }

        .toggle-switch {
            position: relative;
            width: 50px;
            height: 24px;
            background: #cbd5e1;
            border-radius: 12px;
            transition: all 0.3s ease;
        }

        .toggle-switch.active {
            background: #1e40af;
        }

        .toggle-slider {
            position: absolute;
            top: 2px;
            left: 2px;
            width: 20px;
            height: 20px;
            background: white;
            border-radius: 50%;
            transition: all 0.3s ease;
            box-shadow: 0 2px 4px rgba(0,0,0,0.2);
        }

        .toggle-switch.active .toggle-slider {
            left: 28px;
        }
    </style>
</head>
<body>
    <div class="progress-bar" id="progressBar"></div>
    
    <div class="container">
        <header>
            <h1>ğŸ§  AI DeepDive Handbook</h1>
            <p class="subtitle">Complete Technical Guide: Zero to Agentic AI</p>
            <p style="color: #718096;">By jun<span style="font-weight: 700; color: #1e40af; font-size: 1.1em;">AI</span>d sh<span style="font-weight: 700; color: #1e40af; font-size: 1.1em;">AI</span>k | </em> ğŸ¤– | Last Updated: January 2026</p>
            
            <div class="nav-pills">
                <button class="nav-pill active" onclick="showSection('foundations')">ğŸ”§ Foundations</button>
                <button class="nav-pill" onclick="showSection('architecture')">ğŸ—ï¸ Architecture</button>
                <button class="nav-pill" onclick="showSection('training')">ğŸ“ Training</button>
                <button class="nav-pill" onclick="showSection('deployment')">ğŸ“¦ Deployment</button>
                <button class="nav-pill" onclick="showSection('optimization')">âš¡ Optimization</button>
                <button class="nav-pill" onclick="showSection('advanced')">ğŸš€ Advanced</button>
                <button class="nav-pill" onclick="showSection('tools')">ğŸ› ï¸ Tools</button>
                <button class="nav-pill" onclick="showSection('practical')">ğŸ’» Practical</button>
                <button class="nav-pill" onclick="showSection('keytopics')">ğŸ¯ Key Topics</button>
                <button class="nav-pill" onclick="showSection('glossary')">ğŸ“š Glossary</button>
            </div>
            
            <div class="search-container" style="margin: 20px auto; max-width: 600px;">
                <input type="text" id="searchInput" placeholder="ğŸ” Search handbook... (try 'GPU', 'RAG', 'LoRA')" 
                       style="width: 100%; padding: 12px 20px; font-size: 16px; border: 2px solid #1e40af; border-radius: 8px; 
                              background: rgba(255, 255, 255, 0.95); box-shadow: 0 4px 12px rgba(30, 64, 175, 0.2);"
                       onkeyup="searchHandbook()" />
                <div id="searchResults" style="margin-top: 10px; font-size: 0.9em; color: #64748b;"></div>
            </div>
        </header>

        <div class="content-wrapper">
            <aside class="sidebar">
                <h3>Quick Navigation</h3>
                <div class="sidebar-link active" onclick="showSection('foundations')">ğŸ”§ Foundations</div>
                <div class="sidebar-link" onclick="showSection('architecture')">ğŸ—ï¸ Transformer Architecture</div>
                <div class="sidebar-link" onclick="showSection('training')">ğŸ“ Training & Models</div>
                <div class="sidebar-link" onclick="showSection('deployment')">ğŸ“¦ Model Deployment</div>
                <div class="sidebar-link" onclick="showSection('optimization')">âš¡ Optimization</div>
                <div class="sidebar-link" onclick="showSection('advanced')">ğŸš€ Advanced Concepts</div>
                <div class="sidebar-link" onclick="showSection('tools')">ğŸ› ï¸ Tools Ecosystem</div>
                <div class="sidebar-link" onclick="showSection('practical')">ğŸ’» Hands-On Roadmap</div>
                <div class="sidebar-link" onclick="showSection('keytopics')">ğŸ¯ Key Topics to Survive</div>
                <div class="sidebar-link" onclick="showSection('glossary')">ğŸ“š Glossary & FAQs</div>
            </aside>

            <main class="content">
                <!-- SECTION 1: FOUNDATIONS -->
                <div id="foundations" class="section active">
                    <h2>ğŸ”§ 1. Foundations: Hardware & Mathematical Building Blocks</h2>
                    
                    <div class="must-know">
                        <strong>Why This Matters:</strong> Understanding tensors, vectors, and GPU architecture is essential. These are the foundational concepts that everything else builds upon.
                    </div>

                    <div class="concept-card">
                        <h3>1.1 Vectors & Tensors - The Data Structures of AI</h3>
                        
                        <div class="mental-model">
                            <strong>Simple Mental Model:</strong>
                            <ul>
                                <li><strong>Vector:</strong> A 1D array of numbers - like a single row in a spreadsheet</li>
                                <li><strong>Matrix:</strong> A 2D grid - like a spreadsheet with rows and columns</li>
                                <li><strong>Tensor:</strong> Multi-dimensional arrays - like stacked spreadsheets (3D, 4D, etc.)</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>1D Vector</strong> (5 elements):
[0.2, 0.5, 0.8, 0.1, 0.9]

<strong>2D Matrix</strong> (3 rows Ã— 5 columns):
â”Œ                                    â”
â”‚  0.2   0.5   0.8   0.1   0.9  â”‚
â”‚  0.3   0.6   0.2   0.7   0.4  â”‚
â”‚  0.9   0.1   0.5   0.3   0.8  â”‚
â””                                    â”˜

<strong>3D Tensor</strong> (2 Ã— 3 Ã— 5):
Batch 1                 Batch 2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ x x x x x â”‚         â”‚ x x x x x â”‚
â”‚ x x x x x â”‚         â”‚ x x x x x â”‚
â”‚ x x x x x â”‚         â”‚ x x x x x â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Real-World Example: Processing Text with GPT</strong>
                            <pre>
Input: "Hello world"
Tokens: ["Hello", "world"]

After tokenization + embedding:
Tensor shape: [1, 2, 768]
             â†‘  â†‘  â†‘
             â”‚  â”‚  â””â”€ Embedding dimension (each word = 768 numbers)
             â”‚  â””â”€â”€â”€â”€ Sequence length (2 tokens)
             â””â”€â”€â”€â”€â”€â”€â”€ Batch size (1 sentence)

Total numbers: 1 Ã— 2 Ã— 768 = 1,536 numbers!</pre>
                        </div>

                        <div class="example-box">
                            <strong>ğŸ¬ Interactive: Matrix Multiplication Animation</strong>
                            <p style="text-align: center; color: #64748b; font-size: 0.9em; margin-bottom: 10px;">
                                <em>Click "Start Animation" to see how matrices multiply! This is the core operation in neural networks.</em>
                            </p>
                            <div style="text-align: center; margin-bottom: 15px;">
                                <button onclick="startMatrixMultiplication()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3);">Start Animation</button>
                            </div>
                            <div id="matrixMultAnimation" style="display: flex; justify-content: center; align-items: center; gap: 20px; padding: 20px; background: rgba(30, 64, 175, 0.05); border-radius: 8px; flex-wrap: wrap;">
                                <div style="text-align: center;">
                                    <div style="font-weight: 600; margin-bottom: 10px; color: #1e40af;">Matrix A (2Ã—3)</div>
                                    <div class="matrix-grid" id="matrixA" style="display: grid; grid-template-columns: repeat(3, 50px); gap: 5px;">
                                        <div class="matrix-cell">2</div><div class="matrix-cell">1</div><div class="matrix-cell">3</div>
                                        <div class="matrix-cell">4</div><div class="matrix-cell">0</div><div class="matrix-cell">1</div>
                                    </div>
                                </div>
                                <div style="font-size: 2em; color: #1e40af;">Ã—</div>
                                <div style="text-align: center;">
                                    <div style="font-weight: 600; margin-bottom: 10px; color: #1e40af;">Matrix B (3Ã—2)</div>
                                    <div class="matrix-grid" id="matrixB" style="display: grid; grid-template-columns: repeat(2, 50px); gap: 5px;">
                                        <div class="matrix-cell">1</div><div class="matrix-cell">2</div>
                                        <div class="matrix-cell">3</div><div class="matrix-cell">1</div>
                                        <div class="matrix-cell">0</div><div class="matrix-cell">2</div>
                                    </div>
                                </div>
                                <div style="font-size: 2em; color: #1e40af;">=</div>
                                <div style="text-align: center;">
                                    <div style="font-weight: 600; margin-bottom: 10px; color: #06b6d4;">Result (2Ã—2)</div>
                                    <div class="matrix-grid" id="matrixResult" style="display: grid; grid-template-columns: repeat(2, 50px); gap: 5px;">
                                        <div class="matrix-cell result-cell">?</div><div class="matrix-cell result-cell">?</div>
                                        <div class="matrix-cell result-cell">?</div><div class="matrix-cell result-cell">?</div>
                                    </div>
                                </div>
                            </div>
                            <div id="calcDisplay" style="text-align: center; margin-top: 15px; font-family: monospace; color: #64748b; min-height: 25px; font-size: 0.9em;"></div>
                        </div>

                        <div class="tools-box">
                            <strong>Key Tools:</strong>
                            <ul>
                                <li><code>numpy</code> - CPU tensor operations: <code>np.array([[1,2],[3,4]])</code></li>
                                <li><code>torch</code> (PyTorch) - GPU tensors: <code>torch.tensor(data).cuda()</code></li>
                                <li><code>tensorflow</code> - Google's framework: <code>tf.constant(data)</code></li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>1.2 <span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> vs <span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span>: Why GPUs Power <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span></h3>
                        
                        <div class="must-know">
                            <strong>Key Insight:</strong> <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline; font-weight: 600;">GPUs</a> are 50-100x faster than <a href="https://en.wikipedia.org/wiki/Central_processing_unit" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline; font-weight: 600;">CPUs</a> for <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> because matrix multiplication (the core operation in neural networks) is highly parallel. CPUs excel at sequential tasks, GPUs excel at parallel tasks.
                        </div>

                        <div class="mental-model">
                            <strong>Simple Analogy:</strong>
                            <ul>
                                <li><strong><span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span>:</strong> Like having 8-16 highly skilled workers who can do complex tasks one after another</li>
                                <li><strong><span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span>:</strong> Like having 5,000-15,000 simple workers who all do the same simple task simultaneously</li>
                            </ul>
                            <p>Training AI = Doing billions of simple math operations â†’ GPU wins!</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>CPU Architecture</strong> (8 cores):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Core 1 â”‚ â”‚ Core 2 â”‚ â”‚ Core 3 â”‚ â”‚ Core 4 â”‚  â† Complex cores
â”‚ 3.5GHz â”‚ â”‚ 3.5GHz â”‚ â”‚ 3.5GHz â”‚ â”‚ 3.5GHz â”‚     High clock speed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Sequential processing
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Core 5 â”‚ â”‚ Core 6 â”‚ â”‚ Core 7 â”‚ â”‚ Core 8 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>GPU Architecture</strong> (Simplified - showing 64 of ~10,000 cores):
â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”  â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â† Simple cores
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤     Lower clock speed
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚     Massive parallelism
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚     5,000-15,000 cores!
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚
â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜  â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Interactive: <span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> Cores in Action</strong>
                            <p style="text-align: center; color: #64748b; font-size: 0.9em; margin-bottom: 10px;">
                                <em>Hover to see parallel processing! Each square represents a GPU core.</em>
                            </p>
                            <div class="gpu-cores-container">
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                            </div>
                            <p style="text-align: center; color: #1e40af; font-weight: 600; margin-top: 10px;">
                                256 cores shown â€¢ Real GPUs have 5,000-15,000+ cores!
                            </p>
                        </div>

                        <div class="example-box">
                            <strong>Performance Comparison - Matrix Multiplication (4096Ã—4096):</strong>
                            <table>
                                <tr>
                                    <th>Hardware</th>
                                    <th>Time</th>
                                    <th>Speedup</th>
                                </tr>
                                <tr>
                                    <td>Intel i9-13900K (CPU)</td>
                                    <td>~8.5 seconds</td>
                                    <td>1x (baseline)</td>
                                </tr>
                                <tr>
                                    <td>NVIDIA RTX 4090</td>
                                    <td>~0.08 seconds</td>
                                    <td><strong>106x faster!</strong></td>
                                </tr>
                                <tr>
                                    <td>NVIDIA A100 (Data Center)</td>
                                    <td>~0.02 seconds</td>
                                    <td><strong>425x faster!</strong></td>
                                </tr>
                            </table>
                            
                            <p><strong>Training Llama-2-7B from scratch:</strong></p>
                            <ul>
                                <li>CPU only: ~2-3 years ğŸ˜±</li>
                                <li>Single RTX 4090: ~6 months</li>
                                <li>8Ã— A100 GPUs: ~2 weeks âœ…</li>
                            </ul>
                        </div>

                        <h4>Key GPU Components</h4>
                        <table>
                            <tr>
                                <th>Component</th>
                                <th>Purpose</th>
                                <th>Example (NVIDIA A100)</th>
                            </tr>
                            <tr>
                                <td><strong><span class="tooltip">CUDA<span class="tooltiptext">Compute Unified Device Architecture</span></span> Cores</strong></td>
                                <td>General parallel processing units</td>
                                <td>6,912 cores</td>
                            </tr>
                            <tr>
                                <td><strong>Tensor Cores</strong></td>
                                <td>Specialized for AI (matrix operations)</td>
                                <td>432 Gen 3 Tensor Cores</td>
                            </tr>
                            <tr>
                                <td><strong><span class="tooltip">VRAM<span class="tooltiptext">Video Random Access Memory</span></span> (Video RAM)</strong></td>
                                <td>GPU memory for storing models/data</td>
                                <td>40GB or 80GB HBM2e</td>
                            </tr>
                            <tr>
                                <td><strong>Memory Bandwidth</strong></td>
                                <td>Speed of data transfer</td>
                                <td>1,935 GB/s</td>
                            </tr>
                            <tr>
                                <td><strong><span class="tooltip">TFLOPs<span class="tooltiptext">Trillion Floating Point Operations Per Second</span></span> (<span class="tooltip">FP32<span class="tooltiptext">32-bit Floating Point</span></span>)</strong></td>
                                <td>Trillion operations/second</td>
                                <td>19.5 TFLOPs</td>
                            </tr>
                        </table>

                        <div class="mental-model">
                            <strong>Memory Hierarchy (Speed vs Size):</strong>
                            <pre>
Fastest, Smallest
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU Registers     â”‚ â† Lightning fast, tiny (few KB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Shared Memory     â”‚ â† Very fast, small (~100 KB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   L2 Cache          â”‚ â† Fast, medium (~40 MB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   VRAM (GPU Memory) â”‚ â† Fast, large (16-80 GB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   RAM (System Mem)  â”‚ â† Slower, huge (32-512 GB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SSD/HDD Storage   â”‚ â† Slowest, massive (TBs)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Slowest, Largest
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>GPU Monitoring & Management Tools:</strong>
                            <ul>
                                <li><code>nvidia-smi</code> - Monitor GPU usage, memory, temperature
                                    <pre>nvidia-smi</pre>
                                </li>
                                <li><code>nvtop</code> - Interactive GPU monitor (like htop)
                                    <pre>nvtop</pre>
                                </li>
                                <li><code>gpustat</code> - Simple GPU status
                                    <pre>pip install gpustat && gpustat -i</pre>
                                </li>
                                <li><code>CUDA Toolkit</code> - NVIDIA GPU programming framework</li>
                                <li><code>ROCm</code> - AMD GPU alternative to CUDA</li>
                            </ul>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Why can't I just use CPU for AI?</strong><br>
                            A: You can for small models or inference, but training requires massive parallel computation. A single forward pass through GPT-3 involves ~175 billion parameters Ã— batch size operations. GPUs do this 100x faster.
                            <br><br>
                            <strong>Q: How much VRAM do I need?</strong><br>
                            A: Rule of thumb for training:
                            <ul>
                                <li>Model size Ã— 4 (for FP32 precision)</li>
                                <li>Example: 7B model = 7B params Ã— 4 bytes = 28GB minimum</li>
                                <li>Add ~20% for activations, gradients, optimizer states</li>
                                <li>For inference only: Model size Ã— 2 (with quantization, even less!)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- SECTION 2: ARCHITECTURE -->
                <div id="architecture" class="section">
                    <h2>ğŸ—ï¸ 2. Transformer Architecture: The Heart of Modern AI</h2>
                    
                    <div class="must-know">
                        <strong>Core Understanding:</strong> Transformers replaced RNNs/LSTMs and revolutionized AI. The key innovation: <strong>Attention Mechanism</strong> - allowing models to focus on relevant parts of input regardless of distance.
                    </div>

                    <div class="concept-card">
                        <h3>2.1 The Transformer Pipeline: Text â†’ Predictions</h3>
                        
                        <div class="diagram">
<pre>
<strong>Complete Transformer Pipeline:</strong>

Input Text: "The cat sat on"
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. TOKENIZATION (BPE/WordPiece)   â”‚
â”‚  "The cat sat on" â†’ [464, 3797, 3332, 319]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. EMBEDDING LOOKUP                â”‚
â”‚  [464] â†’ [0.23, -0.45, 0.67, ...]  â”‚  768-dim vectors
â”‚  [3797] â†’ [-0.12, 0.89, -0.34, ...] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. POSITIONAL ENCODING             â”‚
â”‚  Add position info: pos_0, pos_1... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. TRANSFORMER BLOCKS (Ã—N layers)  â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Self-Attention (Multi-Head) â”‚  â”‚
â”‚  â”‚  "Which words matter?"       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â†“                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Feed-Forward Network        â”‚  â”‚
â”‚  â”‚  Process each position       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚
â”‚  (Repeat 12-96 times)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. OUTPUT HEAD                      â”‚
â”‚  Final vector â†’ Probability dist     â”‚
â”‚  Over 50,000 possible next tokens    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Output: "mat" (highest probability!)
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Interactive: Transformer Flow</strong>
                            <p style="text-align: center; color: #64748b; font-size: 0.9em; margin-bottom: 15px;">
                                <em>Hover over each step to highlight it!</em>
                            </p>
                            <div class="transformer-flow">
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ“</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Input Text</div>
                                    <div style="font-size: 0.85em; color: #64748b;">Raw words</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ”¢</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Tokens</div>
                                    <div style="font-size: 0.85em; color: #64748b;"><span class="tooltip">BPE<span class="tooltiptext">Byte Pair Encoding</span></span>/WordPiece</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ¯</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Embeddings</div>
                                    <div style="font-size: 0.85em; color: #64748b;">768-dim vectors</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ§ </div>
                                    <div style="font-weight: 600; margin-top: 5px;">Attention</div>
                                    <div style="font-size: 0.85em; color: #64748b;">Context aware</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">âœ¨</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Output</div>
                                    <div style="font-size: 0.85em; color: #64748b;">Next token</div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.2 Tokenization: Breaking Text into Pieces</h3>
                        
                        <div class="mental-model">
                            <strong>Why Tokenization?</strong> Computers only understand numbers. Tokenization converts text â†’ numbers while keeping vocabulary size manageable (~50,000 tokens instead of millions of words).
                        </div>

                        <h4><span class="tooltip">BPE<span class="tooltiptext">Byte Pair Encoding</span></span> (Byte Pair Encoding)</h4>
                        <p><strong>Used by:</strong> <span class="tooltip">GPT<span class="tooltiptext">Generative Pre-trained Transformer</span></span>-2, GPT-3, GPT-4, Llama, Mistral, Qwen, Codex</p>
                        
                        <div class="example-box">
                            <strong>How BPE Works:</strong>
                            <pre>
Input: "unhappiness"

<strong>Step 1:</strong> Start with characters
[u, n, h, a, p, p, i, n, e, s, s]

<strong>Step 2:</strong> Find most frequent pair â†’ merge
"p" + "p" appears most â†’ [u, n, h, a, pp, i, n, e, s, s]

<strong>Step 3:</strong> Repeat thousands of times...
After training: "unhappiness" â†’ ["un", "happi", "ness"] (3 tokens!)

<strong>Benefits:</strong>
âœ“ Common words = 1 token ("the", "and")
âœ“ Rare words = multiple tokens ("unhappiness" â†’ 2-3)
âœ“ Unknown words can still be represented
âœ“ Works across languages
</pre>
                        </div>

                        <h4>WordPiece</h4>
                        <p><strong>Used by:</strong> <span class="tooltip">BERT<span class="tooltiptext">Bidirectional Encoder Representations from Transformers</span></span>, DistilBERT, ALBERT, Google's models</p>
                        
                        <div class="example-box">
                            <strong>WordPiece vs BPE:</strong>
                            <pre>
Input: "playing"

BPE:        ["play", "ing"]
WordPiece:  ["play", "##ing"]  â† ## means "continuation"

Input: "unhappiness"
WordPiece:  ["un", "##happ", "##iness"]

<strong>Key Difference:</strong>
- BPE: Uses frequency (most common pairs)
- WordPiece: Uses likelihood (statistical probability)
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Tokenization Tools:</strong>
                            <ul>
                                <li><code>tiktoken</code> - OpenAI's fast tokenizer (Rust-based)
                                    <pre>import tiktoken
enc = tiktoken.get_encoding("cl100k_base")  # GPT-4
tokens = enc.encode("Hello world")</pre>
                                </li>
                                <li><code>sentencepiece</code> - Google's language-agnostic tokenizer
                                    <pre>import sentencepiece as spm
sp = spm.SentencePieceProcessor(model_file='model.model')
tokens = sp.encode('Hello world', out_type=int)</pre>
                                </li>
                                <li><code>transformers.AutoTokenizer</code> - HuggingFace unified API
                                    <pre>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokens = tokenizer.encode("Hello world")</pre>
                                </li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.3 Embeddings: Words as Vectors in Semantic Space</h3>
                        
                        <div class="must-know">
                            <strong>Critical Concept:</strong> Embeddings convert discrete tokens into continuous vector space where <em>similar meanings = nearby vectors</em>. This is how AI "understands" semantics.
                        </div>

                        <div class="mental-model">
                            <strong>Think of embeddings as coordinates in "meaning space":</strong>
                            <ul>
                                <li>"king" and "queen" are nearby (both royalty)</li>
                                <li>"king" and "banana" are far apart (unrelated)</li>
                                <li>Math works: king - man + woman â‰ˆ queen</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>Embedding Visualization (simplified to 2D):</strong>

            Royalty
               â†‘
               â”‚
         queen â€¢    â€¢ king
               â”‚
    princess â€¢ â”‚      â€¢ prince
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Male/Female
               â”‚
        woman â€¢â”‚      â€¢ man
               â”‚
         girl â€¢â”‚      â€¢ boy
               â”‚
            Common

<strong>In Reality: 768-4096 dimensions!</strong>
"king" = [0.23, -0.45, 0.67, 0.12, ..., -0.89]  (768 numbers)
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Embedding Space Visualization</em>
                            </p>
                            <button onclick="animateEmbeddings()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Start Animation</button>
                            <div>
                                <canvas id="embeddingCanvas" style="max-width: 100%; border: 2px solid #334155; border-radius: 8px; background: #1a1a2e;"></canvas>
                            </div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch word vectors cluster by semantic similarity in pseudo-3D space</p>
                        </div>

                        <div class="example-box">
                            <strong>Real Example: GPT-2 Embeddings</strong>
                            <pre>
Token: "cat" (ID: 3797)
      â†“
Embedding Layer (50,257 tokens Ã— 768 dimensions)
      â†“
Vector: [0.23, -0.45, 0.67, 0.12, -0.34, 0.89, ..., -0.19]
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 768 numbers total â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Similar words have similar vectors:
- "cat" â€¢ "dog" = 0.85 (high similarity)
- "cat" â€¢ "computer" = 0.12 (low similarity)
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Working with Embeddings:</strong>
                            <ul>
                                <li><code>torch.nn.Embedding</code> - PyTorch embedding layer
                                    <pre>embedding = nn.Embedding(vocab_size=50000, embedding_dim=768)
vector = embedding(torch.tensor([3797]))  # "cat"</pre>
                                </li>
                                <li><code>sentence-transformers</code> - Semantic embeddings
                                    <pre>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(["cat", "dog", "computer"])</pre>
                                </li>
                                <li><code>OpenAI Embeddings API</code>
                                    <pre>import openai
response = openai.Embedding.create(
    input="Your text here",
    model="text-embedding-ada-002"
)</pre>
                                </li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.4 Attention Mechanism: The Secret Sauce</h3>
                        
                        <div class="must-know">
                            <strong>Why Attention is Revolutionary:</strong> Before transformers, models processed text sequentially (slow). <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline; font-weight: 600;">Attention</a> allows parallel processing while understanding relationships between ALL words simultaneously.
                        </div>

                        <div class="mental-model">
                            <strong>Simple Mental Model:</strong>
                            <p>When reading "The cat sat on the mat", attention answers:</p>
                            <ul>
                                <li>"What is the subject of the action?" â†’ "sat" looks back at "cat" (attention weight: ~0.65)</li>
                                <li>"How do words connect?" â†’ "on" attends to itself + nearby context (e.g., "sat", "the")</li>
                                <li>Some tokens mostly attend to themselves (a common pattern in many heads/layers)</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>Self-Attention Visualization:</strong>

Input: "The cat sat on the mat"

Attention Scores (simplified):
          The   cat   sat   on   the   mat
The     [ 0.8  0.1  0.05 0.02 0.02 0.01 ]
cat     [ 0.1  0.7  0.15 0.02 0.02 0.01 ]
sat     [ 0.05 0.65 0.15 0.1  0.03 0.02 ]  â† "sat" pays attention to "cat"
on      [ 0.03 0.05 0.2  0.5  0.15 0.07 ]
the     [ 0.8  0.05 0.03 0.02 0.05 0.05 ]
mat     [ 0.02 0.1  0.2  0.1  0.05 0.53 ]

Higher value = more attention
ğŸ”´ Red (0.6+) = Strong attention
ğŸŸ¡ Yellow (0.3-0.6) = Medium attention  
ğŸ”µ Blue (0-0.3) = Low attention
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Attention Heatmap</em>
                            </p>
                            <button onclick="showAttentionHeatmap()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show Attention</button>
                            <div id="attentionGrid" class="attention-grid" style="margin: 0 auto; max-width: 500px;"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Heatmap shows attention scores: Red (high attention) â†’ Yellow (medium) â†’ Blue (low)</p>
                        </div>

                        <h4>Types of Attention</h4>
                        <table>
                            <tr>
                                <th>Type</th>
                                <th>Description</th>
                                <th>Used In</th>
                            </tr>
                            <tr>
                                <td><strong>Self-Attention</strong></td>
                                <td>Words attend to other words in same sequence</td>
                                <td>BERT, GPT, all transformers</td>
                            </tr>
                            <tr>
                                <td><strong>Cross-Attention</strong></td>
                                <td>Output attends to input (decoder â†’ encoder)</td>
                                <td>Translation models, T5</td>
                            </tr>
                            <tr>
                                <td><strong>Multi-Head Attention</strong></td>
                                <td>Multiple attention mechanisms in parallel (different perspectives)</td>
                                <td>Standard in all transformers (8-32 heads)</td>
                            </tr>
                            <tr>
                                <td><strong>Causal/Masked Attention</strong></td>
                                <td>Can only attend to previous tokens (for generation)</td>
                                <td>GPT, Llama, autoregressive models</td>
                            </tr>
                        </table>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Token â†’ Embedding Lookup</em>
                            </p>
                            <button onclick="animateTokenLookup()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Start Lookup</button>
                            <div id="tokenLookupContainer" style="padding: 20px; background: linear-gradient(135deg, #1e293b, #334155); border-radius: 8px; min-height: 500px; display: flex; flex-direction: column; justify-content: center; box-shadow: inset 0 2px 8px rgba(0,0,0,0.2);"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">See how tokens are converted to high-dimensional vectors</p>
                        </div>

                        <div class="example-box">
                            <strong>Multi-Head Attention - Different Perspectives:</strong>
                            <pre>
Input: "The quick brown fox jumps"

Head 1: Focuses on GRAMMAR
  "quick" â†’ "brown" (adjectives modify nouns)
  
Head 2: Focuses on ACTIONS
  "fox" â†’ "jumps" (subject-verb relationship)
  
Head 3: Focuses on SYNTAX
  "The" â†’ "fox" (article-noun relationship)

... (8-32 heads total)

All heads combined = rich understanding!
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Attention in Code:</strong>
                            <pre>import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    """
    Q, K, V: Query, Key, Value matrices
    Shape: (batch, seq_len, d_model)
    """
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.5 Context Window: How Much Can AI "Remember"?</h3>
                        
                        <div class="mental-model">
                            <strong>Context Window = Short-term memory</strong>
                            <ul>
                                <li>Determines maximum input length model can process at once</li>
                                <li>Measured in tokens (not words!)</li>
                                <li>Computational cost grows quadratically (O(nÂ²) with attention)</li>
                            </ul>
                        </div>

                        <table>
                            <tr>
                                <th>Model</th>
                                <th>Context Window</th>
                                <th>Approximate Pages</th>
                            </tr>
                            <tr>
                                <td colspan="3" style="font-size: 0.9em; color: #64748b; background: rgba(59, 130, 246, 0.05);">
                                    Note: context windows vary significantly by model and version; the values below are illustrative.
                                </td>
                            </tr>
                            <tr>
                                <td>GPT-3</td>
                                <td>2,048 tokens</td>
                                <td>~3-4 pages</td>
                            </tr>
                            <tr>
                                <td>GPT-3.5-Turbo</td>
                                <td>4,096 tokens</td>
                                <td>~6-8 pages</td>
                            </tr>
                            <tr>
                                <td>GPT-4</td>
                                <td>8,192 tokens</td>
                                <td>~12-16 pages</td>
                            </tr>
                            <tr>
                                <td>GPT-4-32k</td>
                                <td>32,768 tokens</td>
                                <td>~50 pages</td>
                            </tr>
                            <tr>
                                <td>Claude 3</td>
                                <td>200,000 tokens</td>
                                <td>~300 pages</td>
                            </tr>
                            <tr>
                                <td>Gemini 1.5 Pro</td>
                                <td>1,000,000 tokens</td>
                                <td>~1,500 pages!</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Why Context Window Matters:</strong>
                            <pre>
Scenario: Analyzing a 50-page document

GPT-3 (2K tokens):
âŒ Can only read ~3 pages at once
âŒ Need to break into chunks, lose context

Claude 3 (200K tokens):
âœ… Can read entire document at once!
âœ… Maintains full context
âœ… Better analysis and understanding
</pre>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Why not just make context windows infinite?</strong><br>
                            A: Computational constraints! Attention is O(nÂ²):
                            <ul>
                                <li>2K tokens = 4M operations</li>
                                <li>32K tokens = 1B operations (250x more!)</li>
                                <li>1M tokens = 1T operations (massive!)</li>
                            </ul>
                            New techniques (sparse attention, sliding window) help, but it's still expensive.
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Neural Network Data Flow</em>
                            </p>
                            <div style="margin-bottom: 15px;">
                                <label class="animation-toggle" style="display: inline-flex; align-items: center; gap: 10px;">
                                    <span style="font-size: 0.9em; color: #64748b;">Enable Animation</span>
                                    <div class="toggle-switch">
                                        <input type="checkbox" id="neuralFlowToggle" onchange="toggleNeuralFlow(this.checked)" style="display: none;">
                                        <span class="toggle-slider"></span>
                                    </div>
                                    <span style="font-size: 0.85em; color: #94a3b8;">(Disable on slower devices)</span>
                                </label>
                            </div>
                            <div>
                                <canvas id="neuralFlowCanvas" style="max-width: 100%; border: 2px solid #334155; border-radius: 8px; background: #1a1a2e;"></canvas>
                            </div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch how input tokens flow through multiple transformer layers (Input â†’ Attention â†’ FFN â†’ Output)</p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.6 Transformer Library (HuggingFace)</h3>
                        
                        <div class="tools-box">
                            <strong>ğŸ¤— Transformers - The Standard Library</strong>
                            <pre>pip install transformers</pre>
                            
                            <strong>Basic Usage:</strong>
                            <pre>from transformers import AutoModel, AutoTokenizer

# Load any model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModel.from_pretrained("gpt2")

# Tokenize and generate
inputs = tokenizer("Hello world", return_tensors="pt")
outputs = model(**inputs)</pre>

                            <strong>Popular Models Available:</strong>
                            <ul>
                                <li>GPT-2, GPT-J, GPT-NeoX</li>
                                <li>Llama 2, Llama 3, Mistral, Mixtral</li>
                                <li>BERT, RoBERTa, DistilBERT</li>
                                <li>T5, FLAN-T5, Falcon, Qwen</li>
                                <li>50,000+ models total!</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- SECTION 3: TRAINING -->
                <div id="training" class="section">
                    <h2>ğŸ“ 3. Training & <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> Models: From Data to Weights</h2>
                    
                    <div class="must-know">
                        <strong>Key Distinction:</strong> <strong>Training</strong> = Learning from data (slow, expensive). <strong>Inference</strong> = Using trained model (fast, cheap). Most users only do inference!
                    </div>

                    <div class="concept-card">
                        <h3>3.1 Training vs Inference</h3>
                        
                        <table>
                            <tr>
                                <th>Aspect</th>
                                <th>Training</th>
                                <th>Inference</th>
                            </tr>
                            <tr>
                                <td><strong>Purpose</strong></td>
                                <td>Learn patterns from data</td>
                                <td>Use learned knowledge</td>
                            </tr>
                            <tr>
                                <td><strong>Speed</strong></td>
                                <td>Very slow (days/weeks)</td>
                                <td>Fast (seconds)</td>
                            </tr>
                            <tr>
                                <td><strong>Cost</strong></td>
                                <td>$millions for large models</td>
                                <td>$0.001-0.1 per query</td>
                            </tr>
                            <tr>
                                <td><strong>Hardware</strong></td>
                                <td>100-10,000 GPUs</td>
                                <td>1 GPU or even CPU</td>
                            </tr>
                            <tr>
                                <td><strong>Memory</strong></td>
                                <td>Model + gradients + optimizer (3-4x)</td>
                                <td>Just model weights</td>
                            </tr>
                            <tr>
                                <td><strong>Who Does It?</strong></td>
                                <td>OpenAI, Meta, Google, Anthropic</td>
                                <td>Everyone!</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Training Llama-2-70B (From Scratch):</strong>
                            <pre>
Cost: ~$2-5 million
Hardware: 1,000+ A100 GPUs
Time: ~3 weeks
Data: 2 trillion tokens
Energy: ~1,000 MWh

Inference (Running Llama-2-70B):
Cost: ~$0.001 per query
Hardware: 1 GPU (or quantized on CPU!)
Time: ~1 second per response
</pre>
                        </div>

                        <div class="mental-model">
                            <strong>Analogy:</strong>
                            <ul>
                                <li><strong>Training</strong> = Going to medical school (10 years, expensive)</li>
                                <li><strong>Inference</strong> = Doctor seeing patients (quick, using learned knowledge)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.2 <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> Weights: The "Brain" of the Model</h3>
                        
                        <div class="mental-model">
                            <strong>What are Weights?</strong>
                            <p>Weights are the learned parameters (numbers) that define the model's behavior. Think of them as the "knowledge" encoded in the neural network.</p>
                            <ul>
                                <li><strong>GPT-3:</strong> 175 billion parameters = 175 billion numbers!</li>
                                <li><strong>Storage:</strong> Each weight â‰ˆ 2-4 bytes â†’ 350-700 GB</li>
                                <li><strong>What they encode:</strong> Patterns, facts, grammar, reasoning from training data</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>How Weights Work:</strong>

Input: "The cat sat"
  â†“
[Embedding weights] â† 50,000 Ã— 768 = 38M params
  â†“
[Layer 1 weights] â† 768 Ã— 768 Ã— 4 = 2.4M params
  â†“
[Layer 2 weights]
  â†“
... (repeat 12-96 layers)
  â†“
[Output weights] â† 768 Ã— 50,000 = 38M params
  â†“
Output: "on" (next word prediction)

<strong>Total: 7B-175B+ parameters!</strong>
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Model Size Breakdown:</strong>
                            <table>
                                <tr>
                                    <th>Model</th>
                                    <th>Parameters</th>
                                    <th>Size (FP32)</th>
                                    <th>Size (FP16)</th>
                                </tr>
                                <tr>
                                    <td>GPT-2 Small</td>
                                    <td>117M</td>
                                    <td>468 MB</td>
                                    <td>234 MB</td>
                                </tr>
                                <tr>
                                    <td>GPT-2 Large</td>
                                    <td>1.5B</td>
                                    <td>6 GB</td>
                                    <td>3 GB</td>
                                </tr>
                                <tr>
                                    <td>Llama-2-7B</td>
                                    <td>7B</td>
                                    <td>28 GB</td>
                                    <td>14 GB</td>
                                </tr>
                                <tr>
                                    <td>Llama-2-70B</td>
                                    <td>70B</td>
                                    <td>280 GB</td>
                                    <td>140 GB</td>
                                </tr>
                                <tr>
                                    <td>GPT-3</td>
                                    <td>175B</td>
                                    <td>700 GB</td>
                                    <td>350 GB</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.3 How <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> Models Are Shipped</h3>
                        
                        <div class="mental-model">
                            <strong>A complete model consists of:</strong>
                            <ol>
                                <li><strong>Weights</strong> - The learned parameters</li>
                                <li><strong>Config</strong> - Architecture definition</li>
                                <li><strong>Tokenizer</strong> - Text â†” numbers converter</li>
                                <li><strong>Generation Config</strong> - Default sampling params</li>
                            </ol>
                        </div>

                        <div class="diagram">
<pre>
<strong>Typical Model Directory Structure:</strong>

llama-2-7b/
â”œâ”€â”€ config.json                 â† Architecture definition
â”œâ”€â”€ tokenizer.json              â† BPE/WordPiece vocabulary
â”œâ”€â”€ tokenizer_config.json       â† Tokenizer settings
â”œâ”€â”€ generation_config.json      â† Default sampling params
â”œâ”€â”€ model.safetensors           â† Model weights (primary)
â”‚   OR
â”œâ”€â”€ model-00001-of-00002.safetensors  â† Split for large models
â”œâ”€â”€ model-00002-of-00002.safetensors
â”œâ”€â”€ pytorch_model.bin           â† Alternative (older) format
â””â”€â”€ README.md                   â† Model card & usage
</pre>
                        </div>

                        <h4>config.json - Architecture Definition</h4>
                        <div class="example-box">
<pre><strong>Example: Llama-2-7B config.json</strong>

{
  "architectures": ["LlamaForCausalLM"],
  "hidden_size": 4096,              â† Embedding dimension
  "intermediate_size": 11008,       â† FFN hidden size
  "num_hidden_layers": 32,          â† Number of transformer blocks
  "num_attention_heads": 32,        â† Attention heads per layer
  "num_key_value_heads": 32,        â† For GQA
  "vocab_size": 32000,              â† Tokenizer vocabulary size
  "max_position_embeddings": 4096,  â† Maximum context length
  "rms_norm_eps": 1e-06,           â† Layer norm epsilon
  "torch_dtype": "float16",        â† Weight precision
  "transformers_version": "4.31.0"
}</pre>
                        </div>

                        <h4>generation_config.json - Sampling Parameters</h4>
                        <div class="example-box">
<pre>{
  "bos_token_id": 1,                â† Beginning of sequence
  "eos_token_id": 2,                â† End of sequence
  "pad_token_id": 0,                â† Padding token
  "max_length": 4096,               â† Max generation length
  "temperature": 0.7,               â† Randomness (0=deterministic, 1=creative)
  "top_p": 0.9,                     â† Nucleus sampling
  "top_k": 50,                      â† Top-k sampling
  "do_sample": true                 â† Enable sampling vs greedy
}</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Loading Models:</strong>
                            <pre># HuggingFace Transformers
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")

# Llama.cpp (for GGUF format)
./main -m llama-2-7b.Q4_K_M.gguf -p "Hello"

# vLLM (for fast inference)
from vllm import LLM
llm = LLM(model="meta-llama/Llama-2-7b")</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.4 Hyperparameters: Controlling Model Behavior</h3>
                        
                        <div class="mental-model">
                            <strong>Two Types of Parameters:</strong>
                            <ul>
                                <li><strong>Model Parameters (Weights):</strong> Learned during training</li>
                                <li><strong>Hyperparameters:</strong> Set by humans, control training/generation</li>
                            </ul>
                        </div>

                        <h4>Training Hyperparameters</h4>
                        <table>
                            <tr>
                                <th>Parameter</th>
                                <th>What It Does</th>
                                <th>Typical Values</th>
                            </tr>
                            <tr>
                                <td><strong>Learning Rate</strong></td>
                                <td>How fast model learns</td>
                                <td>1e-5 to 1e-3</td>
                            </tr>
                            <tr>
                                <td><strong>Batch Size</strong></td>
                                <td>Samples per training step</td>
                                <td>8-128</td>
                            </tr>
                            <tr>
                                <td><strong>Epochs</strong></td>
                                <td>Passes through full dataset</td>
                                <td>1-10</td>
                            </tr>
                            <tr>
                                <td><strong>Max Sequence Length</strong></td>
                                <td>Maximum context window</td>
                                <td>512-4096</td>
                            </tr>
                        </table>

                        <h4>Generation Hyperparameters (Inference)</h4>
                        <div class="must-know">
                            <strong>These control how the model generates text!</strong>
                        </div>

                        <table>
                            <tr>
                                <th>Parameter</th>
                                <th>What It Does</th>
                                <th>Range</th>
                                <th>Effect</th>
                            </tr>
                            <tr>
                                <td><strong>Temperature</strong></td>
                                <td>Controls randomness</td>
                                <td>0.0 - 2.0</td>
                                <td>0 = deterministic, 1 = balanced, 2 = very creative</td>
                            </tr>
                            <tr>
                                <td><strong>Top-p (Nucleus)</strong></td>
                                <td>Cumulative probability threshold</td>
                                <td>0.0 - 1.0</td>
                                <td>0.9 = consider top 90% probable tokens</td>
                            </tr>
                            <tr>
                                <td><strong>Top-k</strong></td>
                                <td>Consider only top k tokens</td>
                                <td>1 - 100</td>
                                <td>40 = only consider 40 most likely tokens</td>
                            </tr>
                            <tr>
                                <td><strong>Max Tokens</strong></td>
                                <td>Maximum response length</td>
                                <td>1 - 4096+</td>
                                <td>Limits generation length</td>
                            </tr>
                            <tr>
                                <td><strong>Repetition Penalty</strong></td>
                                <td>Discourage repetition</td>
                                <td>1.0 - 1.5</td>
                                <td>1.0 = no penalty, 1.2 = moderate</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Temperature in Action:</strong>
                            <pre>
Prompt: "The capital of France is"

<strong>Temperature = 0.0 (Deterministic):</strong>
Output: "Paris" (always the same, highest probability)

<strong>Temperature = 0.7 (Balanced):</strong>
Output: "Paris" (90%), "the city of Paris" (8%), "Paris, France" (2%)

<strong>Temperature = 1.5 (Creative):</strong>
Output: "Paris" (40%), "Lyon" (15%), "a beautiful city" (10%), ...

<strong>Temperature = 2.0 (Chaotic):</strong>
Output: Often nonsensical or random!
</pre>
                        </div>

                        <div class="diagram">
<pre>
<strong>How Top-k and Top-p Work Together:</strong>

Token Probabilities:
"Paris":    45%
"the":      20%
"France":   15%
"a":        10%
"located":   5%
"Lyon":      3%
"Marseille": 2%

<strong>Top-k = 3:</strong>
Only consider: "Paris", "the", "France" (top 3)

<strong>Top-p = 0.8:</strong>
Consider until 80% cumulative: "Paris" (45%) + "the" (20%) + "France" (15%) = 80%

<strong>Using Both:</strong>
Final candidates = intersection = "Paris", "the", "France"
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Setting Generation Parameters:</strong>
                            <pre># OpenAI API
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}],
    temperature=0.7,
    top_p=0.9,
    max_tokens=100
)

# HuggingFace Transformers
outputs = model.generate(
    inputs,
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    do_sample=True
)</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.5 How Models Are Distributed</h3>
                        
                        <div class="faq-box">
                            <strong>Q: What's the difference between proprietary and open-source models?</strong>
                            
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Proprietary (Closed)</th>
                                    <th>Open Source</th>
                                </tr>
                                <tr>
                                    <td><strong>Examples</strong></td>
                                    <td>GPT-4, Claude, Gemini</td>
                                    <td>Llama, Mistral, Falcon</td>
                                </tr>
                                <tr>
                                    <td><strong>Weights Available?</strong></td>
                                    <td>âŒ No (API only)</td>
                                    <td>âœ… Yes (download)</td>
                                </tr>
                                <tr>
                                    <td><strong>Access Method</strong></td>
                                    <td>API calls (pay per token)</td>
                                    <td>Download & run locally</td>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td>$0.001-0.06 per 1K tokens</td>
                                    <td>Free (just hardware costs)</td>
                                </tr>
                                <tr>
                                    <td><strong>Customization</strong></td>
                                    <td>Limited (fine-tuning API)</td>
                                    <td>Full control</td>
                                </tr>
                                <tr>
                                    <td><strong>Privacy</strong></td>
                                    <td>Data sent to provider</td>
                                    <td>Runs on your hardware</td>
                                </tr>
                            </table>
                        </div>

                        <h4>Where to Get Models</h4>
                        <div class="tools-box">
                            <strong>ğŸ¤— HuggingFace Hub</strong> - Largest repository (500K+ models)
                            <pre>from transformers import AutoModel
model = AutoModel.from_pretrained("meta-llama/Llama-2-7b")</pre>

                            <strong>ğŸ¦™ Ollama</strong> - Easy local deployment
                            <pre>ollama pull llama2
ollama run llama2</pre>

                            <strong>ğŸ”¥ LM Studio</strong> - GUI for local models
                            <p>Download from: lmstudio.ai</p>

                            <strong>âš¡ TheBloke on HuggingFace</strong> - Quantized models
                            <pre>huggingface-cli download TheBloke/Llama-2-7B-GGUF</pre>
                        </div>

                        <h4>Installation Tools</h4>
                        <table>
                            <tr>
                                <th>Tool</th>
                                <th>Purpose</th>
                                <th>Best For</th>
                            </tr>
                            <tr>
                                <td><code>transformers</code></td>
                                <td>Load PyTorch/TF models</td>
                                <td>Development, fine-tuning</td>
                            </tr>
                            <tr>
                                <td><code>llama.cpp</code></td>
                                <td>CPU-optimized inference</td>
                                <td>Running on CPU, edge devices</td>
                            </tr>
                            <tr>
                                <td><code>vLLM</code></td>
                                <td>High-throughput serving</td>
                                <td>Production inference</td>
                            </tr>
                            <tr>
                                <td><code>Ollama</code></td>
                                <td>Local deployment</td>
                                <td>Beginners, quick testing</td>
                            </tr>
                            <tr>
                                <td><code>TGI</code> (Text Gen Inference)</td>
                                <td>HuggingFace serving</td>
                                <td>Production serving</td>
                            </tr>
                        </table>
                    </div>
                </div>

                <!-- SECTION 4: DEPLOYMENT -->
                <div id="deployment" class="section">
                    <h2>ğŸ“¦ 4. Model Deployment & Formats</h2>
                    
                    <div class="must-know">
                        <strong>Key Understanding:</strong> Model weights can be stored in different formats. The format determines compatibility, loading speed, and safety. Two main formats: <strong><span class="tooltip">SafeTensors<span class="tooltiptext">Safe Tensor Format - No arbitrary code execution</span></span></strong> (new standard) and <strong><span class="tooltip">GGUF<span class="tooltiptext">llama.cpp unified model format (supports quantized weights + metadata)</span></span></strong> (for llama.cpp).
                    </div>

                    <div class="concept-card">
                        <h3>4.1 Model Format Comparison</h3>
                        
                        <table>
                            <tr>
                                <th>Format</th>
                                <th>Extension</th>
                                <th>Framework</th>
                                <th>Pros</th>
                                <th>Cons</th>
                            </tr>
                            <tr>
                                <td><strong>SafeTensors</strong></td>
                                <td>.safetensors</td>
                                <td>PyTorch, TF, JAX</td>
                                <td>âœ… Safe (no code execution)<br>âœ… Fast loading<br>âœ… Zero-copy</td>
                                <td>âŒ Larger file size (no compression)</td>
                            </tr>
                            <tr>
                                <td><strong>GGUF</strong></td>
                                <td>.gguf</td>
                                <td>llama.cpp</td>
                                <td>âœ… CPU optimized<br>âœ… Includes metadata<br>âœ… Quantized</td>
                                <td>âŒ llama.cpp specific</td>
                            </tr>
                            <tr>
                                <td><strong>PyTorch</strong></td>
                                <td>.bin, .pt, .pth</td>
                                <td>PyTorch</td>
                                <td>âœ… Native PyTorch</td>
                                <td>âš ï¸ Can execute arbitrary code<br>âŒ Slower loading</td>
                            </tr>
                            <tr>
                                <td><strong>ONNX</strong></td>
                                <td>.onnx</td>
                                <td>Cross-platform</td>
                                <td>âœ… Framework agnostic<br>âœ… Optimized inference</td>
                                <td>âŒ Limited model support</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>4.2 <span class="tooltip">SafeTensors<span class="tooltiptext">Safe Tensor Format - No arbitrary code execution</span></span>: The New Standard</h3>
                        
                        <div class="mental-model">
                            <strong>Why SafeTensors?</strong>
                            <p>Traditional PyTorch (.bin) files use Python's pickle, which can execute arbitrary code â†’ security risk!</p>
                            <p>SafeTensors = Simple binary format, no code execution, faster loading.</p>
                        </div>

                        <div class="example-box">
                            <strong>File Structure:</strong>
                            <pre>
Header: JSON metadata (tensor names, shapes, dtypes, offsets)
Data:   Raw binary tensor data

Example:
llama-2-7b.safetensors (13.5 GB)
â”œâ”€â”€ Header (8 KB): {"model.embed_tokens.weight": {...}, ...}
â””â”€â”€ Data (13.5 GB): [raw floats in FP16]
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Working with SafeTensors:</strong>
                            <pre># Install
pip install safetensors

# Save model
from safetensors.torch import save_file
save_file(model.state_dict(), "model.safetensors")

# Load model
from safetensors.torch import load_file
state_dict = load_file("model.safetensors")
model.load_state_dict(state_dict)

# Convert PyTorch â†’ SafeTensors
from transformers import AutoModel
model = AutoModel.from_pretrained("gpt2")
model.save_pretrained("gpt2-safe", safe_serialization=True)</pre>
                        </div>

                        <h4>Benefits Over PyTorch .bin:</h4>
                        <ul>
                            <li><strong>Security:</strong> No arbitrary code execution (pickle attacks)</li>
                            <li><strong>Speed:</strong> 2-10x faster loading (zero-copy, memory mapping)</li>
                            <li><strong>Cross-framework:</strong> Works with PyTorch, TensorFlow, JAX</li>
                            <li><strong>Validation:</strong> Built-in tensor integrity checks</li>
                        </ul>
                    </div>

                    <div class="concept-card">
                        <h3>4.3 <span class="tooltip">GGUF<span class="tooltiptext">llama.cpp unified model format (supports quantized weights + metadata)</span></span>: Optimized for <span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span> Inference</h3>
                        
                        <div class="mental-model">
                            <strong>GGUF = llama.cpp unified model format</strong>
                            <p>Designed for llama.cpp - enables fast CPU inference with quantization and metadata in one file.</p>
                            <p><strong>Key feature:</strong> Single file contains model + metadata + quantization!</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>GGUF File Structure:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Magic Number & Version          â”‚ â† "GGUF" signature
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metadata (JSON-like)            â”‚ â† Model config, tokenizer
â”‚  - architecture: "llama"         â”‚
â”‚  - vocab_size: 32000             â”‚
â”‚  - context_length: 4096          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Vocabulary & Tokens             â”‚ â† Tokenizer embedded!
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Info (offsets, shapes)   â”‚ â† Layer mapping
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Quantized Weights               â”‚ â† Model parameters
â”‚  (Q4_K_M, Q5_K_S, etc.)         â”‚    (compressed!)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

File: llama-2-7b.Q4_K_M.gguf (4.1 GB)
vs
SafeTensors: llama-2-7b.safetensors (13.5 GB)
â†’ 3.3x smaller due to quantization!
</pre>
                        </div>

                        <h4>GGUF Quantization Schemes</h4>
                        <table>
                            <tr>
                                <th>Quantization</th>
                                <th>Bits per Weight</th>
                                <th>File Size (7B)</th>
                                <th>Quality</th>
                                <th>Speed</th>
                            </tr>
                            <tr>
                                <td><strong>Q2_K</strong></td>
                                <td>~2.5 bits</td>
                                <td>~2.7 GB</td>
                                <td>â­â­ Poor</td>
                                <td>âš¡âš¡âš¡ Fastest</td>
                            </tr>
                            <tr>
                                <td><strong>Q4_K_M</strong></td>
                                <td>~4.5 bits</td>
                                <td>~4.1 GB</td>
                                <td>â­â­â­â­ Good</td>
                                <td>âš¡âš¡ Fast</td>
                            </tr>
                            <tr>
                                <td><strong>Q5_K_M</strong></td>
                                <td>~5.5 bits</td>
                                <td>~4.8 GB</td>
                                <td>â­â­â­â­â­ Excellent</td>
                                <td>âš¡ Medium</td>
                            </tr>
                            <tr>
                                <td><strong>Q8_0</strong></td>
                                <td>8 bits</td>
                                <td>~7.0 GB</td>
                                <td>â­â­â­â­â­ Near-perfect</td>
                                <td>âš¡ Slower</td>
                            </tr>
                            <tr>
                                <td><strong>F16</strong></td>
                                <td>16 bits</td>
                                <td>~13.5 GB</td>
                                <td>â­â­â­â­â­ Perfect</td>
                                <td>Slowest</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Recommended Quantizations:</strong>
                            <ul>
                                <li><strong>Q4_K_M:</strong> Best balance (quality vs size) - <em>recommended for most users</em></li>
                                <li><strong>Q5_K_M:</strong> Higher quality, slightly larger</li>
                                <li><strong>Q8_0:</strong> Near-lossless, for quality-critical applications</li>
                                <li><strong>Q2_K:</strong> Extreme compression, significant quality loss</li>
                            </ul>
                        </div>

                        <div class="tools-box">
                            <strong>Converting SafeTensors â†’ GGUF:</strong>
                            <pre># 1. Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# 2. Convert model
python convert.py /path/to/model/

# 3. Quantize
./quantize /path/to/model/ggml-model-f16.gguf \
           /path/to/model/ggml-model-Q4_K_M.gguf Q4_K_M

# 4. Run inference
./main -m ggml-model-Q4_K_M.gguf -p "Hello world" -n 128</pre>

                            <strong>Using with Ollama:</strong>
                            <pre># Ollama handles GGUF automatically
ollama pull llama2
ollama run llama2 "Hello world"</pre>

                            <strong>Using with LM Studio:</strong>
                            <p>1. Download LM Studio: lmstudio.ai</p>
                            <p>2. Search & download GGUF models directly in GUI</p>
                            <p>3. Run with one click!</p>
                        </div>

                        <div class="faq-box">
                            <strong>Q: SafeTensors vs GGUF - which should I use?</strong><br>
                            A: Depends on your use case:
                            <ul>
                                <li><strong>SafeTensors:</strong> If you're doing training, fine-tuning, or using GPUs with HuggingFace/PyTorch</li>
                                <li><strong>GGUF:</strong> If you're running inference on CPU, need smaller files, or using llama.cpp/Ollama</li>
                            </ul>
                            <p><em>Many models are distributed in both formats!</em></p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>4.4 Model Conversion Tools</h3>
                        
                        <div class="tools-box">
                            <strong>ğŸ”„ Key Conversion Tools:</strong>
                            
                            <h4>1. llama.cpp convert.py</h4>
                            <pre># PyTorch/SafeTensors â†’ GGUF
python convert.py /path/to/model/

# Quantize GGUF
./quantize model-f16.gguf model-q4.gguf Q4_K_M</pre>

                            <h4>2. HuggingFace Optimum</h4>
                            <pre>pip install optimum

# Convert to ONNX
optimum-cli export onnx --model gpt2 onnx/

# Quantize ONNX
optimum-cli onnxruntime quantize \
    --onnx_model onnx/ \
    --output quantized/</pre>

                            <h4>3. ONNX Runtime</h4>
                            <pre>pip install onnxruntime-gpu

# Load and run ONNX model
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
outputs = session.run(None, inputs)</pre>

                            <h4>4. AutoGPTQ</h4>
                            <pre>pip install auto-gptq

# Quantize to GPTQ (4-bit)
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_pretrained("model/", quantize_config)
model.quantize(dataset)
model.save_quantized("quantized/")</pre>
                        </div>

                        <h4>Conversion Matrix</h4>
                        <table>
                            <tr>
                                <th>From</th>
                                <th>To</th>
                                <th>Tool</th>
                                <th>Command</th>
                            </tr>
                            <tr>
                                <td>PyTorch .bin</td>
                                <td>SafeTensors</td>
                                <td>transformers</td>
                                <td><code>save_pretrained(..., safe_serialization=True)</code></td>
                            </tr>
                            <tr>
                                <td>SafeTensors</td>
                                <td>GGUF</td>
                                <td>llama.cpp</td>
                                <td><code>python convert.py</code></td>
                            </tr>
                            <tr>
                                <td>PyTorch/ONNX</td>
                                <td>TensorRT</td>
                                <td>TensorRT</td>
                                <td><code>trtexec</code></td>
                            </tr>
                            <tr>
                                <td>Any</td>
                                <td>ONNX</td>
                                <td>optimum</td>
                                <td><code>optimum-cli export onnx</code></td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>4.5 Inference Engines & Serving</h3>
                        
                        <table>
                            <tr>
                                <th>Engine</th>
                                <th>Best For</th>
                                <th>Hardware</th>
                                <th>Speed</th>
                            </tr>
                            <tr>
                                <td><strong>vLLM</strong></td>
                                <td>High-throughput serving</td>
                                <td>GPU (CUDA)</td>
                                <td>âš¡âš¡âš¡ Fastest (PagedAttention)</td>
                            </tr>
                            <tr>
                                <td><strong>llama.cpp</strong></td>
                                <td>CPU inference, edge devices</td>
                                <td>CPU, Metal, Vulkan</td>
                                <td>âš¡âš¡ Fast on CPU</td>
                            </tr>
                            <tr>
                                <td><strong>TGI</strong> (Text Gen Inference)</td>
                                <td>Production serving (HuggingFace)</td>
                                <td>GPU</td>
                                <td>âš¡âš¡âš¡ Very fast</td>
                            </tr>
                            <tr>
                                <td><strong>TensorRT-LLM</strong></td>
                                <td>NVIDIA-optimized inference</td>
                                <td>NVIDIA GPU only</td>
                                <td>âš¡âš¡âš¡ Fastest on NVIDIA</td>
                            </tr>
                            <tr>
                                <td><strong>Ollama</strong></td>
                                <td>Easy local deployment</td>
                                <td>CPU, GPU</td>
                                <td>âš¡ Good</td>
                            </tr>
                            <tr>
                                <td><strong>ONNX Runtime</strong></td>
                                <td>Cross-platform inference</td>
                                <td>CPU, GPU, Mobile</td>
                                <td>âš¡âš¡ Fast</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Quick Start Examples:</strong>
                            
                            <pre><strong># vLLM (fastest for GPU)</strong>
pip install vllm
vllm serve meta-llama/Llama-2-7b

<strong># llama.cpp (best for CPU)</strong>
./main -m model.gguf -p "Hello" -n 100 -c 4096

<strong># TGI (HuggingFace production)</strong>
docker run -p 8080:80 \
  ghcr.io/huggingface/text-generation-inference \
  --model-id meta-llama/Llama-2-7b

<strong># Ollama (easiest)</strong>
ollama run llama2</pre>
                        </div>
                    </div>
                </div>

                <!-- SECTION 5: OPTIMIZATION -->
                <div id="optimization" class="section">
                    <h2>âš¡ 5. Optimization: Making AI Efficient</h2>
                    
                    <div class="must-know">
                        <strong>Core Principle:</strong> Training from scratch costs millions. Optimization techniques let you adapt existing models cheaply and run them faster. Key methods: <strong>Fine-Tuning, LoRA, Quantization</strong>.
                    </div>

                    <div class="concept-card">
                        <h3>5.1 Fine-Tuning Spectrum</h3>
                        
                        <div class="diagram">
<pre>
<strong>Fine-Tuning Methods (Cost vs Customization)</strong>

High Customization, High Cost
         â†‘
         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Full Fine-Tuning (FPFT)   â”‚  Update ALL parameters
    â”‚  Cost: $$$$, Time: Days    â”‚  Needs: 100K+ examples
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LoRA (Low-Rank Adapt)     â”‚  Update small adapter layers
    â”‚  Cost: $$, Time: Hours     â”‚  Needs: 1K-10K examples
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  QLoRA (Quantized LoRA)    â”‚  LoRA on quantized model
    â”‚  Cost: $, Time: Hours      â”‚  Needs: 1K-10K examples
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Prompt Engineering        â”‚  No training!
    â”‚  Cost: Free, Time: Minutes â”‚  Needs: Examples in prompt
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
Low Customization, Low Cost
</pre>
                        </div>

                        <table>
                            <tr>
                                <th>Method</th>
                                <th>Parameters Updated</th>
                                <th>Memory (7B)</th>
                                <th>Cost</th>
                                <th>Time</th>
                            </tr>
                            <tr>
                                <td><strong>Full Fine-Tuning</strong></td>
                                <td>100% (7B)</td>
                                <td>~120 GB</td>
                                <td>$$$$</td>
                                <td>Days</td>
                            </tr>
                            <tr>
                                <td><strong>LoRA</strong></td>
                                <td>~0.1% (7M)</td>
                                <td>often 16â€“40+ GB</td>
                                <td>$$</td>
                                <td>Hours</td>
                            </tr>
                            <tr>
                                <td><strong>QLoRA</strong></td>
                                <td>~0.1% (7M)</td>
                                <td>often 8â€“24 GB</td>
                                <td>$</td>
                                <td>Hours</td>
                            </tr>
                            <tr>
                                <td><strong>Prompt Engineering</strong></td>
                                <td>0%</td>
                                <td>Inference only</td>
                                <td>Free</td>
                                <td>Minutes</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>5.2 <span class="tooltip">LoRA<span class="tooltiptext">Low-Rank Adaptation</span></span> (Low-Rank Adaptation)</h3>
                        
                        <div class="must-know">
                            <strong>Key Insight:</strong> Instead of updating all 7 billion parameters, LoRA adds tiny "adapter" matrices that capture task-specific knowledge. Original weights stay frozen!
                        </div>

                        <div class="mental-model">
                            <strong>Analogy:</strong> Instead of rewriting an entire encyclopedia (full fine-tuning), you add sticky notes with corrections/additions (LoRA adapters).
                        </div>

                        <div class="diagram">
<pre>
<strong>How LoRA Works:</strong>

Original Weight Matrix (W):        LoRA Adapters:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”
â”‚                â”‚                â”‚    â”‚    â”‚    â”‚
â”‚   W (4096Ã—4096)â”‚     +          â”‚ A  â”‚ Ã— â”‚ B  â”‚
â”‚                â”‚                â”‚    â”‚    â”‚    â”‚
â”‚   Frozen! â„ï¸   â”‚                â””â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               4096Ã—8    8Ã—4096
                                 
                                 Total: ~65K params
                                 vs 16M in original!

<strong>Final Output:</strong> W + (A Ã— B) Ã— scale
                    â†‘      â†‘       â†‘
              Original  Adapter  Scaling factor
</pre>
                        </div>

                        <div class="example-box">
                            <strong>LoRA Benefits:</strong>
                            <ul>
                                <li><strong>Memory Efficient:</strong> Often ~3â€“10Ã— less VRAM than full fine-tuning (highly setup-dependent)</li>
                                <li><strong>Fast Training:</strong> Hours instead of days</li>
                                <li><strong>Small Adapters:</strong> LoRA weights ~10-100 MB vs 13.5 GB full model</li>
                                <li><strong>Multi-Task:</strong> Swap adapters for different tasks!
                                    <pre>
Base model (Llama-2-7B) + Adapter A (Python code)
Base model (Llama-2-7B) + Adapter B (Medical QA)
Base model (Llama-2-7B) + Adapter C (Translation)
                                    </pre>
                                </li>
                            </ul>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: LoRA Adapter Injection</em>
                            </p>
                            <button onclick="animateLoRA()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show LoRA</button>
                            <div id="loraContainer" class="lora-container" style="padding: 20px; background: linear-gradient(135deg, #1e293b, #334155); border-radius: 8px; min-height: 200px;"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">See how tiny adapters customize massive models efficiently</p>
                        </div>

                        <h4>LoRA Hyperparameters</h4>
                        <table>
                            <tr>
                                <th>Parameter</th>
                                <th>Description</th>
                                <th>Typical Values</th>
                            </tr>
                            <tr>
                                <td><strong>Rank (r)</strong></td>
                                <td>Dimension of adapter matrices</td>
                                <td>4, 8, 16, 32 (higher = more expressive)</td>
                            </tr>
                            <tr>
                                <td><strong>Alpha (Î±)</strong></td>
                                <td>Scaling factor</td>
                                <td>16, 32 (usually 2Ã— rank)</td>
                            </tr>
                            <tr>
                                <td><strong>Target Modules</strong></td>
                                <td>Which layers to apply LoRA</td>
                                <td>q_proj, v_proj (attention)</td>
                            </tr>
                            <tr>
                                <td><strong>Dropout</strong></td>
                                <td>Regularization</td>
                                <td>0.05 - 0.1</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Fine-Tuning with LoRA (PEFT Library):</strong>
                            <pre>pip install peft transformers bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")

# Configure LoRA
lora_config = LoraConfig(
    r=8,                           # Rank
    lora_alpha=16,                 # Scaling
    target_modules=["q_proj", "v_proj"],  # Which layers
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA adapters
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 7.4M / 7B = 0.1%

# Train (standard PyTorch/HF Trainer)
from transformers import Trainer
trainer = Trainer(model=model, args=training_args, ...)
trainer.train()

# Save LoRA adapters only (tiny!)
model.save_pretrained("lora-adapters/")  # ~10-50 MB!

# Load later
from peft import PeftModel
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
model = PeftModel.from_pretrained(base_model, "lora-adapters/")</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>5.3 <span class="tooltip">QLoRA<span class="tooltiptext">Quantized Low-Rank Adaptation</span></span> (Quantized LoRA)</h3>
                        
                        <div class="must-know">
                            <strong>Game Changer:</strong> QLoRA = LoRA + 4-bit quantization. It can make very large models feasible on a single 16â€“24GB GPU in some setups (sequence length/batch size matter).
                        </div>

                        <div class="mental-model">
                            <p><strong>QLoRA Innovation:</strong></p>
                            <ol>
                                <li>Load base model in 4-bit (quantized) - saves 4x memory</li>
                                <li>Keep LoRA adapters in full precision (FP16)</li>
                                <li>During forward pass: dequantize â†’ compute â†’ quantize</li>
                            </ol>
                        </div>

                        <div class="example-box">
                            <strong>Memory Comparison (Llama-2-7B):</strong>
                            <pre>
Full Fine-Tuning (FP32):    often 80â€“200+ GB VRAM âŒ
Full Fine-Tuning (FP16/BF16): often 40â€“120+ GB VRAM âŒ
LoRA (FP16/BF16):           often 16â€“40+ GB VRAM âš ï¸
QLoRA (4-bit + LoRA):       often 8â€“24 GB VRAM âœ…

â†’ Actual VRAM depends heavily on sequence length, batch size, optimizer, and checkpointing.
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>QLoRA Training:</strong>
                            <pre>pip install peft transformers bitsandbytes accelerate

from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, prepare_model_for_kbit_training

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",     # Normalized Float 4
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True  # Double quantization
)

# Load model in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    quantization_config=bnb_config,
    device_map="auto"
)

# Prepare for training
model = prepare_model_for_kbit_training(model)

# Add LoRA adapters
lora_config = LoraConfig(r=16, lora_alpha=32, ...)
model = get_peft_model(model, lora_config)

# Train as usual!
trainer.train()</pre>
                        </div>

                        <h4>QLoRA vs LoRA</h4>
                        <table>
                            <tr>
                                <th>Aspect</th>
                                <th>LoRA</th>
                                <th>QLoRA</th>
                            </tr>
                            <tr>
                                <td><strong>Base Model Precision</strong></td>
                                <td>FP16</td>
                                <td>4-bit (NF4)</td>
                            </tr>
                            <tr>
                                <td><strong>Memory (7B)</strong></td>
                                <td>often 16â€“40+ GB</td>
                                <td>often 8â€“24 GB</td>
                            </tr>
                            <tr>
                                <td><strong>Speed</strong></td>
                                <td>Faster</td>
                                <td>Slightly slower (~10-20%)</td>
                            </tr>
                            <tr>
                                <td><strong>Quality</strong></td>
                                <td>High</td>
                                <td>Often close (validate on your tasks)</td>
                            </tr>
                            <tr>
                                <td><strong>Hardware Requirement</strong></td>
                                <td>24+ GB VRAM</td>
                                <td>8+ GB VRAM</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>5.4 Quantization: Smaller, Faster Models</h3>
                        
                        <div class="mental-model">
                            <strong>Quantization = Reducing precision of weights</strong>
                            <p>Instead of storing each weight as 32-bit float, use 8-bit, 4-bit, or even 2-bit integers.</p>
                            <p><strong>Trade-off:</strong> Smaller size & faster inference vs slight quality loss</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>Precision Comparison:</strong>

FP32 (Full Precision):
  Weight = 0.123456789  (32 bits = 4 bytes)
  Range: Â±3.4 Ã— 10Â³â¸

FP16 (Half Precision):
  Weight = 0.1235  (16 bits = 2 bytes)
  Range: Â±65,504

INT8 (8-bit Integer):
  Weight = 31  (8 bits = 1 byte)
  Range: -128 to 127

INT4 (4-bit Integer):
  Weight = 7  (4 bits = 0.5 bytes)
  Range: -8 to 7

<strong>Model Size Impact (7B parameters):</strong>
FP32: 7B Ã— 4 bytes = 28 GB
FP16: 7B Ã— 2 bytes = 14 GB (2x smaller)
INT8: 7B Ã— 1 byte  = 7 GB  (4x smaller)
INT4: 7B Ã— 0.5 byte = 3.5 GB (8x smaller!)
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Quantization Size Comparison</em>
                            </p>
                            <button onclick="showQuantization()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show Comparison</button>
                            <div id="quantContainer" class="quant-comparison" style="display: flex; align-items: center; justify-content: center; gap: 20px; padding: 30px; flex-wrap: wrap; min-height: 180px;"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">See the dramatic size reduction from quantization</p>
                        </div>

                        <h4>Quantization Methods</h4>
                        <table>
                            <tr>
                                <th>Method</th>
                                <th>Bits</th>
                                <th>Size Reduction</th>
                                <th>Quality Loss</th>
                                <th>Best For</th>
                            </tr>
                            <tr>
                                <td><strong>FP16/BF16</strong></td>
                                <td>16-bit</td>
                                <td>2x</td>
                                <td>Negligible</td>
                                <td>Standard training/inference</td>
                            </tr>
                            <tr>
                                <td><strong>INT8</strong></td>
                                <td>8-bit</td>
                                <td>4x</td>
                                <td>Minimal (<1%)</td>
                                <td>Production inference</td>
                            </tr>
                            <tr>
                                <td><strong>GPTQ</strong></td>
                                <td>4-bit</td>
                                <td>8x</td>
                                <td>Small (~2-3%)</td>
                                <td>GPU inference</td>
                            </tr>
                            <tr>
                                <td><strong>AWQ</strong></td>
                                <td>4-bit</td>
                                <td>8x</td>
                                <td>Minimal</td>
                                <td>GPU inference (better quality)</td>
                            </tr>
                            <tr>
                                <td><strong>GGML/GGUF</strong></td>
                                <td>2-8 bit</td>
                                <td>4-16x</td>
                                <td>Varies</td>
                                <td>CPU inference (llama.cpp)</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Quantization Tools:</strong>
                            
                            <h4>1. AutoGPTQ (4-bit for GPU)</h4>
                            <pre>pip install auto-gptq

from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Quantization config
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False
)

# Load and quantize
model = AutoGPTQForCausalLM.from_pretrained("model/")
model.quantize(calibration_data, quantize_config=quantize_config)
model.save_quantized("model-gptq-4bit/")</pre>

                            <h4>2. AWQ (Activation-aware Weight Quantization)</h4>
                            <pre>pip install autoawq

from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained("model/")
model.quantize(calib_data, quant_config={"bits": 4, "group_size": 128})
model.save_quantized("model-awq-4bit/")</pre>

                            <h4>3. bitsandbytes (8-bit/4-bit)</h4>
                            <pre>from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Load in 8-bit
model = AutoModelForCausalLM.from_pretrained(
    "model/",
    load_in_8bit=True,
    device_map="auto"
)

# Load in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    "model/",
    load_in_4bit=True,
    device_map="auto"
)</pre>

                            <h4>4. llama.cpp (GGUF quantization)</h4>
                            <pre># Quantize to various levels
./quantize model-f16.gguf model-q4.gguf Q4_K_M
./quantize model-f16.gguf model-q5.gguf Q5_K_M
./quantize model-f16.gguf model-q8.gguf Q8_0</pre>

                            <h4>5. ONNX Runtime (INT8)</h4>
                            <pre>from optimum.onnxruntime import ORTQuantizer, ORTModelForCausalLM

# Quantize ONNX model
quantizer = ORTQuantizer.from_pretrained("model/")
quantizer.quantize(save_dir="model-int8/")</pre>
                        </div>

                        <div class="example-box">
                            <strong>Quantization Workflow:</strong>
                            <pre>
1. Start with full precision model (FP16)
   Size: 13.5 GB

2. Choose quantization method based on target:
   - GPU inference â†’ GPTQ or AWQ (4-bit)
   - CPU inference â†’ GGUF (llama.cpp)
   - Production â†’ INT8 (ONNX or bitsandbytes)

3. Quantize using appropriate tool
   ./quantize model-f16.gguf model-q4.gguf Q4_K_M

4. Test quality on validation set
   Compare perplexity, accuracy metrics

5. Deploy quantized model
   Size: 3.5 GB (4x smaller!)
   Speed: 2-3x faster
    Quality: often close to original on many tasks (always validate)
</pre>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Which quantization should I use?</strong><br>
                            <strong>For GPU Inference:</strong>
                            <ul>
                                <li>Best quality: AWQ (4-bit) or INT8</li>
                                <li>Best balance: GPTQ (4-bit)</li>
                                <li>Extreme compression: GPTQ (3-bit)</li>
                            </ul>
                            <strong>For CPU Inference:</strong>
                            <ul>
                                <li>Recommended: GGUF Q4_K_M or Q5_K_M</li>
                                <li>Best quality: GGUF Q8_0</li>
                                <li>Smallest: GGUF Q2_K</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>5.5 Full Parameter Fine-Tuning (FPFT)</h3>
                        
                        <div class="mental-model">
                            <strong>When to use FPFT:</strong>
                            <ul>
                                <li>You need maximum customization</li>
                                <li>You have large, high-quality dataset (100K+ examples)</li>
                                <li>You have significant compute budget</li>
                                <li>Domain is very different from pre-training (e.g., medical, legal)</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>FPFT Requirements (Llama-2-7B):</strong>
                            <pre>
Hardware: typically 1â€“8 high-memory GPUs (varies by sequence length & batch size)
Memory: often tens to 100+ GB VRAM total (optimizer + activations dominate)
Time: hours to days (dataset size + hardware dependent)
Cost: ranges from tens of dollars to thousands+ (hardware + time dependent)
Data: 100K-1M high-quality examples

<strong>Why so expensive?</strong>
- Must store: Model (14GB) + Gradients (14GB) + Optimizer states (28GB)
- Must update all 7 billion parameters
- Requires multiple epochs over large dataset
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>FPFT with Transformers:</strong>
                            <pre>from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,  # Effective batch = 16
    learning_rate=2e-5,
    fp16=True,                      # Use FP16 to save memory
    gradient_checkpointing=True,    # Trade compute for memory
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()</pre>
                        </div>

                        <table>
                            <tr>
                                <th>Technique</th>
                                <th>When to Use</th>
                                <th>Cost</th>
                                <th>Quality</th>
                            </tr>
                            <tr>
                                <td><strong>Prompt Engineering</strong></td>
                                <td>Quick prototyping, few examples</td>
                                <td>Free</td>
                                <td>â­â­â­</td>
                            </tr>
                            <tr>
                                <td><strong>QLoRA</strong></td>
                                <td>Best starting point for most</td>
                                <td>$</td>
                                <td>â­â­â­â­</td>
                            </tr>
                            <tr>
                                <td><strong>LoRA</strong></td>
                                <td>More resources, better quality</td>
                                <td>$$</td>
                                <td>â­â­â­â­â­</td>
                            </tr>
                            <tr>
                                <td><strong>Full Fine-Tuning</strong></td>
                                <td>Maximum customization, large dataset</td>
                                <td>$$$$</td>
                                <td>â­â­â­â­â­</td>
                            </tr>
                        </table>
                    </div>
                </div>

                <!-- SECTION 6: ADVANCED -->
                <div id="advanced" class="section">
                    <h2>ğŸš€ 6. Advanced Concepts: Beyond Basic LLMs</h2>
                    
                    <div class="must-know">
                        <strong>Evolution of AI:</strong> LLMs alone have limitations (hallucination, outdated knowledge, no tools). Advanced techniques extend capabilities: RAG (external knowledge), RLHF (human alignment), Agents (autonomous action).
                    </div>

                    <div class="concept-card">
                        <h3>6.1 <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> (Retrieval-Augmented Generation)</h3>
                        
                        <div class="mental-model">
                            <strong>The Problem RAG Solves:</strong>
                            <ul>
                                <li>LLMs have fixed knowledge (from training data)</li>
                                <li>Can't access recent information</li>
                                <li>Can't access private/proprietary data</li>
                                <li>Hallucinate when uncertain</li>
                            </ul>
                            <p><strong>RAG Solution:</strong> Retrieve relevant documents â†’ Augment prompt â†’ Generate answer</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>RAG Pipeline:</strong>

User Query: "What is our Q4 revenue?"
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. EMBEDDING: Query â†’ Vector           â”‚
â”‚  "Q4 revenue" â†’ [0.23, -0.45, 0.67, ...]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. VECTOR DATABASE: Similarity Search   â”‚
â”‚  Find most similar documents             â”‚
â”‚                                           â”‚
â”‚  Documents (embedded):                   â”‚
â”‚  â€¢ Q4 Financial Report [0.24, -0.44, ...] â† 95% match!â”‚
â”‚  â€¢ Q3 Revenue Summary [0.12, -0.67, ...] â† 72% match  â”‚
â”‚  â€¢ Annual Report [0.45, 0.23, ...]      â† 68% match  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. RETRIEVAL: Get top K documents       â”‚
â”‚  Retrieved: Q4 Financial Report          â”‚
â”‚  Content: "Q4 revenue was $2.3M..."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. AUGMENTATION: Build prompt           â”‚
â”‚  Context: [Q4 Financial Report content]  â”‚
â”‚  Question: "What is our Q4 revenue?"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. GENERATION: LLM generates answer     â”‚
â”‚  Output: "Q4 revenue was $2.3M,          â”‚
â”‚  up 15% from Q3..."                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: RAG Pipeline Flow</em>
                            </p>
                            <button onclick="animateRAG()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Start RAG Flow</button>
                            <div id="ragPipeline" class="rag-pipeline" style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 10px; padding: 20px; background: linear-gradient(135deg, #f8fafc, #e2e8f0); border-radius: 8px; min-height: 150px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch the complete RAG workflow from query to response</p>
                        </div>

                        <div class="example-box">
                            <strong>RAG vs Fine-Tuning:</strong>
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>RAG</th>
                                    <th>Fine-Tuning</th>
                                </tr>
                                <tr>
                                    <td><strong>Knowledge Update</strong></td>
                                    <td>âœ… Real-time (update documents)</td>
                                    <td>âŒ Requires retraining</td>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td>ğŸ’° Low (vector DB + inference)</td>
                                    <td>ğŸ’°ğŸ’°ğŸ’° High (GPU training)</td>
                                </tr>
                                <tr>
                                    <td><strong>Setup Time</strong></td>
                                    <td>âš¡ Minutes to hours</td>
                                    <td>ğŸŒ Hours to days</td>
                                </tr>
                                <tr>
                                    <td><strong>Transparency</strong></td>
                                    <td>âœ… Can cite sources</td>
                                    <td>âŒ Knowledge in weights</td>
                                </tr>
                                <tr>
                                    <td><strong>Domain Adaptation</strong></td>
                                    <td>â­â­â­ Good for facts</td>
                                    <td>â­â­â­â­â­ Best for style/behavior</td>
                                </tr>
                            </table>
                        </div>

                        <h4>Vector Databases for RAG</h4>
                        <table>
                            <tr>
                                <th>Database</th>
                                <th>Type</th>
                                <th>Best For</th>
                                <th>Standout Feature</th>
                            </tr>
                            <tr>
                                <td><strong>Pinecone</strong></td>
                                <td>Managed</td>
                                <td>Production, scale</td>
                                <td>Fully managed, fast</td>
                            </tr>
                            <tr>
                                <td><strong>Weaviate</strong></td>
                                <td>Open source</td>
                                <td>Hybrid search</td>
                                <td>GraphQL API, modules</td>
                            </tr>
                            <tr>
                                <td><strong>Qdrant</strong></td>
                                <td>Open source</td>
                                <td>High performance</td>
                                <td>Rust-based, fast filtering</td>
                            </tr>
                            <tr>
                                <td><strong>ChromaDB</strong></td>
                                <td>Open source</td>
                                <td>Local development</td>
                                <td>Simple, embeds in app</td>
                            </tr>
                            <tr>
                                <td><strong>FAISS</strong></td>
                                <td>Library</td>
                                <td>Research, prototyping</td>
                                <td>Meta's library, flexible</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Building a RAG System:</strong>
                            <pre># 1. Install dependencies
pip install langchain chromadb openai sentence-transformers

# 2. Load and chunk documents
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

loader = TextLoader("docs.txt")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Size of each chunk
    chunk_overlap=200       # Overlap to maintain context
)
chunks = text_splitter.split_documents(documents)

# 3. Create embeddings and store in vector DB
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# 4. Create RAG chain
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(k=3),  # Retrieve top 3
    return_source_documents=True
)

# 5. Query
result = qa_chain({"query": "What is our Q4 revenue?"})
print(result["result"])
print("Sources:", result["source_documents"])</pre>

                            <strong>Advanced: Semantic Search Visualization</strong>
                            <pre># How semantic search works
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# Documents in database
docs = [
    "The cat sat on the mat",
    "Python programming tutorial",
    "Feline resting on carpet"
]

# Embed documents
doc_embeddings = model.encode(docs)

# Query
query = "kitten on rug"
query_embedding = model.encode(query)

# Calculate cosine similarity
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity([query_embedding], doc_embeddings)

print(similarities)
# Output: [[0.82, 0.23, 0.88]]  â† doc 3 is most similar!</pre>
                        </div>

                        <div class="diagram">
<pre>
<strong>Semantic Search in Vector Space (2D visualization):</strong>

            |
     "cat"  â€¢ "feline"
            |  â€¢ "kitten" â† Query
            |
     "dog"  â€¢
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            |
            | â€¢ "Python"
            |     â€¢ "code"
            |

Distance in vector space = Semantic similarity!
"kitten" is close to "cat", "feline" (high similarity)
"kitten" is far from "Python", "code" (low similarity)
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.2 <span class="tooltip">RLHF<span class="tooltiptext">Reinforcement Learning from Human Feedback</span></span> (Reinforcement Learning from Human Feedback)</h3>
                        
                        <div class="mental-model">
                            <strong>The Alignment Problem:</strong>
                            <p>Pre-trained LLMs predict next tokens, but that doesn't mean helpful/safe/aligned responses!</p>
                            <p><strong>RLHF</strong> teaches models what humans actually want through feedback.</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>RLHF Pipeline (3 Stages):</strong>

Stage 1: Supervised Fine-Tuning (SFT)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Human demonstrations                 â”‚
â”‚ Q: "How do I bake bread?"           â”‚
â”‚ A: [High-quality human answer]      â”‚
â”‚                                      â”‚
â”‚ Train model to mimic humans          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Stage 2: Reward Model Training
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Humans rank multiple outputs         â”‚
â”‚ Q: "Explain quantum physics"         â”‚
â”‚ Output A: [detailed] â­â­â­â­â­          â”‚
â”‚ Output B: [brief] â­â­â­               â”‚
â”‚ Output C: [wrong] â­                  â”‚
â”‚                                      â”‚
â”‚ Train reward model to predict rating â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Stage 3: Reinforcement Learning (PPO)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM generates response               â”‚
â”‚ â†’ Reward model scores it             â”‚
â”‚ â†’ Update LLM to maximize reward      â”‚
â”‚                                      â”‚
â”‚ Repeat thousands of times            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Aligned Model! ğŸ¯
</pre>
                        </div>

                        <div class="example-box">
                            <strong>RLHF in Action (GPT-4):</strong>
                            <pre>
<strong>Before RLHF:</strong>
User: "How do I hack into a website?"
Model: "Here's a step-by-step guide..." âŒ

<strong>After RLHF:</strong>
User: "How do I hack into a website?"
Model: "I can't help with that. However, I can explain ethical security testing..." âœ…

<strong>Impact:</strong>
- More helpful, harmless, honest responses
- Follows instructions better
- Reduces harmful content
- Better conversation flow
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>RLHF Libraries:</strong>
                            <ul>
                                <li><strong>trl</strong> (Transformer Reinforcement Learning) - HuggingFace
                                    <pre>pip install trl</pre>
                                </li>
                                <li><strong>RL4LMs</strong> - Research framework</li>
                                <li><strong>DeepSpeed-Chat</strong> - Microsoft's RLHF</li>
                            </ul>

                            <pre>from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from transformers import AutoTokenizer

# Load model with value head for RL
model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# PPO config
config = PPOConfig(batch_size=8, learning_rate=1.41e-5)

# Create trainer
ppo_trainer = PPOTrainer(config, model, ref_model=None, tokenizer=tokenizer)

# Training loop
for batch in dataloader:
    query_tensors = batch["input_ids"]
    response_tensors = ppo_trainer.generate(query_tensors, ...)
    rewards = compute_rewards(response_tensors)  # From reward model
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.3 AI Agents: From Passive to Active</h3>
                        
                        <div class="must-know">
                            <strong>Key Distinction:</strong>
                            <ul>
                                <li><strong>LLM:</strong> Takes input â†’ Generates output (passive)</li>
                                <li><strong>Agent:</strong> Observes â†’ Reasons â†’ Takes Actions â†’ Repeats (active)</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>Agent = LLM + Tools + Memory + Planning</strong>
                        </div>

                        <div class="diagram">
<pre>
<strong>Agent Architecture:</strong>

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚      Environment            â”‚
          â”‚  (External World, APIs)     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†‘           â†“
            Observation   Action
                 â”‚           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
          â”‚       AGENT              â”‚
          â”‚                          â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚   Perception       â”‚ â”‚ â† Process observations
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚           â†“              â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚   LLM (Brain)      â”‚ â”‚ â† Reasoning
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚     â†“           â†‘        â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚Toolsâ”‚    â”‚ Memory â”‚ â”‚ â† Context
          â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚     â†“                   â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚   Action Selection â”‚ â”‚ â† Choose what to do
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Agent in Action - Travel Planner:</strong>
                            <pre>
User: "Plan a 3-day trip to Paris"

Agent Workflow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. PERCEIVE: User wants Paris trip, 3 days  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PLAN: Break into sub-tasks               â”‚
â”‚    - Find flights                            â”‚
â”‚    - Book hotel                              â”‚
â”‚    - Create itinerary                        â”‚
â”‚    - Check weather                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ACT: Execute with tools                   â”‚
â”‚    Tool: search_flights("Paris", "3 days")   â”‚
â”‚    Result: Found flights $450                â”‚
â”‚                                               â”‚
â”‚    Tool: search_hotels("Paris", "3 nights")  â”‚
â”‚    Result: Found hotel $300/night            â”‚
â”‚                                               â”‚
â”‚    Tool: get_weather("Paris", "next week")   â”‚
â”‚    Result: Sunny, 22Â°C                       â”‚
â”‚                                               â”‚
â”‚    Tool: create_itinerary(...)               â”‚
â”‚    Result: Day-by-day plan                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. RESPOND: Synthesize results               â”‚
â”‚    "Here's your Paris trip plan:             â”‚
â”‚     - Flights: $450 ...                      â”‚
â”‚     - Hotel: $900 ...                        â”‚
â”‚     - Itinerary: [detailed plan]"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <h4>Types of Agents</h4>
                        <table>
                            <tr>
                                <th>Type</th>
                                <th>Description</th>
                                <th>Example</th>
                            </tr>
                            <tr>
                                <td><strong>ReAct Agent</strong></td>
                                <td>Reason + Act iteratively</td>
                                <td>LangChain agents, most common</td>
                            </tr>
                            <tr>
                                <td><strong>Plan-and-Execute</strong></td>
                                <td>Plan first, then execute steps</td>
                                <td>BabyAGI, AutoGPT</td>
                            </tr>
                            <tr>
                                <td><strong>Multi-Agent</strong></td>
                                <td>Multiple agents collaborate</td>
                                <td>AutoGen, CrewAI</td>
                            </tr>
                            <tr>
                                <td><strong>Tool-Use Agent</strong></td>
                                <td>Specialized in using external tools</td>
                                <td>Function calling (GPT-4)</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Building Agents:</strong>
                            
                            <h4>1. LangChain ReAct Agent</h4>
                            <pre>from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI

# Define tools
tools = [
    Tool(
        name="Calculator",
        func=lambda x: eval(x),
        description="Useful for math calculations"
    ),
    Tool(
        name="Wikipedia",
        func=search_wikipedia,
        description="Search Wikipedia for information"
    )
]

# Create agent
agent = initialize_agent(
    tools,
    OpenAI(temperature=0),
    agent="zero-shot-react-description",
    verbose=True
)

# Run
result = agent.run("What is 25% of 180, and who invented calculus?")</pre>

                            <h4>2. AutoGen (Multi-Agent)</h4>
                            <pre>from autogen import AssistantAgent, UserProxyAgent

# Create agents
assistant = AssistantAgent("assistant")
user_proxy = UserProxyAgent("user")

# Start conversation
user_proxy.initiate_chat(
    assistant,
    message="Analyze this dataset and create visualization"
)</pre>

                            <h4>3. CrewAI (Role-Based Agents)</h4>
                            <pre>from crewai import Agent, Task, Crew

# Define agents
researcher = Agent(
    role="Researcher",
    goal="Find information about AI",
    tools=[search_tool]
)

writer = Agent(
    role="Writer",
    goal="Write blog post",
    tools=[writing_tool]
)

# Create crew
crew = Crew(agents=[researcher, writer], tasks=[task1, task2])
result = crew.kickoff()</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.4 Agentic <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> & <span class="tooltip">MCP<span class="tooltiptext">Model Context Protocol</span></span> (Model Context Protocol)</h3>
                        
                        <div class="mental-model">
                            <strong>Agentic AI = Next Evolution</strong>
                            <p>Not just answering questions, but autonomously completing complex tasks end-to-end.</p>
                        </div>

                        <h4>What is MCP (Model Context Protocol)?</h4>
                        <div class="must-know">
                            <strong>MCP</strong> is an open protocol (by Anthropic) that standardizes how AI models connect to external data sources and tools.
                            <p><strong>Problem it solves:</strong> Every AI app reinvents how to connect models to tools. MCP provides a universal interface.</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>MCP Architecture:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AI Model (Claude, GPT)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†• MCP Protocol
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            MCP Server                     â”‚
â”‚  (Standardized interface for resources)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“          â†“          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Databaseâ”‚ â”‚   API   â”‚ â”‚  Files  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>MCP Components:</strong>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Resources: Data sources (files, DBs) â”‚
â”‚ 2. Tools: Actions model can take        â”‚
â”‚ 3. Prompts: Templates for common tasks   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>MCP Benefits:</strong>
                            <ul>
                                <li><strong>Standardization:</strong> One protocol, many tools</li>
                                <li><strong>Security:</strong> Controlled access to resources</li>
                                <li><strong>Composability:</strong> Mix and match MCP servers</li>
                                <li><strong>Flexibility:</strong> Add new tools without changing model code</li>
                            </ul>

                            <strong>Example MCP Servers:</strong>
                            <pre>
mcp-server-filesystem   â†’ Access local files
mcp-server-postgres     â†’ Query databases
mcp-server-github       â†’ GitHub API
mcp-server-web-search   â†’ Web search
mcp-server-slack        â†’ Slack integration
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Using MCP with Claude Desktop:</strong>
                            <pre># 1. Install MCP server
npm install -g @anthropic/mcp-server-filesystem

# 2. Configure Claude Desktop (claude_desktop_config.json)
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["@anthropic/mcp-server-filesystem", "/path/to/allowed/files"]
    }
  }
}

# 3. Claude can now access files!
User: "Read the file analysis.txt"
Claude: [Uses MCP to read file] â†’ Provides answer</pre>

                            <strong>Building Custom MCP Server:</strong>
                            <pre>import { MCPServer } from "@anthropic/mcp";

const server = new MCPServer({
  name: "my-database-server",
  version: "1.0.0"
});

// Define a tool
server.addTool({
  name: "query_db",
  description: "Query the database",
  inputSchema: {
    type: "object",
    properties: {
      query: { type: "string" }
    }
  },
  handler: async (input) => {
    const result = await db.query(input.query);
    return result;
  }
});

server.start();</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.5 Memory Architectures for Agents</h3>
                        
                        <div class="mental-model">
                            <strong>Why Memory Matters:</strong>
                            <p>LLMs are stateless - they forget everything after each request. Agents need memory for:</p>
                            <ul>
                                <li>Maintaining conversation context</li>
                                <li>Learning from past interactions</li>
                                <li>Building long-term knowledge</li>
                            </ul>
                        </div>

                        <h4>Types of Memory</h4>
                        <table>
                            <tr>
                                <th>Type</th>
                                <th>Duration</th>
                                <th>Purpose</th>
                                <th>Implementation</th>
                            </tr>
                            <tr>
                                <td><strong>Short-Term</strong></td>
                                <td>Single conversation</td>
                                <td>Context window</td>
                                <td>Recent messages in prompt</td>
                            </tr>
                            <tr>
                                <td><strong>Working Memory</strong></td>
                                <td>Current task</td>
                                <td>Track progress</td>
                                <td>Variables, state dict</td>
                            </tr>
                            <tr>
                                <td><strong>Long-Term</strong></td>
                                <td>Persistent</td>
                                <td>Historical knowledge</td>
                                <td>Vector DB, SQL database</td>
                            </tr>
                            <tr>
                                <td><strong>Episodic</strong></td>
                                <td>Past interactions</td>
                                <td>Learn from experience</td>
                                <td>Summarized conversations</td>
                            </tr>
                        </table>

                        <div class="diagram">
<pre>
<strong>Memory Architecture:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Short-Term (Context Window)         â”‚
â”‚   Last 10 messages, current conversation   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ Store summaries
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Working Memory (Task State)          â”‚
â”‚   Current goals, progress, variables       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ Persist important info
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Long-Term Memory (Vector DB)         â”‚
â”‚   All past knowledge, searchable by query  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Implementing Memory:</strong>
                            <pre>from langchain.memory import ConversationBufferMemory, VectorStoreMemory
from langchain.chains import ConversationChain

# Short-term memory
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory)

conversation.predict(input="Hi!")
conversation.predict(input="What did I just say?")  # Remembers!

# Long-term vector memory
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

vectorstore = FAISS.from_texts([], OpenAIEmbeddings())
memory = VectorStoreMemory(vectorstore=vectorstore)

# Automatically stores and retrieves relevant context</pre>
                        </div>
                    </div>
                </div>

                <!-- SECTION 6.6: TOOLS ECOSYSTEM -->
                <div id="tools" class="section">
                    <h2>ğŸ› ï¸ 6.6. Essential Tools Ecosystem</h2>
                    
                    <div class="must-know">
                        <strong>Why This Matters:</strong> The <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> landscape has hundreds of tools. This section focuses on the most popular, actively maintained tools that professionals actually use in production.
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ“ Model Training & Fine-Tuning</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Purpose</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td><strong>PyTorch</strong></td>
                                    <td>Deep learning framework</td>
                                    <td>Research, custom architectures, flexibility</td>
                                </tr>
                                <tr>
                                    <td><strong>Transformers (HuggingFace)</strong></td>
                                    <td>Pre-trained model library</td>
                                    <td>Quick start with SOTA models</td>
                                </tr>
                                <tr>
                                    <td><strong><span class="tooltip">PEFT<span class="tooltiptext">Parameter-Efficient Fine-Tuning</span></span></strong></td>
                                    <td><span class="tooltip">LoRA<span class="tooltiptext">Low-Rank Adaptation</span></span>, QLoRA implementations</td>
                                    <td>Memory-efficient fine-tuning</td>
                                </tr>
                                <tr>
                                    <td><strong>Unsloth</strong></td>
                                    <td>2-5x faster fine-tuning</td>
                                    <td>Fast <span class="tooltip">LoRA<span class="tooltiptext">Low-Rank Adaptation</span></span>/QLoRA on consumer <span class="tooltip">GPUs<span class="tooltiptext">Graphics Processing Units</span></span></td>
                                </tr>
                                <tr>
                                    <td><strong>Axolotl</strong></td>
                                    <td>One-stop fine-tuning tool</td>
                                    <td>Complete training pipeline with YAML configs</td>
                                </tr>
                                <tr>
                                    <td><strong>Accelerate</strong></td>
                                    <td>HuggingFace training library</td>
                                    <td>Multi-<span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span>, mixed precision, easy scaling</td>
                                </tr>
                                <tr>
                                    <td><strong>DeepSpeed</strong></td>
                                    <td>Distributed training optimizer</td>
                                    <td>Multi-<span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> training, ZeRO optimization</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: Fine-Tuning with Unsloth</strong>
                            <pre>
pip install unsloth

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"]
)

# Train 2x faster!
trainer.train()</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>âš¡ Inference Engines</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Format</th>
                                    <th>Speed</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td><strong>vLLM</strong></td>
                                    <td>SafeTensors</td>
                                    <td>âš¡âš¡âš¡</td>
                                    <td><span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> inference, production APIs</td>
                                </tr>
                                <tr>
                                    <td><strong>llama.cpp</strong></td>
                                    <td><span class="tooltip">GGUF<span class="tooltiptext">GPT-Generated Unified Format</span></span></td>
                                    <td>âš¡âš¡</td>
                                    <td><span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span> inference, consumer hardware</td>
                                </tr>
                                <tr>
                                    <td><strong>Ollama</strong></td>
                                    <td>GGUF</td>
                                    <td>âš¡âš¡</td>
                                    <td>Local development, easy setup</td>
                                </tr>
                                <tr>
                                    <td><strong><span class="tooltip">TGI<span class="tooltiptext">Text Generation Inference</span></span></strong></td>
                                    <td>SafeTensors</td>
                                    <td>âš¡âš¡âš¡</td>
                                    <td>Production GPU serving (HuggingFace)</td>
                                </tr>
                                <tr>
                                    <td><strong>LM Studio</strong></td>
                                    <td>GGUF</td>
                                    <td>âš¡âš¡</td>
                                    <td>Desktop app with <span class="tooltip">GUI<span class="tooltiptext">Graphical User Interface</span></span></td>
                                </tr>
                                <tr>
                                    <td><strong>TensorRT-LLM</strong></td>
                                    <td>SafeTensors</td>
                                    <td>âš¡âš¡âš¡</td>
                                    <td>NVIDIA optimization, maximum speed</td>
                                </tr>
                                <tr>
                                    <td><strong>ONNX Runtime</strong></td>
                                    <td>ONNX</td>
                                    <td>âš¡âš¡</td>
                                    <td>Cross-platform, production deployment</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: vLLM for Fast GPU Inference</strong>
                            <pre>
pip install vllm

from vllm import LLM, SamplingParams

# Load model
llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")

# Configure generation
sampling = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)

# Generate (batched!)
outputs = llm.generate(["Tell me about AI"], sampling)

# 10-20x faster than vanilla HuggingFace!</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ”¢ Quantization Tools</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Method</th>
                                    <th>Output</th>
                                </tr>
                                <tr>
                                    <td><strong>llama.cpp quantize</strong></td>
                                    <td>K-quants (Q4_K_M, Q5_K_M)</td>
                                    <td>GGUF files</td>
                                </tr>
                                <tr>
                                    <td><strong>AutoGPTQ</strong></td>
                                    <td>GPTQ (4-bit)</td>
                                    <td>SafeTensors</td>
                                </tr>
                                <tr>
                                    <td><strong>AutoAWQ</strong></td>
                                    <td><span class="tooltip">AWQ<span class="tooltiptext">Activation-aware Weight Quantization</span></span> (4-bit)</td>
                                    <td>SafeTensors</td>
                                </tr>
                                <tr>
                                    <td><strong>bitsandbytes</strong></td>
                                    <td>8-bit, 4-bit on-the-fly</td>
                                    <td>Runtime quantization</td>
                                </tr>
                                <tr>
                                    <td><strong>Optimum</strong></td>
                                    <td>ONNX quantization</td>
                                    <td>HuggingFace optimization toolkit</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: Quantize to GGUF</strong>
                            <pre>
# 1. Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# 2. Convert HuggingFace to GGUF
python convert.py /path/to/model --outfile model-f16.gguf

# 3. Quantize
./quantize model-f16.gguf model-Q4_K_M.gguf Q4_K_M

# Result: 13GB â†’ 4GB model!</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ“š <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> & Vector Databases</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Category</th>
                                    <th>Tool</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td rowspan="2"><strong>RAG Frameworks</strong></td>
                                    <td>LangChain</td>
                                    <td>Mature ecosystem, lots of integrations</td>
                                </tr>
                                <tr>
                                    <td>LlamaIndex</td>
                                    <td>Data-focused, better indexing strategies</td>
                                </tr>
                                <tr>
                                    <td rowspan="3"><strong>Vector DBs</strong></td>
                                    <td>Pinecone</td>
                                    <td>Managed cloud service, scalable</td>
                                </tr>
                                <tr>
                                    <td>Qdrant</td>
                                    <td>Self-hosted, Rust-based, fast</td>
                                </tr>
                                <tr>
                                    <td>Chroma</td>
                                    <td>Embedded, easy local development</td>
                                </tr>
                                <tr>
                                    <td rowspan="2"><strong>Additional DBs</strong></td>
                                    <td>Weaviate</td>
                                    <td>Hybrid search (vector + keyword)</td>
                                </tr>
                                <tr>
                                    <td>FAISS</td>
                                    <td>Facebook's library, research/prototyping</td>
                                </tr>
                                <tr>
                                    <td><strong>Embeddings</strong></td>
                                    <td>Sentence Transformers</td>
                                    <td>Local embedding models</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: RAG with LangChain + Chroma</strong>
                            <pre>
pip install langchain chromadb sentence-transformers

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load documents
loader = TextLoader("docs.txt")
documents = loader.load()

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500)
chunks = splitter.split_documents(documents)

# Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store in Chroma
vectorstore = Chroma.from_documents(chunks, embeddings)

# Query
results = vectorstore.similarity_search("What is AI?", k=3)</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ¤– Agent Frameworks</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Framework</th>
                                    <th>Type</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td><strong>LangChain Agents</strong></td>
                                    <td>ReAct, Tools</td>
                                    <td>Quick prototypes, <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> chaining</td>
                                </tr>
                                <tr>
                                    <td><strong>AutoGen</strong></td>
                                    <td>Multi-agent</td>
                                    <td>Agent collaboration, conversations</td>
                                </tr>
                                <tr>
                                    <td><strong>CrewAI</strong></td>
                                    <td>Role-based</td>
                                    <td>Task delegation, team workflows</td>
                                </tr>
                                <tr>
                                    <td><strong>LangGraph</strong></td>
                                    <td>Graph-based</td>
                                    <td>Complex workflows, state management</td>
                                </tr>
                                <tr>
                                    <td><strong>Semantic Kernel</strong></td>
                                    <td>Microsoft SDK</td>
                                    <td>Enterprise .NET/Python apps</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: Simple Agent with LangChain</strong>
                            <pre>
from langchain.agents import create_react_agent, Tool
from langchain_openai import ChatOpenAI

# Define tools
def calculator(query):
    # âš ï¸ Demo only: never use eval() on untrusted input.
    return eval(query)

tools = [Tool(name="Calculator", func=calculator, 
              description="For math calculations")]

# Create agent
llm = ChatOpenAI(model="gpt-4")
agent = create_react_agent(llm, tools)

# Run
result = agent.invoke("What is 25 * 34?")
# Agent thinks: "I need Calculator tool"
# Runs: calculator("25 * 34")
# Returns: "850"</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸŒ Model Hosting Platforms</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Platform</th>
                                    <th>Type</th>
                                    <th>Pricing</th>
                                </tr>
                                <tr>
                                    <td><strong>HuggingFace</strong></td>
                                    <td>Model hub, free hosting</td>
                                    <td>Free tier + paid plans (varies)</td>
                                </tr>
                                <tr>
                                    <td><strong>Replicate</strong></td>
                                    <td><span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> for open models</td>
                                    <td>Pay per second</td>
                                </tr>
                                <tr>
                                    <td><strong>Together AI</strong></td>
                                    <td>Fast inference API</td>
                                    <td>Varies (priced per token / model)</td>
                                </tr>
                                <tr>
                                    <td><strong>Groq</strong></td>
                                    <td>Lightning fast LPUs</td>
                                    <td>Free tier + paid</td>
                                </tr>
                                <tr>
                                    <td><strong>Modal</strong></td>
                                    <td>Serverless GPU compute</td>
                                    <td>Varies (per-second GPU pricing)</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ’» Development Tools</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Purpose</th>
                                </tr>
                                <tr>
                                    <td><strong>Jupyter Notebook</strong></td>
                                    <td>Interactive development, experimentation</td>
                                </tr>
                                <tr>
                                    <td><strong>Weights & Biases</strong></td>
                                    <td>Experiment tracking, model metrics</td>
                                </tr>
                                <tr>
                                    <td><strong>TensorBoard</strong></td>
                                    <td>Visualization of training metrics</td>
                                </tr>
                                <tr>
                                    <td><strong>MLflow</strong></td>
                                    <td><span class="tooltip">ML<span class="tooltiptext">Machine Learning</span></span> lifecycle management, model registry</td>
                                </tr>
                                <tr>
                                    <td><strong>Docker</strong></td>
                                    <td>Containerization, deployment</td>
                                </tr>
                                <tr>
                                    <td><strong>FastAPI</strong></td>
                                    <td>Build production APIs quickly</td>
                                </tr>
                                <tr>
                                    <td><strong>Streamlit</strong></td>
                                    <td>Quick <span class="tooltip">UI<span class="tooltiptext">User Interface</span></span> prototypes for demos</td>
                                </tr>
                                <tr>
                                    <td><strong>Gradio</strong></td>
                                    <td>ML model demos and sharing</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                    <div class="must-know">
                        <strong>ğŸ’¡ Pro Tip:</strong> Don't try to learn everything at once! Start with PyTorch + Transformers for training, Ollama for local inference, and LangChain for basic agents. Add specialized tools as you need them.
                    </div>
                </div>

                <!-- SECTION 7: PRACTICAL -->
                <div id="practical" class="section">
                    <h2>ğŸ’» 7. Hands-On Roadmap: Zero to Agentic AI</h2>
                    
                    <div class="must-know">
                        <strong>Learning Path:</strong> This roadmap takes you from beginner to building production AI agents. Each phase builds on the previous one. Estimated timeline: 3-6 months with consistent practice.
                    </div>

                    <div class="concept-card">
                        <h3>Phase 1: Foundations (Weeks 1-2)</h3>
                        
                        <h4>Goal: Understand basics of Python, ML, and tensors</h4>
                        
                        <div class="example-box">
                            <strong>Week 1: Python & NumPy</strong>
                            <pre>
# Learn NumPy for tensor operations
import numpy as np

# Create tensors
vector = np.array([1, 2, 3])
matrix = np.array([[1, 2], [3, 4]])
tensor = np.zeros((2, 3, 4))  # 3D tensor

# Matrix operations
A = np.random.rand(3, 3)
B = np.random.rand(3, 3)
C = np.dot(A, B)  # Matrix multiplication

<strong>Practice Projects:</strong>
1. Implement basic linear algebra operations
2. Build a simple neural network from scratch (no libraries)
3. Understand backpropagation manually
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Week 2: PyTorch Basics</strong>
                            <pre>
import torch
import torch.nn as nn

# Tensors on GPU
x = torch.tensor([1, 2, 3]).cuda()

# Simple neural network
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleNet()

<strong>Practice Projects:</strong>
1. MNIST digit classification
2. Image classification with CNN
3. Understand training loops, loss functions
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Resources:</strong>
                            <ul>
                                <li>Fast.ai - Practical Deep Learning (free)</li>
                                <li>PyTorch Tutorials - pytorch.org</li>
                                <li>3Blue1Brown - Neural Networks YouTube series</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 2: LLMs & Transformers (Weeks 3-4)</h3>
                        
                        <h4>Goal: Understand and use pre-trained LLMs</h4>
                        
                        <div class="example-box">
                            <strong>Week 3: HuggingFace Transformers</strong>
                            <pre>
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Generate text
inputs = tokenizer("The future of AI is", return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))

<strong>Practice Projects:</strong>
1. Text generation with different models
2. Compare tokenizers (BPE vs WordPiece)
3. Experiment with generation parameters (temperature, top-p)
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Week 4: Understanding Attention</strong>
                            <pre>
# Visualize attention weights
from transformers import AutoModel
import torch

model = AutoModel.from_pretrained("bert-base-uncased", output_attentions=True)
tokens = tokenizer("The cat sat on the mat", return_tensors="pt")

with torch.no_grad():
    outputs = model(**tokens)
    attentions = outputs.attentions  # Attention weights from all layers

# Plot attention patterns
import matplotlib.pyplot as plt
# Visualize which words attend to which

<strong>Practice Projects:</strong>
1. Build transformer from scratch (simplified)
2. Visualize attention patterns
3. Compare BERT vs GPT architectures
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Resources:</strong>
                            <ul>
                                <li>Andrej Karpathy - GPT from Scratch (YouTube)</li>
                                <li>The Illustrated Transformer (Jay Alammar)</li>
                                <li>HuggingFace Course - huggingface.co/course</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 3: Fine-Tuning & Optimization (Weeks 5-6)</h3>
                        
                        <h4>Goal: Customize models for your use case</h4>
                        
                        <div class="example-box">
                            <strong>Week 5: LoRA Fine-Tuning</strong>
                            <pre>
from peft import LoraConfig, get_peft_model
from transformers import Trainer, TrainingArguments

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")

# Add LoRA adapters
lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"])
model = get_peft_model(model, lora_config)

# Prepare dataset
from datasets import load_dataset
dataset = load_dataset("your-dataset")

# Train
training_args = TrainingArguments(...)
trainer = Trainer(model=model, args=training_args, train_dataset=dataset)
trainer.train()

# Save adapters
model.save_pretrained("my-lora-adapters/")

<strong>Practice Projects:</strong>
1. Fine-tune for specific domain (medical, legal, code)
2. Compare LoRA vs QLoRA
3. Measure before/after performance
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Week 6: Quantization & Deployment</strong>
                            <pre>
# Quantize model with llama.cpp
./quantize model-f16.gguf model-q4.gguf Q4_K_M

# Deploy with Ollama
ollama create mymodel -f Modelfile
ollama run mymodel

# Or deploy with vLLM for production
from vllm import LLM
llm = LLM(model="mymodel", quantization="awq")
output = llm.generate("Hello", sampling_params=...)

<strong>Practice Projects:</strong>
1. Quantize models at different levels (Q4, Q5, Q8)
2. Benchmark inference speed
3. Deploy API endpoint with FastAPI
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 4: RAG & Vector Databases (Weeks 7-8)</h3>
                        
                        <h4>Goal: Build knowledge-enhanced AI applications</h4>
                        
                        <div class="example-box">
                            <strong>Complete RAG System</strong>
                            <pre>
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# 1. Load documents
loader = DirectoryLoader('./docs', glob="**/*.txt")
documents = loader.load()

# 2. Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# 3. Create embeddings and store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory="./chroma_db")

# 4. Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# 5. Query
result = qa_chain({"query": "What is the revenue?"})
print(result["result"])
print("Sources:", result["source_documents"])

<strong>Practice Projects:</strong>
1. Build RAG system for your documents
2. Experiment with different chunking strategies
3. Compare vector databases (Pinecone, Qdrant, Chroma)
4. Implement hybrid search (semantic + keyword)
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 5: AI Agents (Weeks 9-10)</h3>
                        
                        <h4>Goal: Build autonomous AI agents with tool use</h4>
                        
                        <div class="example-box">
                            <strong>LangChain ReAct Agent</strong>
                            <pre>
from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI
import requests

# Define custom tools
def search_web(query):
    # Use SerpAPI or similar
    api_key = "your-api-key"
    response = requests.get(f"https://serpapi.com/search?q={query}&api_key={api_key}")
    return response.json()["organic_results"][0]["snippet"]

def calculate(expression):
    return eval(expression)

def get_weather(city):
    # Use OpenWeatherMap API
    return f"Weather in {city}: Sunny, 22Â°C"

tools = [
    Tool(name="Search", func=search_web, description="Search the web"),
    Tool(name="Calculator", func=calculate, description="Perform calculations"),
    Tool(name="Weather", func=get_weather, description="Get weather info")
]

# Create agent
agent = initialize_agent(
    tools,
    OpenAI(temperature=0),
    agent="zero-shot-react-description",
    verbose=True
)

# Run complex query
result = agent.run(
    "What's the weather in Paris, and how much would 5 nights cost if hotels are $150/night?"
)

<strong>Practice Projects:</strong>
1. Build agent with custom tools (API calls, database queries)
2. Implement ReAct, Plan-and-Execute patterns
3. Create multi-agent system with AutoGen
4. Build personal assistant agent
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 6: Agentic AI & MCP (Weeks 11-12)</h3>
                        
                        <h4>Goal: Build production-ready agentic systems</h4>
                        
                        <div class="example-box">
                            <strong>MCP Server Setup</strong>
                            <pre>
# 1. Create custom MCP server
import { MCPServer } from "@anthropic/mcp";

const server = new MCPServer({
  name: "my-company-server",
  version: "1.0.0"
});

// Add database tool
server.addTool({
  name: "query_sales_db",
  description: "Query our sales database",
  inputSchema: {
    type: "object",
    properties: {
      query: { type: "string", description: "SQL query" }
    },
    required: ["query"]
  },
  handler: async (input) => {
    const result = await db.query(input.query);
    return { results: result.rows };
  }
});

// Add file system access
server.addResource({
  uri: "file:///company/reports",
  name: "Company Reports",
  description: "Access to company report files"
});

server.start();

<strong>Practice Projects:</strong>
1. Build MCP server for your company's data
2. Create agent that uses multiple MCP servers
3. Implement security and access control
4. Deploy production agent system
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ¯ Capstone Projects</h3>
                        
                        <div class="example-box">
                            <strong>Beginner:</strong>
                            <ul>
                                <li>ğŸ“ Personal document Q&A chatbot (RAG)</li>
                                <li>ğŸ’¬ Customer support chatbot with fine-tuning</li>
                                <li>ğŸ“Š Data analysis assistant</li>
                            </ul>

                            <strong>Intermediate:</strong>
                            <ul>
                                <li>ğŸ¤– Research assistant agent (web search + summarization)</li>
                                <li>ğŸ“§ Email automation agent</li>
                                <li>ğŸ” Code review assistant</li>
                            </ul>

                            <strong>Advanced:</strong>
                            <ul>
                                <li>ğŸ¢ Enterprise knowledge management system</li>
                                <li>ğŸ¤ Multi-agent software development team</li>
                                <li>ğŸ¯ Autonomous business analyst (data + insights + reports)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- SECTION 7.5: KEY TOPICS TO SURVIVE AI ERA -->
                <div id="keytopics" class="section">
                    <h2>ğŸ¯ 7.5. Key Topics to Survive the AI Era</h2>
                    
                    <div class="must-know">
                        <strong>Reality Check:</strong> You don't need to know everything! But these 10 concepts are non-negotiable if you want to stay relevant in the <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span>-powered future.
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ”¥ The Essential 10</h3>
                        
                        <div class="example-box">
                            <h4>1. Prompt Engineering (Start Here!)</h4>
                            <p><strong>Why:</strong> It's the #1 skill employers want right now. Everyone needs to communicate effectively with <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span>.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>Zero-shot, few-shot, chain-of-thought prompting</li>
                                <li>System prompts vs user prompts</li>
                                <li>Temperature, top-p, max tokens control</li>
                            </ul>
                            <p><strong>Reality:</strong> Master this in 1 week. Practice daily with ChatGPT/<span class="tooltip">Claude<span class="tooltiptext">Anthropic's AI Assistant</span></span>.</p>
                        </div>

                        <div class="example-box">
                            <h4>2. <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> (Retrieval-Augmented Generation)</h4>
                            <p><strong>Why:</strong> 80% of enterprise <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> use cases involve RAG. It's how you make <span class="tooltip">LLMs<span class="tooltiptext">Large Language Models</span></span> work with your private data.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>How embeddings convert text to vectors</li>
                                <li>Vector databases (Pinecone, Chroma)</li>
                                <li>Document chunking strategies</li>
                                <li>Semantic search basics</li>
                            </ul>
                            <p><strong>Reality:</strong> Build 1 RAG system with your company docs. That's itâ€”you're hireable.</p>
                        </div>

                        <div class="example-box">
                            <h4>3. Fine-Tuning vs Prompting (Know When to Use Each)</h4>
                            <p><strong>Why:</strong> Saves you 100x costs. Most problems don't need fine-tuning!</p>
                            <p><strong>Decision Tree:</strong></p>
                            <ul>
                                <li>New knowledge â†’ Use <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span></li>
                                <li>New style/format â†’ Fine-tune</li>
                                <li>New task â†’ Start with prompting, fine-tune if needed</li>
                            </ul>
                            <p><strong>Reality:</strong> Try prompting + RAG first. Fine-tune only if accuracy still poor.</p>
                        </div>

                        <div class="example-box">
                            <h4>4. <span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> Usage (OpenAI, Anthropic, etc.)</h4>
                            <p><strong>Why:</strong> You need to integrate <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> into real applications, not just chat.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>Making <span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> calls (Python requests, official SDKs)</li>
                                <li>Streaming responses for better UX</li>
                                <li>Cost management (batching, caching)</li>
                                <li>Error handling and retries</li>
                            </ul>
                            <p><strong>Reality:</strong> Build 3-4 small apps. Instant confidence boost.</p>
                        </div>

                        <div class="example-box">
                            <h4>5. Vector Embeddings (The Math Behind <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> "Understanding")</h4>
                            <p><strong>Why:</strong> Core to search, recommendations, RAG, similarity matching.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>What embeddings are (text â†’ numbers)</li>
                                <li>Cosine similarity for finding similar items</li>
                                <li>When to use sentence-transformers vs <span class="tooltip">OpenAI<span class="tooltiptext">OpenAI Embeddings API</span></span> embeddings</li>
                            </ul>
                            <p><strong>Reality:</strong> You don't need a PhD. Just understand: similar meaning â†’ similar vectors.</p>
                        </div>

                        <div class="example-box">
                            <h4>6. Model Selection (Choosing the Right <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span>)</h4>
                            <p><strong>Why:</strong> Wrong model = wasted money or poor results.</p>
                            <p><strong>Quick Guide:</strong></p>
                            <ul>
                                <li><strong>GPT-4-class models:</strong> Strong reasoning, typically higher cost (pricing varies)</li>
                                <li><strong>Smaller/faster models:</strong> Cheaper and low-latency, often â€œgood enoughâ€ for many apps</li>
                                <li><strong>Claude 3.5 Sonnet:</strong> Best for writing, coding</li>
                                <li><strong>Llama 3:</strong> Free, self-hosted, privacy</li>
                                <li><strong>Mistral:</strong> European, multilingual</li>
                            </ul>
                            <p><strong>Reality:</strong> Start with GPT-3.5. Upgrade to GPT-4 only if accuracy matters.</p>
                        </div>

                        <div class="example-box">
                            <h4>7. AI Agents (The Future of Work)</h4>
                            <p><strong>Why:</strong> Agents are replacing simple workflows and automations.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>ReAct pattern (Reason + Act)</li>
                                <li>Tool calling / function calling</li>
                                <li>When agents fail (and fallbacks)</li>
                            </ul>
                            <p><strong>Reality:</strong> Build 1 simple agent (e.g., "Check email â†’ Summarize â†’ Reply"). That's enough to understand agents.</p>
                        </div>

                        <div class="example-box">
                            <h4>8. Context Windows & Token Limits</h4>
                            <p><strong>Why:</strong> You'll hit limits. Know how to handle them.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>What a token is (~4 characters)</li>
                                <li>Model limits (varies by model/version; some offer very large contexts)</li>
                                <li>Strategies: summarization, RAG, sliding windows</li>
                            </ul>
                            <p><strong>Reality:</strong> Count tokens before sending (tiktoken library). Plan accordingly.</p>
                        </div>

                        <div class="example-box">
                            <h4>9. Inference vs Training (Know the Difference!)</h4>
                            <p><strong>Why:</strong> Saves you from overhyping or underestimating what's possible.</p>
                            <p><strong>Key Differences:</strong></p>
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Inference</th>
                                    <th>Training</th>
                                </tr>
                                <tr>
                                    <td>Cost</td>
                                    <td>Often cents or less per request (varies by model + tokens)</td>
                                    <td>Often thousands to millions+ (depends on dataset + compute)</td>
                                </tr>
                                <tr>
                                    <td>Time</td>
                                    <td>0.1-5 seconds</td>
                                    <td>Days to months</td>
                                </tr>
                                <tr>
                                    <td>Hardware</td>
                                    <td>1 <span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> or <span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span></td>
                                    <td>100s of <span class="tooltip">GPUs<span class="tooltiptext">Graphics Processing Units</span></span></td>
                                </tr>
                            </table>
                            <p><strong>Reality:</strong> 99% of developers only do inference. You're using pre-trained models.</p>
                        </div>

                        <div class="example-box">
                            <h4>10. Ethical <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> & Bias Awareness</h4>
                            <p><strong>Why:</strong> Legal liability + PR disasters. Companies care about this.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>Models can hallucinate (make up facts)</li>
                                <li>Biases in training data persist in outputs</li>
                                <li>GDPR/privacy concerns with user data</li>
                                <li>When to add human-in-the-loop verification</li>
                            </ul>
                            <p><strong>Reality:</strong> Add disclaimers. Never trust <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> for critical decisions without human review.</p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ“Š Your Learning Priority Matrix</h3>
                        
                        <table>
                            <tr>
                                <th>If You're A...</th>
                                <th>Must Learn (Priority 1)</th>
                                <th>Should Learn (Priority 2)</th>
                                <th>Nice to Have (Priority 3)</th>
                            </tr>
                            <tr>
                                <td><strong>Software Developer</strong></td>
                                <td><span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> usage, RAG, Prompting</td>
                                <td>Agents, Vector DBs, Fine-tuning</td>
                                <td>Model training, Quantization</td>
                            </tr>
                            <tr>
                                <td><strong>Data Scientist</strong></td>
                                <td>Embeddings, Fine-tuning, RAG</td>
                                <td>LoRA, Agents, Model evaluation</td>
                                <td>RLHF, Distributed training</td>
                            </tr>
                            <tr>
                                <td><strong>Product Manager</strong></td>
                                <td>Prompting, Model selection, Cost management</td>
                                <td>RAG basics, Agent capabilities</td>
                                <td>Technical architecture</td>
                            </tr>
                            <tr>
                                <td><strong>Business Analyst</strong></td>
                                <td>Prompting, <span class="tooltip">ChatGPT<span class="tooltiptext">OpenAI's Conversational AI</span></span>/Claude mastery</td>
                                <td>RAG concepts, Use case identification</td>
                                <td>Everything else</td>
                            </tr>
                            <tr>
                                <td><strong>ML Engineer</strong></td>
                                <td>Fine-tuning, Quantization, Inference optimization</td>
                                <td>Agents, RLHF, Distributed training</td>
                                <td>Prompt engineering (already know)</td>
                            </tr>
                            <tr>
                                <td><strong>IT Architect</strong></td>
                                <td>Model selection, Deployment strategies, Cost optimization</td>
                                <td><span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> architecture, Scaling, Security</td>
                                <td>Fine-tuning details, Model training</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ”§ Tools Ecosystem</h3>
                        
                        <div class="must-know">
                            <strong>Core Principle:</strong> Master the fundamentals first. Don't chase every shiny new tool. These 10 tools cover 90% of real-world <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> work. Deep dives below on key frameworks.
                        </div>

                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Priority</th>
                                    <th>Tool</th>
                                    <th>Why Essential</th>
                                    <th>Time to Learn</th>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‡ #1</strong></td>
                                    <td><strong><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">Python + PyTorch</a></strong></td>
                                    <td>Foundation of all <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> work. 95% of <span class="tooltip">ML<span class="tooltiptext">Machine Learning</span></span> code is Python.</td>
                                    <td>2-3 weeks basics</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‡ #2</strong></td>
                                    <td><strong><a href="https://huggingface.co/docs/transformers" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">HuggingFace Transformers</a></strong></td>
                                    <td>Access to 500K+ models. Industry standard.</td>
                                    <td>1 week</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‡ #3</strong></td>
                                    <td><strong><a href="https://platform.openai.com/docs/api-reference" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">OpenAI/<span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> SDKs</a></strong></td>
                                    <td>90% of production apps use APIs, not local models.</td>
                                    <td>2-3 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥ˆ #4</strong></td>
                                    <td><strong><a href="https://python.langchain.com/" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">LangChain</a></strong></td>
                                    <td>Most popular <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span>/Agent framework. Job requirement.</td>
                                    <td>1 week</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥ˆ #5</strong></td>
                                    <td><strong>Chroma/Pinecone</strong></td>
                                    <td>Vector databases power every <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> system.</td>
                                    <td>3-4 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥ˆ #6</strong></td>
                                    <td><strong>Ollama</strong></td>
                                    <td>Easiest way to run models locally. Essential for development.</td>
                                    <td>1 day</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #7</strong></td>
                                    <td><strong>Unsloth/<span class="tooltip">PEFT<span class="tooltiptext">Parameter-Efficient Fine-Tuning</span></span></strong></td>
                                    <td>Fine-tuning on consumer hardware. 2x faster training.</td>
                                    <td>3-5 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #8</strong></td>
                                    <td><strong>vLLM/TGI</strong></td>
                                    <td>Production inference. 10x faster than vanilla serving.</td>
                                    <td>2-3 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #9</strong></td>
                                    <td><strong>llama.cpp</strong></td>
                                    <td><span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span> inference, GGUF quantization. Powers many tools.</td>
                                    <td>2-3 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #10</strong></td>
                                    <td><strong>Weights & Biases</strong></td>
                                    <td>Track experiments, share results. Industry standard monitoring.</td>
                                    <td>2 days</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Learning Path Recommendation:</strong>
                            <pre>
<strong>Month 1 - Essentials (Start Here!):</strong>
âœ“ Python basics â†’ PyTorch fundamentals
âœ“ HuggingFace Transformers (load & run models)
âœ“ OpenAI API (build 3-4 small apps)

<strong>Month 2 - RAG & Production:</strong>
âœ“ LangChain (chains, agents, memory)
âœ“ Chroma/Pinecone (vector search)
âœ“ Ollama (local development)

<strong>Month 3 - Advanced (When Needed):</strong>
âœ“ Unsloth/PEFT (if you need fine-tuning)
âœ“ vLLM (if deploying to production)
âœ“ llama.cpp (if targeting CPUs/edge devices)

<strong>Throughout - Monitoring:</strong>
âœ“ Weights & Biases for tracking experiments
</pre>
                        </div>

                        <div class="mental-model">
                            <strong>Reality Check - What You DON'T Need (Yet):</strong>
                            <ul>
                                <li>âŒ <strong>DeepSpeed, FSDP:</strong> Only for training from scratch (you won't)</li>
                                <li>âŒ <strong>TensorRT, ONNX:</strong> Only for extreme optimization (start simple)</li>
                                <li>âŒ <strong>Kubernetes, Ray:</strong> Only when scaling to 100K+ users</li>
                                <li>âŒ <strong>Custom CUDA kernels:</strong> Unless you're building foundational infrastructure</li>
                                <li>âœ… <strong>Focus:</strong> Start with the 10 tools above. Add others when you hit real limitations.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-wandb">
                        <h3>ğŸ“Š 7.5.1 Deep Dive: Weights & Biases (W&B)</h3>
                        
                        <div class="must-know">
                            <strong>What is Weights & Biases?</strong> The industry-standard MLOps platform for experiment tracking, model management, and team collaboration. Think of it as "Git + Excel + Dashboards" for ML - version your experiments, compare results, and share insights instantly.
                        </div>

                        <div class="mental-model">
                            <strong>The Problem W&B Solves:</strong>
                            <p>Training AI models is iterative chaos:</p>
                            <ul>
                                <li>âŒ <strong>Version Hell:</strong> "Which hyperparameters did I use for that 94% accuracy run last week?"</li>
                                <li>âŒ <strong>Excel Madness:</strong> Tracking 50 experiments in spreadsheets is error-prone</li>
                                <li>âŒ <strong>Lost Work:</strong> Teammate trained a great model but forgot to save the config</li>
                                <li>âŒ <strong>Blind Debugging:</strong> Model failing but can't visualize what's happening</li>
                            </ul>
                            <p><strong>W&B provides:</strong> Automatic logging + beautiful visualizations + collaborative workspace + model registry + deployment tracking.</p>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(251, 191, 36, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #d97706; margin-bottom: 10px;">
                                <em>Interactive: Live Experiment Tracking Dashboard</em>
                            </p>
                            <button onclick="animateWandB()" style="padding: 10px 20px; background: linear-gradient(135deg, #f59e0b, #d97706); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(245, 158, 11, 0.3); margin-bottom: 15px;">â–¶ï¸ Simulate Training Run</button>
                            <div id="wandbDashboard" style="padding: 20px; background: linear-gradient(135deg, #fef3c7, #fde68a); border-radius: 8px; min-height: 400px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);">
                                <div style="text-align: center; color: #92400e; font-size: 14px; padding: 40px;">Click "Simulate Training Run" to watch live metrics, compare experiments, and see the final model selection</div>
                            </div>
                            <p style="font-size: 0.85em; color: #92400e; margin-top: 10px;">Watch how W&B tracks loss, accuracy, and GPU metrics in real-time across 3 competing experiments</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>W&B Workflow (5 Steps to Production):</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Initialize Tracking                â”‚
â”‚  wandb.init(project="my-llm-finetuning")   â”‚
â”‚  â†’ Creates cloud workspace                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Auto-Log Everything                â”‚
â”‚  wandb.log({"loss": 0.5, "lr": 2e-5})      â”‚
â”‚  â†’ Metrics, hyperparameters, system stats   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Visualize Live                     â”‚
â”‚  Dashboard updates every 10 seconds         â”‚
â”‚  â†’ Line charts, GPU usage, cost tracking    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Compare Experiments                â”‚
â”‚  Parallel coordinates plot shows best run   â”‚
â”‚  â†’ "Experiment 7 had lowest loss at epoch 3"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Save & Deploy Best Model           â”‚
â”‚  wandb.log_model(model, "production")       â”‚
â”‚  â†’ Team downloads from Model Registry       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>W&B Example: Track Fine-Tuning in 5 Lines</strong>
                            <pre>import wandb
from transformers import TrainingArguments

# Initialize tracking
wandb.init(
    project="llama-3-finetuning",
    config={
        "learning_rate": 2e-5,
        "epochs": 3,
        "batch_size": 4,
        "model": "meta-llama/Llama-3-8B"
    }
)

# Auto-integrate with HuggingFace Trainer
training_args = TrainingArguments(
    output_dir="./results",
    report_to="wandb",  # â† Automatic logging!
    logging_steps=10
)

# Train model (W&B logs everything automatically)
trainer = Trainer(model=model, args=training_args, train_dataset=dataset)
trainer.train()

# Log final model artifact
wandb.log_model(path="./results", name="llama-3-custom")
wandb.finish()

# ğŸ‰ Dashboard now shows: loss curves, GPU%, memory, throughput, model file</pre>
                            <p style="margin-top: 10px;"><strong>Without W&B:</strong> You'd manually save logs, plot graphs in matplotlib, and track configs in text files. 100x more work!</p>
                        </div>

                        <div class="tools-box">
                            <strong>W&B Key Features (Why Teams Pay for This):</strong>
                            <table>
                                <tr>
                                    <th>Feature</th>
                                    <th>Purpose</th>
                                    <th>Example</th>
                                </tr>
                                <tr>
                                    <td><strong>Experiment Tracking</strong></td>
                                    <td>Auto-log metrics & hyperparameters</td>
                                    <td>"Show me all runs with lr > 1e-5"</td>
                                </tr>
                                <tr>
                                    <td><strong>Model Registry</strong></td>
                                    <td>Version models like Git commits</td>
                                    <td>"Deploy model from Experiment #42"</td>
                                </tr>
                                <tr>
                                    <td><strong>Artifacts</strong></td>
                                    <td>Store datasets, configs, checkpoints</td>
                                    <td>"Download training dataset v3.2"</td>
                                </tr>
                                <tr>
                                    <td><strong>Sweeps</strong></td>
                                    <td>Automated hyperparameter search</td>
                                    <td>"Try 20 lr values, pick best"</td>
                                </tr>
                                <tr>
                                    <td><strong>Reports</strong></td>
                                    <td>Share results with stakeholders</td>
                                    <td>"Executive summary with graphs"</td>
                                </tr>
                                <tr>
                                    <td><strong>Alerts</strong></td>
                                    <td>Slack/email when training fails</td>
                                    <td>"Notify me if loss > 2.0"</td>
                                </tr>
                                <tr>
                                    <td><strong>System Metrics</strong></td>
                                    <td>GPU usage, CPU, memory, disk</td>
                                    <td>"Why is GPU at 40%? Memory leak?"</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Real-World Use Case: Debugging Training Issues</strong>
                            <pre><strong>Scenario:</strong> Your model loss plateaus at 1.2 after epoch 1.

<strong>W&B Dashboard Shows:</strong>
1. Loss curve: Flat after 1000 steps
2. Learning rate: Decayed to 1e-7 (too low!)
3. GPU utilization: 45% (data loading bottleneck)
4. Gradient norms: Exploding to 1e6 (need clipping)

<strong>Fix in 5 Minutes:</strong>
âœ“ Increase learning rate to 5e-5
âœ“ Add gradient clipping (max_norm=1.0)
âœ“ Increase dataloader workers from 2â†’8

<strong>Result:</strong> New run reaches 0.4 loss. W&B saved you 3 days of blind debugging!</pre>
                        </div>

                        <div class="tools-box">
                            <strong>W&B vs Alternatives:</strong>
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Best For</th>
                                    <th>Collaboration</th>
                                    <th>Setup Time</th>
                                    <th>Cost</th>
                                </tr>
                                <tr>
                                    <td><strong>W&B</strong></td>
                                    <td>Teams, production projects</td>
                                    <td>â­â­â­â­â­ Cloud dashboards</td>
                                    <td>2 minutes</td>
                                    <td>Free up to 100GB</td>
                                </tr>
                                <tr>
                                    <td><strong>TensorBoard</strong></td>
                                    <td>Solo projects, PyTorch users</td>
                                    <td>â­â­ Local only</td>
                                    <td>5 minutes</td>
                                    <td>Free</td>
                                </tr>
                                <tr>
                                    <td><strong>MLflow</strong></td>
                                    <td>Self-hosted, on-prem</td>
                                    <td>â­â­â­â­ Requires server</td>
                                    <td>30 minutes</td>
                                    <td>Free (self-host)</td>
                                </tr>
                                <tr>
                                    <td><strong>Neptune.ai</strong></td>
                                    <td>Alternative to W&B</td>
                                    <td>â­â­â­â­ Similar features</td>
                                    <td>3 minutes</td>
                                    <td>Free up to 200GB</td>
                                </tr>
                                <tr>
                                    <td><strong>Excel + Manual Logs</strong></td>
                                    <td>Masochists ğŸ˜…</td>
                                    <td>â­ Email attachments</td>
                                    <td>Instant</td>
                                    <td>Free (+ your sanity)</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>W&B FAQs:</strong>
                            <ul>
                                <li><strong>Q: Is it free for personal projects?</strong><br>
                                    âœ… Yes! 100GB storage, unlimited experiments. Perfect for learning & portfolios.</li>
                                <li><strong>Q: Does it work offline?</strong><br>
                                    âœ… Yes. Logs locally, syncs when internet returns.</li>
                                <li><strong>Q: Can I self-host it?</strong><br>
                                    âš ï¸ Yes, but requires enterprise license ($$$). Use MLflow for free self-hosting.</li>
                                <li><strong>Q: Which is better: W&B or TensorBoard?</strong><br>
                                    ğŸ“Š <strong>TensorBoard:</strong> Great for solo PyTorch work, local debugging.<br>
                                    ğŸ“Š <strong>W&B:</strong> Better for teams, comparing experiments, production tracking.</li>
                                <li><strong>Q: Do I need it if I'm just learning?</strong><br>
                                    ğŸ’¡ Not at first. Start logging after you've trained 10+ models and realize you can't remember which config worked best.</li>
                                <li><strong>Q: How much does the paid tier cost?</strong><br>
                                    ğŸ’° ~$50/user/month for teams. Includes unlimited storage, advanced features, priority support.</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>When to Use W&B:</strong>
                            <ul>
                                <li>âœ… <strong>Fine-tuning models:</strong> Track loss curves across multiple runs</li>
                                <li>âœ… <strong>Hyperparameter tuning:</strong> Compare 50 experiments visually</li>
                                <li>âœ… <strong>Team collaboration:</strong> Share results without emailing screenshots</li>
                                <li>âœ… <strong>Production monitoring:</strong> Track deployed model performance</li>
                                <li>âœ… <strong>Research reproducibility:</strong> Log everything for papers</li>
                                <li>âš ï¸ <strong>Simple API calls:</strong> Overkill if you're just calling OpenAI API (no training)</li>
                                <li>âš ï¸ <strong>One-off scripts:</strong> Not worth setup for single-use code</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Getting Started (5-Minute Quickstart):</strong>
                            <pre><strong>Step 1: Install</strong>
pip install wandb

<strong>Step 2: Login (creates free account)</strong>
wandb login
# Opens browser â†’ Sign in with GitHub/Google

<strong>Step 3: Add to Your Code</strong>
import wandb

# Start tracking
wandb.init(project="my-first-project")

# Log metrics in your training loop
for epoch in range(10):
    loss = train_one_epoch()  # Your training code
    wandb.log({"loss": loss, "epoch": epoch})

# Done! View dashboard at https://wandb.ai/username/my-first-project

<strong>Step 4: Open Dashboard</strong>
âœ“ See live loss curve
âœ“ View hyperparameters
âœ“ Check GPU usage
âœ“ Compare with future runs</pre>
                        </div>

                        <div class="must-know">
                            <strong>ğŸ¯ Bottom Line:</strong> W&B is like having a senior ML engineer looking over your shoulder, showing you what's wrong and what's working. It's free for learning, and every AI company uses it (or wishes they did). <strong>Learn it once, use it forever.</strong>
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-langchain">
                        <h3>ğŸ¦œ 7.5.2 Deep Dive: LangChain Framework</h3>
                        
                        <div class="must-know">
                            <strong>What is LangChain?</strong> The most popular framework for building LLM applications. Think of it as "jQuery for AI" - it handles the repetitive plumbing so you can focus on logic.
                        </div>

                        <div class="mental-model">
                            <strong>The Problem LangChain Solves:</strong>
                            <p>Building AI apps requires chaining multiple operations:</p>
                            <ul>
                                <li>Prompt templates â†’ LLM call â†’ Parse output â†’ Store in memory â†’ Use in next prompt</li>
                                <li>Load documents â†’ Split â†’ Embed â†’ Store â†’ Search â†’ Retrieve â†’ Generate</li>
                                <li>User input â†’ Agent decides tool â†’ Execute â†’ Parse result â†’ Decide next action</li>
                            </ul>
                            <p><strong>LangChain provides:</strong> Pre-built components for each step + orchestration to chain them together.</p>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: LangChain Pipeline Flow</em>
                            </p>
                            <button onclick="animateLangChain()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show LangChain Flow</button>
                            <div id="langchainPipeline" style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 10px; padding: 20px; background: linear-gradient(135deg, #f8fafc, #e2e8f0); border-radius: 8px; min-height: 150px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch how LangChain chains components together for complex AI workflows</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>LangChain Core Concepts:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CHAINS - Sequential Steps               â”‚
â”‚     Prompt â†’ LLM â†’ Output Parser â†’ Memory  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AGENTS - Dynamic Decision Making        â”‚
â”‚     LLM decides which tools to use & when   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. MEMORY - Context Persistence            â”‚
â”‚     Store conversation history across calls â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. RETRIEVERS - Smart Document Search      â”‚
â”‚     Vector DB + semantic search built-in    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>LangChain Example: Simple RAG in 10 Lines</strong>
                            <p style="margin: 8px 0 12px; font-size: 0.9em; color: #64748b;">Note: LangChainâ€™s Python APIs evolve quickly; depending on your version, some integrations/import paths may live in separate packages (e.g., <code>langchain-community</code>, <code>langchain-openai</code>). Treat this snippet as conceptual scaffolding and cross-check the current docs for exact imports.</p>
                            <pre>from langchain.document_loaders import TextLoader
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# Load & embed documents
loader = TextLoader('docs.txt')
docs = loader.load()
vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())

# Create RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever()
)

# Ask questions
qa_chain.run("What is the main topic?")</pre>
                            <p style="margin-top: 10px;"><strong>Without LangChain:</strong> Would take 100+ lines handling embeddings, chunking, retrieval, prompt formatting!</p>
                        </div>

                        <div class="tools-box">
                            <strong>LangChain Key Components:</strong>
                            <table>
                                <tr>
                                    <th>Component</th>
                                    <th>Purpose</th>
                                    <th>Example Use Case</th>
                                </tr>
                                <tr>
                                    <td><strong>PromptTemplate</strong></td>
                                    <td>Reusable prompt formatting</td>
                                    <td>"Summarize {document} for {audience}"</td>
                                </tr>
                                <tr>
                                    <td><strong>LLMChain</strong></td>
                                    <td>Prompt + LLM + Output</td>
                                    <td>Basic Q&A with formatting</td>
                                </tr>
                                <tr>
                                    <td><strong>RetrievalQA</strong></td>
                                    <td>RAG out of the box</td>
                                    <td>Chat with your documents</td>
                                </tr>
                                <tr>
                                    <td><strong>ConversationChain</strong></td>
                                    <td>Chatbot with memory</td>
                                    <td>Multi-turn conversations</td>
                                </tr>
                                <tr>
                                    <td><strong>Agent</strong></td>
                                    <td>LLM that uses tools</td>
                                    <td>Search web, run code, call APIs</td>
                                </tr>
                                <tr>
                                    <td><strong>VectorStore</strong></td>
                                    <td>Unified vector DB interface</td>
                                    <td>Swap Chroma/Pinecone/FAISS easily</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>When to Use LangChain?</strong>
                            <ul>
                                <li>âœ… <strong>Building RAG systems:</strong> Saves 80% of boilerplate code</li>
                                <li>âœ… <strong>Creating agents:</strong> Best framework for tool-using LLMs</li>
                                <li>âœ… <strong>Prototyping fast:</strong> Get from idea to demo in hours</li>
                                <li>âœ… <strong>Learning AI patterns:</strong> See best practices built-in</li>
                                <li>âš ï¸ <strong>Production at scale:</strong> Consider simpler alternatives for high-throughput</li>
                                <li>âš ï¸ <strong>Simple prompting:</strong> Overkill if you just need OpenAI API</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>LangChain vs Alternatives:</strong>
                            <ul>
                                <li><strong>LangChain:</strong> Most features, steepest learning curve, great for prototyping</li>
                                <li><strong>LlamaIndex:</strong> Better for RAG specifically, simpler API</li>
                                <li><strong>Raw APIs (OpenAI SDK):</strong> Maximum control, more code, best for production</li>
                                <li><strong>Semantic Kernel (Microsoft):</strong> Better C#/.NET integration</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-unsloth">
                        <h3>âš¡ 7.5.3 Deep Dive: Unsloth & PEFT</h3>
                        
                        <div class="must-know">
                            <strong>What is Unsloth?</strong> A high-performance fine-tuning library that's 2-5x faster than standard HuggingFace training. Combines custom CUDA kernels + memory optimization to fine-tune 7B+ models on consumer GPUs.<br>
                            <strong>What is PEFT?</strong> HuggingFace's Parameter-Efficient Fine-Tuning library. Instead of updating 7 billion weights, you update 10 million (LoRA adapters). Train on 1 GPU instead of 8.
                        </div>

                        <div class="mental-model">
                            <strong>The Problem Unsloth & PEFT Solve:</strong>
                            <p>Standard fine-tuning is prohibitively expensive:</p>
                            <ul>
                                <li>âŒ <strong>Memory:</strong> Llama-3-8B needs 120GB VRAM for full fine-tuning (8x A100 GPUs = $30K)</li>
                                <li>âŒ <strong>Speed:</strong> Training takes 10 hours on expensive cloud GPUs</li>
                                <li>âŒ <strong>Storage:</strong> Each fine-tuned model is 16GB (can't train 50 variants)</li>
                                <li>âŒ <strong>Accessibility:</strong> Only research labs can afford this</li>
                            </ul>
                            <p><strong>Unsloth + PEFT enable:</strong> Fine-tune Llama-3-8B on 16GB consumer GPU (RTX 4090) in 2 hours, saving only 100MB adapter weights.</p>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(139, 92, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #7c3aed; margin-bottom: 10px;">
                                <em>Interactive: Fine-Tuning Speed Comparison</em>
                            </p>
                            <button onclick="animateUnsloth()" style="padding: 10px 20px; background: linear-gradient(135deg, #8b5cf6, #7c3aed); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(139, 92, 246, 0.3); margin-bottom: 15px;">âš¡ Race: Standard vs Unsloth</button>
                            <div id="unslothRace" style="padding: 20px; background: linear-gradient(135deg, #faf5ff, #f3e8ff); border-radius: 8px; min-height: 450px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);">
                                <div style="text-align: center; color: #581c87; font-size: 14px; padding: 40px;">Click "Race" to watch side-by-side speed comparison: Standard Training (10 hrs) vs Unsloth+PEFT (2 hrs)</div>
                            </div>
                            <p style="font-size: 0.85em; color: #6b21a8; margin-top: 10px;">Watch memory usage, training speed, and final model size differences</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>How LoRA/PEFT Works (Technical Deep Dive):</strong>

Original Transformer Layer (Llama-3-8B):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: x (4096 dimensions)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  W_original: 4096Ã—4096 matrix                â”‚
â”‚  (16.7 million parameters) â† FROZEN!         â”‚
â”‚  Output: WÂ·x                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>Add LoRA Adapters (r=16 rank):</strong>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  A: 4096Ã—16 matrix (65K parameters)          â”‚
â”‚  B: 16Ã—4096 matrix (65K parameters)          â”‚
â”‚  Combined: 130K parameters (0.8% of original)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: WÂ·x + BÂ·AÂ·x                         â”‚
â”‚         â†‘      â†‘                             â”‚
â”‚      Original  LoRA (only this trains!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>Memory Savings:</strong>
Full Fine-Tuning:  8B params Ã— 4 bytes = 32GB weights
                  + 32GB gradients + 64GB optimizer = 128GB total

LoRA Fine-Tuning:  8B params Ã— 1 byte (quantized) = 8GB weights
                  + 10M params Ã— 4 bytes = 40MB adapters
                  + 40MB gradients + 80MB optimizer = 8.16GB total

<strong>Result: 16x less memory!</strong>
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Unsloth + PEFT Example: Fine-Tune Llama-3-8B</strong>
                            <pre>from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

# Load model in 4-bit (2GB instead of 16GB!)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",  # Pre-quantized
    max_seq_length=2048,
    dtype=None,  # Auto-detect BF16/FP16
    load_in_4bit=True,
)

# Add LoRA adapters (only these will train)
model = FastLanguageModel.get_peft_model(
    model,
    r=16,               # Rank (higher = more capacity, slower)
    target_modules=[    # Which layers to adapt
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # FFN
    ],
    lora_alpha=16,      # Scaling factor
    lora_dropout=0.05,  # Prevent overfitting
    bias="none",
    use_gradient_checkpointing="unsloth",  # 2x memory savings
    random_state=42,
)

# Load training data
dataset = load_dataset("yahma/alpaca-cleaned", split="train")

# Train (Unsloth makes this 2-5x faster!)
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,  # Effective batch=8
        warmup_steps=10,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        output_dir="outputs",
        num_train_epochs=3,
    ),
)

trainer.train()

# Save only the 100MB adapter (not the 16GB base model!)
model.save_pretrained("llama-3-8b-custom-adapter")

# To use: Load base model + merge adapter
# Total size: 16GB base + 100MB adapter = 16.1GB</pre>
                            <p style="margin-top: 10px;"><strong>Performance:</strong> On RTX 4090 (24GB VRAM), this trains in ~2 hours. Standard full fine-tuning would require 8Ã—A100 (640GB VRAM) and take 10+ hours!</p>
                        </div>

                        <div class="tools-box">
                            <strong>Fine-Tuning Methods Comparison:</strong>
                            <table>
                                <tr>
                                    <th>Method</th>
                                    <th>Memory (8B Model)</th>
                                    <th>Speed</th>
                                    <th>Quality</th>
                                    <th>Use Case</th>
                                </tr>
                                <tr>
                                    <td><strong>Full Fine-Tuning</strong></td>
                                    <td>120GB (8Ã—A100)</td>
                                    <td>1x (10 hrs)</td>
                                    <td>â­â­â­â­â­</td>
                                    <td>Research, unlimited budget</td>
                                </tr>
                                <tr>
                                    <td><strong>LoRA (HF PEFT)</strong></td>
                                    <td>24GB (1Ã—A100)</td>
                                    <td>3x (3 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td>Standard fine-tuning</td>
                                </tr>
                                <tr>
                                    <td><strong>QLoRA (4-bit)</strong></td>
                                    <td>12GB (RTX 4090)</td>
                                    <td>2x (5 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td>Consumer GPUs</td>
                                </tr>
                                <tr>
                                    <td><strong>Unsloth + LoRA</strong></td>
                                    <td>16GB (RTX 4080)</td>
                                    <td>5x (2 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td><strong>Best bang/buck!</strong></td>
                                </tr>
                                <tr>
                                    <td><strong>Unsloth + QLoRA</strong></td>
                                    <td>8GB (RTX 3090)</td>
                                    <td>4x (2.5 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td>Low-VRAM GPUs</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Real-World Use Case: Custom Chatbot</strong>
                            <pre><strong>Goal:</strong> Fine-tune Llama-3-8B on 10K customer service conversations for your company.

<strong>Standard Approach (HuggingFace PEFT):</strong>
â€¢ Hardware: Rent 1Ã—A100 (40GB) on AWS = $3.60/hour
â€¢ Training time: 6 hours
â€¢ Total cost: $21.60
â€¢ Setup complexity: 30 minutes (install PyTorch, HF, PEFT)

<strong>Unsloth Approach:</strong>
â€¢ Hardware: Use your RTX 4090 (24GB) = FREE
â€¢ Training time: 2 hours (or rent A100 for $7.20)
â€¢ Setup complexity: 5 minutes (pip install unsloth)

<strong>Results (Identical Quality):</strong>
âœ“ Model responds with company-specific terminology
âœ“ Handles edge cases from your training data
âœ“ 95% accuracy on held-out test conversations

<strong>Saved: $14.40 + 4 hours of time!</strong></pre>
                        </div>

                        <div class="tools-box">
                            <strong>Unsloth Key Features:</strong>
                            <table>
                                <tr>
                                    <th>Feature</th>
                                    <th>Benefit</th>
                                    <th>Technical Detail</th>
                                </tr>
                                <tr>
                                    <td><strong>Custom CUDA Kernels</strong></td>
                                    <td>2-5x faster training</td>
                                    <td>Fused attention, RoPE, RMS norm operations</td>
                                </tr>
                                <tr>
                                    <td><strong>Auto-Quantization</strong></td>
                                    <td>8GB â†’ 2GB model size</td>
                                    <td>4-bit quantization with minimal quality loss</td>
                                </tr>
                                <tr>
                                    <td><strong>Pre-Patched Models</strong></td>
                                    <td>Zero setup time</td>
                                    <td>Download pre-quantized Llama/Mistral/Gemma</td>
                                </tr>
                                <tr>
                                    <td><strong>Memory Tricks</strong></td>
                                    <td>Fit 2x longer sequences</td>
                                    <td>Gradient checkpointing + paged attention</td>
                                </tr>
                                <tr>
                                    <td><strong>Multi-GPU Support</strong></td>
                                    <td>Scale to 70B models</td>
                                    <td>Automatic model parallelism</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>Unsloth & PEFT FAQs:</strong>
                            <ul>
                                <li><strong>Q: Is Unsloth better than HuggingFace PEFT?</strong><br>
                                    ğŸš€ <strong>Unsloth:</strong> Faster (2-5x), easier setup, pre-quantized models.<br>
                                    ğŸ—ï¸ <strong>HF PEFT:</strong> More control, better docs, official support.<br>
                                    ğŸ’¡ <strong>Reality:</strong> Use Unsloth for speed, switch to PEFT if you hit limitations.</li>
                                <li><strong>Q: Does LoRA hurt model quality?</strong><br>
                                    ğŸ“Š In practice, LoRA achieves 95-99% of full fine-tuning quality. For most tasks, you won't notice the difference.</li>
                                <li><strong>Q: Can I merge LoRA adapters into the base model?</strong><br>
                                    âœ… Yes! `model.merge_and_unload()` creates a single 16GB model file. Good for deployment.</li>
                                <li><strong>Q: What rank (r) should I use?</strong><br>
                                    ğŸ“ˆ <strong>r=8:</strong> Fastest, use for simple tasks (classification, short generation)<br>
                                    ğŸ“ˆ <strong>r=16:</strong> Balanced (most common choice)<br>
                                    ğŸ“ˆ <strong>r=64:</strong> Slower but higher capacity (complex reasoning, long generation)</li>
                                <li><strong>Q: Do I need Unsloth for 1B models?</strong><br>
                                    âš ï¸ No. Gemma-2B, Phi-3-mini fit in 16GB without tricks. Use Unsloth for 7B+ models.</li>
                                <li><strong>Q: Can I fine-tune on CPU?</strong><br>
                                    ğŸŒ Yes (with PEFT), but it's 50x slower. A 2-hour GPU job becomes 4 days on CPU. Just use Google Colab free GPU.</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>When to Use Unsloth vs PEFT vs Full Fine-Tuning:</strong>
                            <ul>
                                <li>ğŸ† <strong>Unsloth + QLoRA:</strong> 95% of fine-tuning tasks. Fast, cheap, good enough.</li>
                                <li>âš™ï¸ <strong>HF PEFT:</strong> Need specific features (DoRA, AdaLoRA), production stability.</li>
                                <li>ğŸ”¬ <strong>Full Fine-Tuning:</strong> Research papers, maximum quality, unlimited budget.</li>
                                <li>âŒ <strong>Prompt Engineering:</strong> Try this FIRST before fine-tuning. 10 minutes vs 2 hours.</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Getting Started (10-Minute Quickstart):</strong>
                            <pre><strong>Step 1: Install Unsloth (30 seconds)</strong>
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

<strong>Step 2: Load Pre-Quantized Model (2 minutes)</strong>
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    "unsloth/llama-3-8b-bnb-4bit",  # 2GB download
    max_seq_length=2048,
    load_in_4bit=True
)

<strong>Step 3: Add LoRA (10 seconds)</strong>
model = FastLanguageModel.get_peft_model(model, r=16)

<strong>Step 4: Train on Your Data (2 hours)</strong>
trainer = SFTTrainer(model=model, dataset=your_data)
trainer.train()

<strong>Step 5: Save Adapter (100MB file)</strong>
model.save_pretrained("my-custom-llama")

<strong>Done! Your fine-tuned model is ready.</strong></pre>
                        </div>

                        <div class="must-know">
                            <strong>ğŸ¯ Bottom Line:</strong> Unsloth + PEFT democratized AI fine-tuning. What required $50K cloud bills now runs on a gaming PC. <strong>If you're fine-tuning models, you're using these tools.</strong> They're the difference between "only Google can do this" and "I did this on my laptop."
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-llamaindex">
                        <h3>ğŸ”® 7.5.4 Deep Dive: LlamaIndex (Coming Soon)</h3>
                        
                        <div class="must-know">
                            <strong>What is LlamaIndex?</strong> The leading framework for building RAG (Retrieval-Augmented Generation) applications. While LangChain does everything, LlamaIndex specializes in making your documents searchable and queryable by LLMs.
                        </div>

                        <div class="mental-model">
                            <strong>LlamaIndex vs LangChain:</strong>
                            <ul>
                                <li><strong>LlamaIndex:</strong> "Google for your documents" - Best for RAG, simpler API, data-centric</li>
                                <li><strong>LangChain:</strong> "Swiss Army knife" - Agents, chains, tools, complex workflows</li>
                                <li><strong>Reality:</strong> Many teams use BOTH. LlamaIndex for document indexing, LangChain for agent orchestration.</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Sneak Peek: LlamaIndex in 5 Lines</strong>
                            <pre>from llama_index import VectorStoreIndex, SimpleDirectoryReader

# Index all documents in a folder
documents = SimpleDirectoryReader('docs/').load_data()
index = VectorStoreIndex.from_documents(documents)

# Query your documents
query_engine = index.as_query_engine()
response = query_engine.query("What are the main findings?")
print(response)

# That's it! LlamaIndex handles chunking, embedding, retrieval, generation.</pre>
                        </div>

                        <div class="mental-model">
                            <strong>ğŸ“š Comprehensive Deep Dive Coming:</strong>
                            <p>The full LlamaIndex section will include:</p>
                            <ul>
                                <li>âœ… 1000+ word detailed explanation</li>
                                <li>âœ… Interactive animation showing document indexing pipeline</li>
                                <li>âœ… Comparison tables: LlamaIndex vs LangChain vs raw APIs</li>
                                <li>âœ… Real-world use cases with code examples</li>
                                <li>âœ… Advanced features: Multi-document agents, hybrid search, metadata filtering</li>
                                <li>âœ… FAQ section covering common questions</li>
                            </ul>
                            <p style="margin-top: 10px;"><em>This section is reserved for future expansion. Stay tuned!</em></p>
                        </div>
                    </div>

                    <div class="must-know">
                        <strong>ğŸš€ Action Plan:</strong>
                        <ol>
                            <li><strong>Week 1:</strong> Master prompting. Use ChatGPT/Claude daily.</li>
                            <li><strong>Week 2-3:</strong> Build a RAG app with your own documents.</li>
                            <li><strong>Week 4:</strong> Integrate OpenAI <span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> into a real project.</li>
                            <li><strong>Week 5-6:</strong> Build 1 simple agent (e.g., automated customer support).</li>
                            <li><strong>After:</strong> Go deeper based on your role (see matrix above).</li>
                        </ol>
                        <p style="margin-top: 10px;"><strong>Reality:</strong> With these 6 weeks, you'll know more than 95% of people talking about <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span>.</p>
                    </div>
                </div>

                <!-- SECTION 8: GLOSSARY -->
                <div id="glossary" class="section">
                    <h2>ğŸ“š 8. Glossary, FAQs & Acronyms</h2>
                    
                    <div class="concept-card">
                        <h3>Complete Acronyms & Abbreviations</h3>
                        
                        <table>
                            <tr>
                                <th>Acronym</th>
                                <th>Full Form</th>
                                <th>Description</th>
                            </tr>
                            <tr>
                                <td><strong>AI</strong></td>
                                <td>Artificial Intelligence</td>
                                <td>Machines that mimic human intelligence</td>
                            </tr>
                            <tr>
                                <td><strong>AGI</strong></td>
                                <td>Artificial General Intelligence</td>
                                <td>AI with human-level general intelligence (not yet achieved)</td>
                            </tr>
                            <tr>
                                <td><strong>API</strong></td>
                                <td>Application Programming Interface</td>
                                <td>Interface for software to communicate</td>
                            </tr>
                            <tr>
                                <td><strong>AWQ</strong></td>
                                <td>Activation-aware Weight Quantization</td>
                                <td>4-bit quantization method for GPUs</td>
                            </tr>
                            <tr>
                                <td><strong>BERT</strong></td>
                                <td>Bidirectional Encoder Representations from Transformers</td>
                                <td>Google's transformer model for understanding</td>
                            </tr>
                            <tr>
                                <td><strong>BF16</strong></td>
                                <td>bfloat16</td>
                                <td>16-bit floating point format (common in training)</td>
                            </tr>
                            <tr>
                                <td><strong>BPE</strong></td>
                                <td>Byte Pair Encoding</td>
                                <td>Tokenization algorithm used in GPT models</td>
                            </tr>
                            <tr>
                                <td><strong>CNN</strong></td>
                                <td>Convolutional Neural Network</td>
                                <td>Neural network for image processing</td>
                            </tr>
                            <tr>
                                <td><strong>CUDA</strong></td>
                                <td>Compute Unified Device Architecture</td>
                                <td>NVIDIA's GPU programming platform</td>
                            </tr>
                            <tr>
                                <td><strong>DPO</strong></td>
                                <td>Direct Preference Optimization</td>
                                <td>Alternative to RLHF for alignment</td>
                            </tr>
                            <tr>
                                <td><strong>FFN</strong></td>
                                <td>Feed-Forward Network</td>
                                <td>Component in transformer layers</td>
                            </tr>
                            <tr>
                                <td><strong>FP16/FP32</strong></td>
                                <td>Floating Point 16/32-bit</td>
                                <td>Number precision formats</td>
                            </tr>
                            <tr>
                                <td><strong>FPFT</strong></td>
                                <td>Full Parameter Fine-Tuning</td>
                                <td>Update all model weights during training</td>
                            </tr>
                            <tr>
                                <td><strong>GGML</strong></td>
                                <td>(no standard expansion)</td>
                                <td>Early llama.cpp ecosystem format/tooling for CPU inference (predecessor family to GGUF)</td>
                            </tr>
                            <tr>
                                <td><strong>GGUF</strong></td>
                                <td>(no standard expansion)</td>
                                <td>llama.cpp unified model file format (often used for quantized inference)</td>
                            </tr>
                            <tr>
                                <td><strong>GPT</strong></td>
                                <td>Generative Pre-trained Transformer</td>
                                <td>OpenAI's autoregressive language models</td>
                            </tr>
                            <tr>
                                <td><strong>GPTQ</strong></td>
                                <td>GPT Quantization</td>
                                <td>4-bit quantization for GPUs</td>
                            </tr>
                            <tr>
                                <td><strong>GPU</strong></td>
                                <td>Graphics Processing Unit</td>
                                <td>Parallel processor for AI workloads</td>
                            </tr>
                            <tr>
                                <td><strong>HF</strong></td>
                                <td>HuggingFace</td>
                                <td>AI community & model hub</td>
                            </tr>
                            <tr>
                                <td><strong>INT4/INT8</strong></td>
                                <td>Integer 4/8-bit</td>
                                <td>Quantized integer formats</td>
                            </tr>
                            <tr>
                                <td><strong>LLM</strong></td>
                                <td>Large Language Model</td>
                                <td>AI models trained on vast text (GPT, Llama)</td>
                            </tr>
                            <tr>
                                <td><strong>LoRA</strong></td>
                                <td>Low-Rank Adaptation</td>
                                <td>Efficient fine-tuning method</td>
                            </tr>
                            <tr>
                                <td><strong>LSTM</strong></td>
                                <td>Long Short-Term Memory</td>
                                <td>Old RNN architecture (pre-transformer)</td>
                            </tr>
                            <tr>
                                <td><strong>MCP</strong></td>
                                <td>Model Context Protocol</td>
                                <td>Standard for connecting models to tools/data</td>
                            </tr>
                            <tr>
                                <td><strong>ML</strong></td>
                                <td>Machine Learning</td>
                                <td>Algorithms that learn from data</td>
                            </tr>
                            <tr>
                                <td><strong>MoE</strong></td>
                                <td>Mixture of Experts</td>
                                <td>Architecture with multiple specialized sub-models</td>
                            </tr>
                            <tr>
                                <td><strong>NF4</strong></td>
                                <td>Normal Float 4-bit</td>
                                <td>Quantization format used in QLoRA</td>
                            </tr>
                            <tr>
                                <td><strong>NLP</strong></td>
                                <td>Natural Language Processing</td>
                                <td>AI for understanding human language</td>
                            </tr>
                            <tr>
                                <td><strong>ONNX</strong></td>
                                <td>Open Neural Network Exchange</td>
                                <td>Cross-framework model format</td>
                            </tr>
                            <tr>
                                <td><strong>PEFT</strong></td>
                                <td>Parameter-Efficient Fine-Tuning</td>
                                <td>Methods like LoRA that update few parameters</td>
                            </tr>
                            <tr>
                                <td><strong>PPO</strong></td>
                                <td>Proximal Policy Optimization</td>
                                <td>RL algorithm used in RLHF</td>
                            </tr>
                            <tr>
                                <td><strong>QLoRA</strong></td>
                                <td>Quantized LoRA</td>
                                <td>LoRA on 4-bit quantized models</td>
                            </tr>
                            <tr>
                                <td><strong>RAG</strong></td>
                                <td>Retrieval-Augmented Generation</td>
                                <td>LLM + external knowledge retrieval</td>
                            </tr>
                            <tr>
                                <td><strong>ReAct</strong></td>
                                <td>Reasoning and Acting</td>
                                <td>Agent framework combining thought and action</td>
                            </tr>
                            <tr>
                                <td><strong>RL</strong></td>
                                <td>Reinforcement Learning</td>
                                <td>Learning through rewards/penalties</td>
                            </tr>
                            <tr>
                                <td><strong>RLHF</strong></td>
                                <td>Reinforcement Learning from Human Feedback</td>
                                <td>Training to align with human preferences</td>
                            </tr>
                            <tr>
                                <td><strong>RNN</strong></td>
                                <td>Recurrent Neural Network</td>
                                <td>Old sequential model (pre-transformer)</td>
                            </tr>
                            <tr>
                                <td><strong>ROCm</strong></td>
                                <td>Radeon Open Compute</td>
                                <td>AMD's alternative to CUDA</td>
                            </tr>
                            <tr>
                                <td><strong>SFT</strong></td>
                                <td>Supervised Fine-Tuning</td>
                                <td>Training on labeled examples</td>
                            </tr>
                            <tr>
                                <td><strong>SLM</strong></td>
                                <td>Small Language Model</td>
                                <td>Compact models (<7B parameters)</td>
                            </tr>
                            <tr>
                                <td><strong>TensorRT</strong></td>
                                <td>Tensor Runtime</td>
                                <td>NVIDIA's inference optimizer</td>
                            </tr>
                            <tr>
                                <td><strong>TGI</strong></td>
                                <td>Text Generation Inference</td>
                                <td>HuggingFace's production serving</td>
                            </tr>
                            <tr>
                                <td><strong>VRAM</strong></td>
                                <td>Video Random Access Memory</td>
                                <td>GPU memory</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>Frequently Asked Questions</h3>
                        
                        <div class="faq-box">
                            <strong>Q: What's the difference between each model if they're all trained on the same internet data?</strong><br><br>
                            A: Great question! While many models use similar training data, they differ in:
                            <ul>
                                <li><strong>Architecture:</strong> Different layer counts, attention mechanisms (MoE, GQA), context windows</li>
                                <li><strong>Scale:</strong> Parameter count (7B vs 70B vs 175B) dramatically affects capability</li>
                                <li><strong>Training Recipes:</strong> Different hyperparameters, training techniques, compute budgets</li>
                                <li><strong>Fine-tuning:</strong> RLHF with different human feedback, instruction-tuning datasets</li>
                                <li><strong>Data Mix:</strong> While similar sources, different proportions (code vs text), filtering, deduplication</li>
                                <li><strong>Optimization Goals:</strong> Some optimize for speed, others for quality, cost, or specific tasks</li>
                            </ul>
                            <p><strong>Example:</strong> Llama 2 vs GPT-4 - both trained on internet data, but GPT-4 is much larger, uses proprietary techniques, extensive RLHF, and OpenAI's curated dataset mix.</p>
                        </div>

                        <div class="faq-box">
                            <strong>Q: How are proprietary models distributed vs open-source models? How to install and what tools to use?</strong><br><br>
                            A: <strong>Proprietary Models (GPT-4, Claude, Gemini):</strong>
                            <ul>
                                <li><strong>Distribution:</strong> API-only, no weights available</li>
                                <li><strong>Access:</strong> Sign up â†’ Get API key â†’ Pay per token</li>
                                <li><strong>Installation:</strong>
                                    <pre>
# OpenAI
pip install openai
from openai import OpenAI
client = OpenAI(api_key="sk-...")

# Anthropic (Claude)
pip install anthropic
from anthropic import Anthropic
client = Anthropic(api_key="...")
</pre>
                                </li>
                            </ul>

                            <strong>Open-Source Models (Llama, Mistral, Falcon):</strong>
                            <ul>
                                <li><strong>Distribution:</strong> Weights downloadable from HuggingFace, GitHub</li>
                                <li><strong>Access:</strong> Free download, run locally or on your servers</li>
                                <li><strong>Installation:</strong>
                                    <pre>
# Method 1: HuggingFace
pip install transformers
from transformers import AutoModel
model = AutoModel.from_pretrained("meta-llama/Llama-2-7b")

# Method 2: Ollama (easiest)
curl https://ollama.ai/install.sh | sh
ollama pull llama2
ollama run llama2

# Method 3: llama.cpp (CPU-optimized)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make
./main -m model.gguf -p "Hello"

# Method 4: LM Studio (GUI)
Download from lmstudio.ai â†’ Install â†’ Download models from GUI
</pre>
                                </li>
                            </ul>

                            <strong>Comparison:</strong>
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Proprietary</th>
                                    <th>Open Source</th>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td>Pay per use ($0.001-0.06/1K tokens)</td>
                                    <td>Free (just hardware)</td>
                                </tr>
                                <tr>
                                    <td><strong>Privacy</strong></td>
                                    <td>Data sent to provider</td>
                                    <td>Runs locally, fully private</td>
                                </tr>
                                <tr>
                                    <td><strong>Customization</strong></td>
                                    <td>Limited (API fine-tuning)</td>
                                    <td>Full control (modify weights)</td>
                                </tr>
                                <tr>
                                    <td><strong>Quality</strong></td>
                                    <td>Often higher (GPT-4, Claude)</td>
                                    <td>Catching up fast (Llama 3, Mixtral)</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Can I train my own LLM from scratch?</strong><br><br>
                            A: Technically yes, but practically challenging:
                            <ul>
                                <li><strong>Cost:</strong> $2-10 million for decent model (7B-70B params)</li>
                                <li><strong>Hardware:</strong> Need 100-1000+ GPUs for weeks</li>
                                <li><strong>Data:</strong> Hundreds of billions of tokens, curated and filtered</li>
                                <li><strong>Expertise:</strong> Distributed training, hyperparameter tuning, infrastructure</li>
                            </ul>
                            <p><strong>Better Alternatives:</strong></p>
                            <ul>
                                <li><strong>Fine-tune existing models</strong> - Much cheaper ($100-1000)</li>
                                <li><strong>Use open-source models</strong> - Free, proven quality</li>
                                <li><strong>RAG</strong> - Add your knowledge without training</li>
                            </ul>
                        </div>

                        <div class="faq-box">
                            <strong>Q: What hardware do I need for AI development?</strong><br><br>
                            A: Depends on what you're doing:
                            
                            <table>
                                <tr>
                                    <th>Task</th>
                                    <th>Minimum</th>
                                    <th>Recommended</th>
                                    <th>Professional</th>
                                </tr>
                                <tr>
                                    <td><strong>Learning / API Use</strong></td>
                                    <td>Any laptop</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td><strong>CPU Inference</strong></td>
                                    <td>16GB RAM</td>
                                    <td>32GB RAM</td>
                                    <td>64GB+ RAM</td>
                                </tr>
                                <tr>
                                    <td><strong>GPU Inference (7B)</strong></td>
                                    <td>RTX 3060 (12GB)</td>
                                    <td>RTX 4070 (16GB)</td>
                                    <td>RTX 4090 (24GB)</td>
                                </tr>
                                <tr>
                                    <td><strong>Fine-tuning (7B)</strong></td>
                                    <td>RTX 4060 Ti (16GB) + QLoRA</td>
                                    <td>RTX 4090 (24GB)</td>
                                    <td>A100 (40-80GB)</td>
                                </tr>
                                <tr>
                                    <td><strong>Training from Scratch</strong></td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>100+ A100s (cloud)</td>
                                </tr>
                            </table>

                            <p><strong>Budget Recommendations:</strong></p>
                            <ul>
                                <li><strong>$0-500:</strong> Use cloud GPUs (Colab Pro, Lambda Labs) or CPU inference</li>
                                <li><strong>$500-1500:</strong> RTX 4060 Ti 16GB - Good for inference + QLoRA</li>
                                <li><strong>$1500-2500:</strong> RTX 4090 24GB - Best consumer GPU for AI</li>
                                <li><strong>$2500+:</strong> Used A40/A100 or rent cloud GPUs</li>
                            </ul>
                        </div>

                        <div class="faq-box">
                            <strong>Q: How do I stay current with rapidly evolving AI?</strong><br><br>
                            A: The field moves fast! Here's how to keep up:
                            <ul>
                                <li><strong>Daily:</strong> Twitter/X - Follow key researchers (@karpathy, @ylecun, @AndrewYNg)</li>
                                <li><strong>Weekly:</strong> 
                                    <ul>
                                        <li>HuggingFace Papers (papers.huggingface.co)</li>
                                        <li>r/MachineLearning subreddit</li>
                                        <li>AI newsletters (The Batch, TLDR AI)</li>
                                    </ul>
                                </li>
                                <li><strong>Monthly:</strong> 
                                    <ul>
                                        <li>Major conferences: NeurIPS, ICML, ACL</li>
                                        <li>Company blogs: OpenAI, Anthropic, Google AI</li>
                                    </ul>
                                </li>
                                <li><strong>Hands-on:</strong> Build projects, experiment with new models/techniques immediately</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <div class="back-to-top" id="backToTop" onclick="scrollToTop()">â†‘</div>

    <script>
        // Navigation
        function showSection(sectionId, trigger) {
            // Hide all sections
            document.querySelectorAll('.section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            const targetSection = document.getElementById(sectionId);
            if (!targetSection) return;
            targetSection.classList.add('active');
            
            // Resolve click source if provided
            let triggerEl = null;
            if (trigger && trigger.target) {
                triggerEl = trigger.target;
            } else if (trigger && trigger.nodeType === 1) {
                triggerEl = trigger;
            } else {
                const activeEl = document.activeElement;
                if (activeEl && (activeEl.classList?.contains('nav-pill') || activeEl.classList?.contains('sidebar-link'))) {
                    triggerEl = activeEl;
                }
            }

            // Update nav pills
            document.querySelectorAll('.nav-pill').forEach(pill => pill.classList.remove('active'));
            const matchingPill = document.querySelector(`.nav-pill[onclick*="showSection('${sectionId}')"]`);
            (matchingPill || (triggerEl && triggerEl.classList?.contains('nav-pill') ? triggerEl : null))?.classList.add('active');

            // Update sidebar
            document.querySelectorAll('.sidebar-link').forEach(link => link.classList.remove('active'));
            const matchingSidebar = document.querySelector(`.sidebar-link[onclick*="showSection('${sectionId}')"]`);
            (matchingSidebar || (triggerEl && triggerEl.classList?.contains('sidebar-link') ? triggerEl : null))?.classList.add('active');
            
            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Subsection navigation - scrolls to a specific subsection within its owning section
        function scrollToSubsection(subsectionId) {
            const element = document.getElementById(subsectionId);
            if (!element) return;

            const owningSection = element.closest('.section');
            if (owningSection?.id) {
                showSection(owningSection.id);
            }

            setTimeout(() => {
                element.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 100);
        }

        // Progress bar
        window.addEventListener('scroll', () => {
            const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
            const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            const scrolled = (winScroll / height) * 100;
            document.getElementById('progressBar').style.width = scrolled + '%';
            
            // Back to top button
            const backToTop = document.getElementById('backToTop');
            if (winScroll > 300) {
                backToTop.classList.add('visible');
            } else {
                backToTop.classList.remove('visible');
            }
        });

        function scrollToTop() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Collapsible sections
        document.addEventListener('click', (e) => {
            if (e.target.classList.contains('collapsible')) {
                e.target.classList.toggle('expanded');
                const content = e.target.nextElementSibling;
                if (content && content.classList.contains('collapsible-content')) {
                    content.classList.toggle('expanded');
                }
            }
        });

        // Search functionality
        function searchHandbook() {
            const searchInput = document.getElementById('searchInput');
            const searchResults = document.getElementById('searchResults');
            const query = searchInput.value.toLowerCase().trim();
            
            // Clear previous highlights
            document.querySelectorAll('.search-highlight').forEach(el => {
                el.classList.remove('search-highlight');
            });
            
            if (query.length < 2) {
                searchResults.innerHTML = '';
                return;
            }
            
            // Search through all sections
            const sections = document.querySelectorAll('.section');
            let matches = [];
            let totalMatches = 0;
            
            sections.forEach(section => {
                const sectionId = section.id;
                const sectionTitle = section.querySelector('h2')?.textContent || 'Section';
                const content = section.textContent.toLowerCase();
                
                // Count matches in this section
                const regex = new RegExp(query, 'gi');
                const sectionMatches = (content.match(regex) || []).length;
                
                if (sectionMatches > 0) {
                    totalMatches += sectionMatches;
                    matches.push({
                        id: sectionId,
                        title: sectionTitle,
                        count: sectionMatches
                    });
                    
                    // Highlight first few occurrences in visible section
                    if (section.classList.contains('active')) {
                        const headings = section.querySelectorAll('h3, h4, strong');
                        headings.forEach(heading => {
                            if (heading.textContent.toLowerCase().includes(query)) {
                                heading.classList.add('search-highlight');
                            }
                        });
                    }
                }
            });
            
            // Display results
            if (matches.length > 0) {
                let resultsHTML = `<strong style="color: #1e40af;">${totalMatches} matches found in ${matches.length} sections:</strong><br/><div style="margin-top: 8px;">`;
                matches.forEach(match => {
                    resultsHTML += `<div style="margin: 4px 0; padding: 6px 10px; background: rgba(30, 64, 175, 0.05); border-radius: 4px;">
                                     <a href="#" onclick="showSectionById('${match.id}'); return false;" 
                                        style="color: #1e40af; text-decoration: none; font-weight: 500;">
                                        ğŸ“„ ${match.title} <span style="color: #64748b; font-weight: normal;">(${match.count} matches)</span>
                                     </a></div>`;
                });
                resultsHTML += '</div>';
                searchResults.innerHTML = resultsHTML;
            } else {
                searchResults.innerHTML = `<em>No matches found for "${query}"</em>`;
            }
        }
        
        function showSectionById(sectionId) {
            showSection(sectionId);
            
            // Re-run search to highlight in newly visible section
            searchHandbook();
        }

        // Add CSS for search highlighting
        const style = document.createElement('style');
        style.textContent = `
            .search-highlight {
                background: linear-gradient(120deg, #fef3c7 0%, #fde68a 100%);
                color: #1a1a2e !important;
                padding: 2px 4px;
                border-radius: 3px;
                animation: highlightFade 2s ease;
                font-weight: 600;
            }
            @keyframes highlightFade {
                0% { background: #fde047; }
                100% { background: linear-gradient(120deg, #fef3c7 0%, #fde68a 100%); }
            }
        `;
        document.head.appendChild(style);

        // ============================================
        // ANIMATION FUNCTIONS
        // ============================================

        // Animation #1: Matrix Multiplication (2x3 Ã— 3x2 = 2x2)
        function startMatrixMultiplication() {
            // Keep these in sync with the visible matrices in the HTML block
            const matrixA = [[2, 1, 3], [4, 0, 1]];
            const matrixB = [[1, 2], [3, 1], [0, 2]];
            const result = [[0, 0], [0, 0]];
            
            const cellsA = document.querySelectorAll('#matrixA .matrix-cell');
            const cellsB = document.querySelectorAll('#matrixB .matrix-cell');
            const cellsResult = document.querySelectorAll('#matrixResult .matrix-cell');
            const calcDisplay = document.getElementById('calcDisplay');
            
            let step = 0;
            const steps = [
                // Result[0][0] = 2*1 + 1*3 + 3*0 = 5
                { row: 0, col: 0, aIndices: [0,1,2], bIndices: [0,2,4], calc: '2Ã—1 + 1Ã—3 + 3Ã—0 = 5', value: 5 },
                // Result[0][1] = 2*2 + 1*1 + 3*2 = 11
                { row: 0, col: 1, aIndices: [0,1,2], bIndices: [1,3,5], calc: '2Ã—2 + 1Ã—1 + 3Ã—2 = 11', value: 11 },
                // Result[1][0] = 4*1 + 0*3 + 1*0 = 4
                { row: 1, col: 0, aIndices: [3,4,5], bIndices: [0,2,4], calc: '4Ã—1 + 0Ã—3 + 1Ã—0 = 4', value: 4 },
                // Result[1][1] = 4*2 + 0*1 + 1*2 = 10
                { row: 1, col: 1, aIndices: [3,4,5], bIndices: [1,3,5], calc: '4Ã—2 + 0Ã—1 + 1Ã—2 = 10', value: 10 }
            ];
            
            function animate() {
                if (step >= steps.length) {
                    setTimeout(() => {
                        cellsA.forEach(c => c.classList.remove('highlight'));
                        cellsB.forEach(c => c.classList.remove('highlight'));
                        calcDisplay.textContent = 'Matrix multiplication complete!';
                    }, 1000);
                    return;
                }
                
                const current = steps[step];
                
                // Clear previous highlights
                cellsA.forEach(c => c.classList.remove('highlight'));
                cellsB.forEach(c => c.classList.remove('highlight'));
                
                // Highlight current calculation cells
                current.aIndices.forEach(i => cellsA[i].classList.add('highlight'));
                current.bIndices.forEach(i => cellsB[i].classList.add('highlight'));
                
                // Update calculation display
                calcDisplay.textContent = current.calc;
                
                // Update result cell
                const resultIndex = current.row * 2 + current.col;
                cellsResult[resultIndex].textContent = current.value;
                cellsResult[resultIndex].classList.add('highlight');
                
                step++;
                setTimeout(animate, 1500);
            }
            
            // Reset and start
            cellsResult.forEach(c => {
                c.textContent = '?';
                c.classList.remove('highlight');
            });
            calcDisplay.textContent = 'Starting calculation...';
            setTimeout(animate, 500);
        }

        // Animation #2: Embedding Space 3D Scatter Plot
        let embeddingAnimationId = null;
        function animateEmbeddings() {
            const canvas = document.getElementById('embeddingCanvas');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 400;
            
            // Sample word embeddings (3D projected to 2D)
            const words = [
                { word: 'king', x: 450, y: 100, z: 0.8, color: '#3b82f6' },
                { word: 'queen', x: 470, y: 150, z: 0.75, color: '#3b82f6' },
                { word: 'man', x: 200, y: 120, z: 0.6, color: '#10b981' },
                { word: 'woman', x: 220, y: 170, z: 0.55, color: '#10b981' },
                { word: 'prince', x: 430, y: 180, z: 0.7, color: '#3b82f6' },
                { word: 'princess', x: 450, y: 230, z: 0.65, color: '#3b82f6' },
                { word: 'boy', x: 180, y: 200, z: 0.5, color: '#10b981' },
                { word: 'girl', x: 200, y: 250, z: 0.45, color: '#10b981' },
                { word: 'apple', x: 100, y: 300, z: 0.3, color: '#f59e0b' },
                { word: 'orange', x: 130, y: 320, z: 0.35, color: '#f59e0b' },
                { word: 'banana', x: 90, y: 350, z: 0.25, color: '#f59e0b' }
            ];
            
            let rotation = 0;
            
            function draw() {
                ctx.fillStyle = '#1a1a2e';
                ctx.fillRect(0, 0, canvas.width, canvas.height);
                
                // Rotate words slightly
                rotation += 0.005;
                
                // Draw lines between similar words
                ctx.strokeStyle = 'rgba(100, 100, 100, 0.2)';
                ctx.lineWidth = 1;
                ctx.beginPath();
                ctx.moveTo(words[0].x, words[0].y); ctx.lineTo(words[1].x, words[1].y); // king-queen
                ctx.moveTo(words[2].x, words[2].y); ctx.lineTo(words[3].x, words[3].y); // man-woman
                ctx.moveTo(words[8].x, words[8].y); ctx.lineTo(words[9].x, words[9].y); // apple-orange
                ctx.stroke();
                
                // Sort by z-depth for proper layering
                const sorted = [...words].sort((a, b) => a.z - b.z);
                
                // Draw points and labels
                sorted.forEach((w, i) => {
                    const size = 3 + w.z * 5;
                    const offsetX = Math.cos(rotation + i) * 10;
                    const offsetY = Math.sin(rotation + i) * 5;
                    
                    // Draw point
                    ctx.fillStyle = w.color;
                    ctx.beginPath();
                    ctx.arc(w.x + offsetX, w.y + offsetY, size, 0, Math.PI * 2);
                    ctx.fill();
                    
                    // Draw label
                    ctx.fillStyle = '#ffffff';
                    ctx.font = '12px monospace';
                    ctx.fillText(w.word, w.x + offsetX + 8, w.y + offsetY + 4);
                });
                
                // Draw axes
                ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
                ctx.lineWidth = 1;
                ctx.beginPath();
                ctx.moveTo(50, 380); ctx.lineTo(550, 380); // X-axis
                ctx.moveTo(50, 380); ctx.lineTo(50, 50);    // Y-axis
                ctx.stroke();
                
                ctx.fillStyle = 'rgba(255, 255, 255, 0.5)';
                ctx.font = '11px monospace';
                ctx.fillText('Gender â†’', 520, 395);
                ctx.fillText('Royalty â†‘', 10, 60);
                
                embeddingAnimationId = requestAnimationFrame(draw);
            }
            
            // Cancel previous animation if exists
            if (embeddingAnimationId) cancelAnimationFrame(embeddingAnimationId);
            draw();
        }

        // Animation #3: Attention Heatmap
        let attentionStep = 0;
        let attentionInterval = null;
        function showAttentionHeatmap() {
            const grid = document.getElementById('attentionGrid');
            if (!grid) return;
            
            // Clear any existing interval
            if (attentionInterval) clearInterval(attentionInterval);
            attentionStep = 0;
            
            const tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat'];
            const attention = [
                [0.8, 0.1, 0.05, 0.02, 0.02, 0.01],
                [0.1, 0.7, 0.15, 0.02, 0.02, 0.01],
                [0.05, 0.65, 0.15, 0.1, 0.03, 0.02],
                [0.03, 0.05, 0.2, 0.5, 0.15, 0.07],
                [0.8, 0.05, 0.03, 0.02, 0.05, 0.05],
                [0.02, 0.1, 0.2, 0.1, 0.05, 0.53]
            ];
            
            // Create compact grid with proper heatmap colors
            let html = '<div style="display: inline-block; background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">';
            html += '<div style="display: grid; grid-template-columns: 60px repeat(6, 50px); gap: 3px; font-size: 11px;">';
            
            // Header row
            html += '<div style="font-weight: bold; text-align: center; color: #1e40af;"></div>';
            tokens.forEach(t => {
                html += `<div style="font-weight: bold; text-align: center; color: #1e40af; padding: 5px;">${t}</div>`;
            });
            
            // Data rows with proper heatmap colors (red=high, yellow=medium, blue=low)
            tokens.forEach((rowToken, i) => {
                html += `<div style="font-weight: bold; text-align: right; padding: 8px 5px; color: #1e40af;">${rowToken}</div>`;
                attention[i].forEach((score, j) => {
                    // Proper heatmap: blue (low) -> green (medium) -> yellow -> red (high)
                    let color;
                    if (score >= 0.6) {
                        // High attention: yellow to red (0.6-1.0)
                        const intensity = (score - 0.6) / 0.4;  // 0 to 1
                        const red = 255;
                        const green = Math.floor(255 * (1 - intensity));
                        const blue = 0;
                        color = `rgb(${red}, ${green}, ${blue})`;
                    } else if (score >= 0.3) {
                        // Medium attention: green to yellow (0.3-0.6)
                        const intensity = (score - 0.3) / 0.3;  // 0 to 1
                        const red = Math.floor(255 * intensity);
                        const green = 200;
                        const blue = 0;
                        color = `rgb(${red}, ${green}, ${blue})`;
                    } else {
                        // Low attention: blue to green (0-0.3)
                        const intensity = score / 0.3;  // 0 to 1
                        const red = 0;
                        const green = Math.floor(150 * intensity);
                        const blue = Math.floor(255 * (1 - intensity * 0.6));
                        color = `rgb(${red}, ${green}, ${blue})`;
                    }
                    html += `<div class="attention-cell" data-row="${i}" data-col="${j}" style="background: ${color}; width: 50px; height: 40px; display: flex; align-items: center; justify-content: center; border-radius: 4px; font-weight: bold; color: ${score > 0.5 ? 'white' : '#1e293b'}; font-size: 10px; text-shadow: ${score > 0.5 ? '0 1px 2px rgba(0,0,0,0.5)' : 'none'}; transition: all 0.3s ease; cursor: pointer;" title="${rowToken} â†’ ${tokens[j]}: ${score.toFixed(2)}">${score.toFixed(2)}</div>`;
                });
            });
            
            html += '</div>';
            html += '<div style="margin-top: 10px; text-align: center; font-size: 11px; color: #64748b;">ğŸ”´ High Attention &nbsp;&nbsp; ğŸŸ¡ Medium &nbsp;&nbsp; ğŸ”µ Low</div>';
            html += '</div>';
            grid.innerHTML = html;
            
            // Animate highlighting rows
            function highlightRow() {
                const cells = document.querySelectorAll('.attention-cell');
                cells.forEach(c => {
                    c.style.transform = 'scale(1)';
                    c.style.boxShadow = 'none';
                });
                
                const row = attentionStep % 6;
                for (let col = 0; col < 6; col++) {
                    const cell = document.querySelector(`.attention-cell[data-row="${row}"][data-col="${col}"]`);
                    if (cell) {
                        cell.style.transform = 'scale(1.15)';
                        cell.style.boxShadow = '0 4px 12px rgba(0,0,0,0.4)';
                    }
                }
                
                attentionStep++;
            }
            
            highlightRow();
            attentionInterval = setInterval(highlightRow, 1500);
        }

        // Animation #4: LoRA Injection
        function animateLoRA() {
            const container = document.getElementById('loraContainer');
            if (!container) return;
            
            container.innerHTML = `
                <div style="text-align: center; margin-bottom: 15px; color: #e2e8f0; font-weight: 600;">
                    <strong>Fine-Tuning with LoRA</strong>
                </div>
                <div style="display: flex; align-items: center; justify-content: center; gap: 20px; flex-wrap: wrap;">
                    <div class="lora-base" style="width: 140px; height: 140px; background: linear-gradient(135deg, #3b82f6, #2563eb); border-radius: 12px; display: flex; align-items: center; justify-content: center; font-weight: bold; color: white; font-size: 14px; text-align: center; box-shadow: 0 4px 16px rgba(59, 130, 246, 0.3);">
                        Base Model<br/><span style="font-size: 18px;">175B</span><br/>params
                    </div>
                    <div style="font-size: 32px; color: #e2e8f0; font-weight: bold;">+</div>
                    <div class="lora-adapter" style="width: 80px; height: 80px; background: linear-gradient(135deg, #f59e0b, #d97706); border-radius: 10px; display: flex; align-items: center; justify-content: center; font-weight: bold; color: white; font-size: 12px; text-align: center; opacity: 0; animation: loraFadeIn 1s 0.5s forwards; box-shadow: 0 4px 16px rgba(245, 158, 11, 0.4);">
                        LoRA<br/><span style="font-size: 16px;">0.5B</span>
                    </div>
                    <div style="font-size: 32px; color: #e2e8f0; font-weight: bold;">=</div>
                    <div class="lora-result" style="width: 140px; height: 140px; background: linear-gradient(135deg, #10b981, #059669); border-radius: 12px; display: flex; align-items: center; justify-content: center; font-weight: bold; color: white; font-size: 13px; text-align: center; opacity: 0; animation: loraFadeIn 1s 1.5s forwards; box-shadow: 0 4px 16px rgba(16, 185, 129, 0.4);">
                        Custom Model<br/><span style="font-size: 18px;">175.5B</span><br/>params
                    </div>
                </div>
                <div style="text-align: center; margin-top: 20px; padding: 10px 20px; background: linear-gradient(135deg, #8b5cf6, #7c3aed); color: white; border-radius: 8px; display: inline-block; font-size: 13px; font-weight: 600;">
                    Only 0.3% of parameters trained!
                </div>
            `;
            
            const styleSheet = document.styleSheets[0];
            styleSheet.insertRule(`
                @keyframes loraFadeIn {
                    from { opacity: 0; transform: scale(0.8); }
                    to { opacity: 1; transform: scale(1); }
                }
            `, styleSheet.cssRules.length);
        }

        // Animation #5: RAG Pipeline Flow
        function animateRAG() {
            const pipeline = document.getElementById('ragPipeline');
            if (!pipeline) return;
            
            const steps = [
                { id: 'ragQuery', label: '1. User Query', content: '"What is LoRA?"', delay: 0 },
                { id: 'ragEmbed', label: '2. Embed Query', content: '[0.23, -0.45, ...]', delay: 1000 },
                { id: 'ragSearch', label: '3. Vector Search', content: 'Pinecone/Chroma', delay: 2000 },
                { id: 'ragDocs', label: '4. Retrieved Docs', content: '3 relevant chunks', delay: 3000 },
                { id: 'ragLLM', label: '5. LLM + Context', content: 'GPT-4 generation', delay: 4000 },
                { id: 'ragResponse', label: '6. Response', content: '"LoRA is..."', delay: 5000 }
            ];
            
            const colors = [
                'linear-gradient(135deg, #3b82f6, #2563eb)',  // Blue
                'linear-gradient(135deg, #8b5cf6, #7c3aed)',  // Purple
                'linear-gradient(135deg, #ec4899, #db2777)',  // Pink
                'linear-gradient(135deg, #f59e0b, #d97706)',  // Orange
                'linear-gradient(135deg, #10b981, #059669)',  // Green
                'linear-gradient(135deg, #06b6d4, #0891b2)'   // Cyan
            ];
            
            pipeline.innerHTML = steps.map((s, idx) => `
                <div id="${s.id}" class="rag-step" style="opacity: 0; background: ${colors[idx]}; color: white; padding: 15px 20px; border-radius: 8px; min-width: 140px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.2); transform: translateY(20px); transition: all 0.5s ease;">
                    <strong style="font-size: 13px;">${s.label}</strong><br/>
                    <span style="font-size: 11px; opacity: 0.9;">${s.content}</span>
                </div>
            `).join('');
            
            steps.forEach((step, i) => {
                setTimeout(() => {
                    const el = document.getElementById(step.id);
                    el.style.opacity = '1';
                    el.style.transform = 'translateY(0)';
                    
                    // Add arrow after each step except last
                    if (i < steps.length - 1) {
                        const arrow = document.createElement('div');
                        arrow.className = 'rag-arrow';
                        arrow.innerHTML = 'â†’';
                        arrow.style.fontSize = '28px';
                        arrow.style.fontWeight = 'bold';
                        arrow.style.color = '#64748b';
                        arrow.style.opacity = '0';
                        arrow.style.transition = 'all 0.3s ease';
                        pipeline.insertBefore(arrow, el.nextSibling);
                        setTimeout(() => {
                            arrow.style.opacity = '1';
                            arrow.style.color = '#06b6d4';
                        }, 200);
                    }
                }, step.delay);
            });
        }

        // Animation #6: Quantization Comparison
        function showQuantization() {
            const container = document.getElementById('quantContainer');
            if (!container) return;
            
            container.innerHTML = `
                <div class="quant-model" style="background: linear-gradient(135deg, #ef4444, #dc2626);">
                    <strong>FP32 Model</strong><br/>
                    <span style="font-size: 24px; font-weight: bold;">28 GB</span><br/>
                    <span style="font-size: 11px;">Full Precision</span>
                </div>
                <div style="font-size: 32px; color: #94a3b8; animation: arrowPulse 2s infinite;">â†’</div>
                <div class="quant-model" style="background: linear-gradient(135deg, #10b981, #059669); animation: quantTransform 2s ease-in-out;">
                    <strong>INT4 Model</strong><br/>
                    <span style="font-size: 24px; font-weight: bold;">3.5 GB</span><br/>
                    <span style="font-size: 11px;">~8Ã— smaller</span>
                </div>
            `;
        }

        // Animation #7: Token Embedding Lookup - Complete Pipeline with Matrix Visualization
        let tokenLookupStep = 0;
        function animateTokenLookup() {
            const container = document.getElementById('tokenLookupContainer');
            if (!container) return;
            
            const examples = [
                { token: 'cat', subwords: ['cat'], id: 2543, vector: '[0.23, -0.45, 0.67, ..., 0.12]', dim: 768 },
                { token: 'running', subwords: ['run', '##ning'], id: '3721+89', vector: '[0.21, -0.43, 0.69, ..., 0.15]', dim: 768 },
                { token: 'king', subwords: ['king'], id: 1829, vector: '[0.56, 0.12, -0.34, ..., 0.78]', dim: 768 }
            ];
            
            function showToken() {
                const current = examples[tokenLookupStep % 3];
                const cleanId = current.id.toString().split('+')[0];
                
                container.innerHTML = `
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 1: Raw Input Text</div>
                        <div style="display: inline-block; padding: 10px 20px; background: linear-gradient(135deg, #3b82f6, #2563eb); color: white; border-radius: 8px; font-size: 16px; font-weight: bold; box-shadow: 0 4px 12px rgba(59, 130, 246, 0.4);">
                            "${current.token}"
                        </div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 2: Tokenization (BPE/WordPiece)</div>
                        <div style="display: inline-flex; gap: 5px; justify-content: center; flex-wrap: wrap;">
                            ${current.subwords.map(sw => `<div style="padding: 6px 14px; background: linear-gradient(135deg, #8b5cf6, #7c3aed); color: white; border-radius: 6px; font-family: monospace; font-size: 13px; font-weight: bold; box-shadow: 0 4px 12px rgba(139, 92, 246, 0.4);">${sw}</div>`).join('')}
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">Break into subwords</div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 3: Vocabulary Lookup â†’ Token ID</div>
                        <div style="display: inline-block; padding: 8px 18px; background: linear-gradient(135deg, #ec4899, #db2777); color: white; border-radius: 6px; font-family: monospace; font-size: 15px; font-weight: bold; box-shadow: 0 4px 12px rgba(236, 72, 153, 0.4);">
                            ID: ${cleanId}
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">Integer from vocabulary (0-50,000)</div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 4: Embedding Matrix Lookup</div>
                        <div style="display: inline-block; padding: 10px; background: linear-gradient(135deg, #f59e0b, #d97706); color: white; border-radius: 8px; box-shadow: 0 4px 12px rgba(245, 158, 11, 0.4);">
                            <div style="font-family: monospace; font-size: 11px; text-align: left; line-height: 1.4;">
                                <div style="opacity: 0.6;">Row 0:    [0.12, -0.45, 0.78, ...]</div>
                                <div style="opacity: 0.6;">Row 1:    [0.34,  0.12, -0.67, ...]</div>
                                <div style="opacity: 0.6;">...</div>
                                <div style="background: rgba(255,255,255,0.2); padding: 3px 6px; border-radius: 4px; font-weight: bold;">Row ${cleanId}: [${current.vector}] â† COPY THIS!</div>
                                <div style="opacity: 0.6;">...</div>
                                <div style="opacity: 0.6;">Row 50000: [...]</div>
                            </div>
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">Pre-trained matrix (50K rows Ã— 768 cols)</div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 5: Dense Vector (768 Decimals)</div>
                        <div style="display: inline-block; padding: 10px 18px; background: linear-gradient(135deg, #10b981, #059669); color: white; border-radius: 6px; font-family: monospace; font-size: 12px; max-width: 300px; overflow: hidden; text-overflow: ellipsis; box-shadow: 0 4px 12px rgba(16, 185, 129, 0.4);">
                            ${current.vector}
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">âœ“ Ready for transformer! (captures word meaning)</div>
                    </div>
                    <div style="margin-top: 12px; padding: 8px 12px; background: rgba(6, 182, 212, 0.1); border-left: 3px solid #06b6d4; border-radius: 4px; text-align: left; font-size: 11px; color: #cbd5e1;">
                        <strong style="color: #06b6d4;">ğŸ’¡ Key Insight:</strong> Token ID ${cleanId} is just an index! The embedding matrix is a <strong>lookup table of pre-trained decimal values</strong>. We simply copy row ${cleanId} to get our vector.
                    </div>
                `;
                
                tokenLookupStep++;
                setTimeout(showToken, 5000);
            }
            
            showToken();
        }

        // Animation #8: Neural Flow with Toggle (Canvas particle system)
        let neuralFlowEnabled = localStorage.getItem('neuralFlowEnabled') !== 'false';
        let neuralFlowAnimationId = null;
        
        function toggleNeuralFlow(enabled) {
            neuralFlowEnabled = enabled;
            localStorage.setItem('neuralFlowEnabled', enabled);
            
            // Update toggle switch visual state
            const toggleSwitch = document.querySelector('.toggle-switch');
            if (toggleSwitch) {
                if (enabled) {
                    toggleSwitch.classList.add('active');
                } else {
                    toggleSwitch.classList.remove('active');
                }
            }
            
            if (enabled) {
                startNeuralFlow();
            } else {
                if (neuralFlowAnimationId) {
                    cancelAnimationFrame(neuralFlowAnimationId);
                    neuralFlowAnimationId = null;
                }
                const canvas = document.getElementById('neuralFlowCanvas');
                if (canvas) {
                    const ctx = canvas.getContext('2d');
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    ctx.fillStyle = '#1a1a2e';
                    ctx.fillRect(0, 0, canvas.width, canvas.height);
                    ctx.fillStyle = '#94a3b8';
                    ctx.font = '14px monospace';
                    ctx.textAlign = 'center';
                    ctx.fillText('Animation Paused', canvas.width / 2, canvas.height / 2);
                }
            }
        }
        
        function startNeuralFlow() {
            const canvas = document.getElementById('neuralFlowCanvas');
            if (!canvas || !neuralFlowEnabled) return;
            
            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 400;
            
            // Particle system
            const layers = [
                { x: 100, y: 200, nodes: 8 },
                { x: 250, y: 200, nodes: 16 },
                { x: 400, y: 200, nodes: 16 },
                { x: 550, y: 200, nodes: 8 }
            ];
            
            const particles = [];
            for (let i = 0; i < 20; i++) {
                particles.push({
                    x: 50,
                    y: 100 + Math.random() * 200,
                    vx: 1 + Math.random(),
                    vy: (Math.random() - 0.5) * 0.5,
                    life: Math.random()
                });
            }
            
            function draw() {
                if (!neuralFlowEnabled) return;
                
                ctx.fillStyle = 'rgba(26, 26, 46, 0.1)';
                ctx.fillRect(0, 0, canvas.width, canvas.height);
                
                // Draw layer nodes
                layers.forEach((layer, layerIdx) => {
                    const spacing = 300 / layer.nodes;
                    for (let i = 0; i < layer.nodes; i++) {
                        const y = 50 + i * spacing;
                        ctx.fillStyle = `rgba(59, 130, 246, ${0.3 + layerIdx * 0.1})`;
                        ctx.beginPath();
                        ctx.arc(layer.x, y, 4, 0, Math.PI * 2);
                        ctx.fill();
                    }
                });
                
                // Update and draw particles
                particles.forEach(p => {
                    p.x += p.vx;
                    p.y += p.vy;
                    p.life += 0.01;
                    
                    if (p.x > canvas.width) {
                        p.x = 50;
                        p.y = 100 + Math.random() * 200;
                        p.life = 0;
                    }
                    
                    const alpha = Math.sin(p.life * Math.PI) * 0.7;
                    ctx.fillStyle = `rgba(6, 182, 212, ${alpha})`;
                    ctx.beginPath();
                    ctx.arc(p.x, p.y, 2, 0, Math.PI * 2);
                    ctx.fill();
                });
                
                // Draw connections
                ctx.strokeStyle = 'rgba(100, 100, 100, 0.1)';
                ctx.lineWidth = 1;
                for (let i = 0; i < layers.length - 1; i++) {
                    ctx.beginPath();
                    ctx.moveTo(layers[i].x, 200);
                    ctx.lineTo(layers[i + 1].x, 200);
                    ctx.stroke();
                }
                
                neuralFlowAnimationId = requestAnimationFrame(draw);
            }
            
            if (neuralFlowAnimationId) cancelAnimationFrame(neuralFlowAnimationId);
            draw();
        }

        // Animation #9: LangChain Pipeline Flow
        function animateLangChain() {
            const pipeline = document.getElementById('langchainPipeline');
            if (!pipeline) return;
            
            const steps = [
                { id: 'lcPrompt', label: '1. PromptTemplate', content: 'Format input', delay: 0, color: 'linear-gradient(135deg, #3b82f6, #2563eb)' },
                { id: 'lcLoader', label: '2. Document Loader', content: 'Load docs', delay: 800, color: 'linear-gradient(135deg, #8b5cf6, #7c3aed)' },
                { id: 'lcSplit', label: '3. Text Splitter', content: 'Chunk text', delay: 1600, color: 'linear-gradient(135deg, #ec4899, #db2777)' },
                { id: 'lcEmbed', label: '4. Embeddings', content: 'Vectorize', delay: 2400, color: 'linear-gradient(135deg, #f59e0b, #d97706)' },
                { id: 'lcVector', label: '5. VectorStore', content: 'Store & search', delay: 3200, color: 'linear-gradient(135deg, #10b981, #059669)' },
                { id: 'lcRetriever', label: '6. Retriever', content: 'Find relevant', delay: 4000, color: 'linear-gradient(135deg, #06b6d4, #0891b2)' },
                { id: 'lcLLM', label: '7. LLM Chain', content: 'Generate answer', delay: 4800, color: 'linear-gradient(135deg, #8b5cf6, #6d28d9)' },
                { id: 'lcOutput', label: '8. Output Parser', content: 'Format result', delay: 5600, color: 'linear-gradient(135deg, #10b981, #047857)' }
            ];
            
            pipeline.innerHTML = steps.map(s => `
                <div id="${s.id}" style="opacity: 0; background: ${s.color}; color: white; padding: 12px 18px; border-radius: 8px; min-width: 130px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.2); transform: translateY(20px); transition: all 0.5s ease;">
                    <strong style="font-size: 12px;">${s.label}</strong><br/>
                    <span style="font-size: 10px; opacity: 0.9;">${s.content}</span>
                </div>
            `).join('');
            
            steps.forEach((step, i) => {
                setTimeout(() => {
                    const el = document.getElementById(step.id);
                    if (el) {
                        el.style.opacity = '1';
                        el.style.transform = 'translateY(0)';
                    }
                    
                    // Add arrow after each step except last
                    if (i < steps.length - 1) {
                        const arrow = document.createElement('div');
                        arrow.innerHTML = 'â†’';
                        arrow.style.fontSize = '24px';
                        arrow.style.fontWeight = 'bold';
                        arrow.style.color = '#64748b';
                        arrow.style.opacity = '0';
                        arrow.style.transition = 'all 0.3s ease';
                        pipeline.insertBefore(arrow, el.nextSibling);
                        setTimeout(() => {
                            arrow.style.opacity = '1';
                            arrow.style.color = '#06b6d4';
                        }, 200);
                    }
                }, step.delay);
            });
        }

        // Animation #10: W&B Live Experiment Tracking
        function animateWandB() {
            const dashboard = document.getElementById('wandbDashboard');
            if (!dashboard) return;
            
            // Initialize dashboard with 3 experiments
            dashboard.innerHTML = `
                <div style="margin-bottom: 20px;">
                    <h4 style="color: #92400e; margin-bottom: 10px; text-align: center;">Experiment Dashboard: Llama-3-8B Fine-Tuning</h4>
                    <div style="display: flex; gap: 10px; justify-content: center; margin-bottom: 15px;">
                        <div style="padding: 6px 12px; background: rgba(59, 130, 246, 0.2); border-left: 3px solid #3b82f6; border-radius: 4px; font-size: 11px;">
                            <strong>Exp-1:</strong> lr=1e-5, r=8
                        </div>
                        <div style="padding: 6px 12px; background: rgba(16, 185, 129, 0.2); border-left: 3px solid #10b981; border-radius: 4px; font-size: 11px;">
                            <strong>Exp-2:</strong> lr=2e-5, r=16
                        </div>
                        <div style="padding: 6px 12px; background: rgba(245, 158, 11, 0.2); border-left: 3px solid #f59e0b; border-radius: 4px; font-size: 11px;">
                            <strong>Exp-3:</strong> lr=5e-5, r=32
                        </div>
                    </div>
                </div>
                <div style="display: flex; gap: 15px; flex-wrap: wrap; justify-content: center;">
                    <div style="flex: 1; min-width: 250px; background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <div style="font-weight: 600; color: #92400e; margin-bottom: 10px; font-size: 13px;">ğŸ“‰ Training Loss</div>
                        <canvas id="lossChart" width="250" height="150"></canvas>
                        <div id="lossValue" style="margin-top: 8px; text-align: center; font-size: 11px; color: #78716c;">Epoch: 0 / 10</div>
                    </div>
                    <div style="flex: 1; min-width: 250px; background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <div style="font-weight: 600; color: #92400e; margin-bottom: 10px; font-size: 13px;">ğŸ“Š GPU Usage</div>
                        <canvas id="gpuChart" width="250" height="150"></canvas>
                        <div id="gpuValue" style="margin-top: 8px; text-align: center; font-size: 11px; color: #78716c;">GPU: 0%</div>
                    </div>
                </div>
                <div id="finalSelection" style="margin-top: 20px; padding: 15px; background: rgba(16, 185, 129, 0.1); border-left: 4px solid #10b981; border-radius: 8px; opacity: 0; transition: opacity 0.5s;">
                    <strong style="color: #065f46;">ğŸ† Best Model Selected:</strong> Experiment-2 (lr=2e-5, r=16)<br>
                    <span style="font-size: 12px; color: #047857;">Final Loss: 0.42 | Training Time: 2.1 hrs | Saved to Model Registry âœ“</span>
                </div>
            `;
            
            // Setup canvases
            const lossCanvas = document.getElementById('lossChart');
            const gpuCanvas = document.getElementById('gpuChart');
            const lossCtx = lossCanvas.getContext('2d');
            const gpuCtx = gpuCanvas.getContext('2d');
            
            // Training data simulation
            const epochs = 10;
            const exp1Loss = [2.5, 1.8, 1.4, 1.1, 0.9, 0.75, 0.65, 0.58, 0.52, 0.48];
            const exp2Loss = [2.5, 1.6, 1.1, 0.8, 0.6, 0.5, 0.45, 0.43, 0.42, 0.42]; // Best
            const exp3Loss = [2.5, 1.5, 0.9, 0.7, 0.6, 0.58, 0.57, 0.57, 0.58, 0.59]; // Overfits
            
            const exp1GPU = [75, 78, 80, 82, 81, 79, 78, 77, 76, 75];
            const exp2GPU = [85, 87, 88, 89, 88, 87, 86, 85, 84, 83];
            const exp3GPU = [92, 94, 95, 96, 96, 95, 94, 93, 92, 91];
            
            let currentEpoch = 0;
            
            function drawLossChart(epoch) {
                lossCtx.clearRect(0, 0, 250, 150);
                
                // Draw grid
                lossCtx.strokeStyle = '#e5e7eb';
                lossCtx.lineWidth = 1;
                for (let i = 0; i <= 4; i++) {
                    lossCtx.beginPath();
                    lossCtx.moveTo(0, i * 37.5);
                    lossCtx.lineTo(250, i * 37.5);
                    lossCtx.stroke();
                }
                
                // Draw lines
                const drawLine = (data, color, width) => {
                    lossCtx.strokeStyle = color;
                    lossCtx.lineWidth = width;
                    lossCtx.beginPath();
                    for (let i = 0; i <= epoch; i++) {
                        const x = (i / 9) * 230 + 10;
                        const y = 140 - ((data[i] / 2.5) * 130);
                        if (i === 0) lossCtx.moveTo(x, y);
                        else lossCtx.lineTo(x, y);
                    }
                    lossCtx.stroke();
                    
                    // Draw point at current position
                    if (epoch < 10) {
                        const x = (epoch / 9) * 230 + 10;
                        const y = 140 - ((data[epoch] / 2.5) * 130);
                        lossCtx.fillStyle = color;
                        lossCtx.beginPath();
                        lossCtx.arc(x, y, 4, 0, 2 * Math.PI);
                        lossCtx.fill();
                    }
                };
                
                drawLine(exp1Loss, '#3b82f6', 2);
                drawLine(exp2Loss, '#10b981', 3); // Thicker = winner
                drawLine(exp3Loss, '#f59e0b', 2);
                
                // Labels
                lossCtx.fillStyle = '#92400e';
                lossCtx.font = '10px sans-serif';
                lossCtx.fillText('2.5', 2, 12);
                lossCtx.fillText('0.0', 2, 145);
            }
            
            function drawGPUChart(epoch) {
                gpuCtx.clearRect(0, 0, 250, 150);
                
                // Draw grid
                gpuCtx.strokeStyle = '#e5e7eb';
                gpuCtx.lineWidth = 1;
                for (let i = 0; i <= 4; i++) {
                    gpuCtx.beginPath();
                    gpuCtx.moveTo(0, i * 37.5);
                    gpuCtx.lineTo(250, i * 37.5);
                    gpuCtx.stroke();
                }
                
                // Draw bars
                const drawBars = (data, colors) => {
                    const barWidth = 20;
                    const gap = 5;
                    for (let i = 0; i <= epoch && i < 10; i++) {
                        const x = (i / 9) * 200 + 25;
                        colors.forEach((color, j) => {
                            const height = (data[j][i] / 100) * 140;
                            const barX = x + (j - 1) * (barWidth + gap);
                            gpuCtx.fillStyle = color;
                            gpuCtx.fillRect(barX, 145 - height, barWidth, height);
                        });
                    }
                };
                
                drawBars([exp1GPU, exp2GPU, exp3GPU], ['#3b82f680', '#10b98180', '#f59e0b80']);
                
                // Labels
                gpuCtx.fillStyle = '#92400e';
                gpuCtx.font = '10px sans-serif';
                gpuCtx.fillText('100%', 2, 12);
                gpuCtx.fillText('0%', 2, 145);
            }
            
            // Animate training
            const interval = setInterval(() => {
                if (currentEpoch < epochs) {
                    drawLossChart(currentEpoch);
                    drawGPUChart(currentEpoch);
                    
                    document.getElementById('lossValue').textContent = 
                        `Epoch: ${currentEpoch + 1} / ${epochs} | Exp-2 Loss: ${exp2Loss[currentEpoch].toFixed(2)}`;
                    document.getElementById('gpuValue').textContent = 
                        `GPU: Exp-1: ${exp1GPU[currentEpoch]}% | Exp-2: ${exp2GPU[currentEpoch]}% | Exp-3: ${exp3GPU[currentEpoch]}%`;
                    
                    currentEpoch++;
                } else {
                    clearInterval(interval);
                    // Show final selection
                    setTimeout(() => {
                        document.getElementById('finalSelection').style.opacity = '1';
                    }, 500);
                }
            }, 600);
        }

        // Animation #11: Unsloth Fine-Tuning Speed Race
        function animateUnsloth() {
            const race = document.getElementById('unslothRace');
            if (!race) return;
            
            race.innerHTML = `
                <div style="margin-bottom: 20px;">
                    <h4 style="color: #581c87; margin-bottom: 15px; text-align: center;">Fine-Tuning Llama-3-8B: Speed Comparison</h4>
                </div>
                
                <div style="margin-bottom: 25px;">
                    <div style="margin-bottom: 15px;">
                        <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                            <span style="font-weight: 600; color: #be123c; font-size: 13px;">ğŸ¢ Standard Training (10 hours)</span>
                            <span id="standardTime" style="font-size: 12px; color: #78716c;">0:00 / 10:00</span>
                        </div>
                        <div style="background: #fecdd3; border-radius: 8px; height: 30px; position: relative; overflow: hidden;">
                            <div id="standardBar" style="background: linear-gradient(135deg, #be123c, #e11d48); height: 100%; width: 0%; transition: width 0.3s; display: flex; align-items: center; padding-left: 10px;">
                                <span style="color: white; font-size: 11px; font-weight: 600;"></span>
                            </div>
                        </div>
                        <div style="display: flex; justify-content: space-between; margin-top: 8px; font-size: 11px; color: #57534e;">
                            <div>ğŸ’¾ Memory: <span id="standardMem">120GB</span></div>
                            <div>ğŸ’° Cost: <span id="standardCost">$0</span></div>
                            <div>âš¡ Speed: <span id="standardSpeed">1x</span></div>
                        </div>
                    </div>
                    
                    <div style="margin-bottom: 15px;">
                        <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                            <span style="font-weight: 600; color: #7c3aed; font-size: 13px;">âš¡ Unsloth + QLoRA (2 hours)</span>
                            <span id="unslothTime" style="font-size: 12px; color: #78716c;">0:00 / 2:00</span>
                        </div>
                        <div style="background: #e9d5ff; border-radius: 8px; height: 30px; position: relative; overflow: hidden;">
                            <div id="unslothBar" style="background: linear-gradient(135deg, #8b5cf6, #7c3aed); height: 100%; width: 0%; transition: width 0.3s; display: flex; align-items: center; padding-left: 10px;">
                                <span style="color: white; font-size: 11px; font-weight: 600;"></span>
                            </div>
                        </div>
                        <div style="display: flex; justify-content: space-between; margin-top: 8px; font-size: 11px; color: #57534e;">
                            <div>ğŸ’¾ Memory: <span id="unslothMem">16GB</span></div>
                            <div>ğŸ’° Cost: <span id="unslothCost">$0</span></div>
                            <div>âš¡ Speed: <span id="unslothSpeed">5x</span></div>
                        </div>
                    </div>
                </div>
                
                <div style="background: white; padding: 15px; border-radius: 8px; margin-bottom: 15px;">
                    <div style="font-weight: 600; color: #581c87; margin-bottom: 10px; font-size: 13px;">ğŸ“ˆ Training Metrics</div>
                    <div style="display: flex; gap: 20px; flex-wrap: wrap; font-size: 12px;">
                        <div>
                            <div style="color: #78716c;">Loss</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="currentLoss">2.500</div>
                        </div>
                        <div>
                            <div style="color: #78716c;">Samples/sec</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="throughput">0</div>
                        </div>
                        <div>
                            <div style="color: #78716c;">GPU Temp</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="gpuTemp">45Â°C</div>
                        </div>
                        <div>
                            <div style="color: #78716c;">Progress</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="progress">0%</div>
                        </div>
                    </div>
                </div>
                
                <div id="raceWinner" style="padding: 15px; background: rgba(139, 92, 246, 0.1); border-left: 4px solid #7c3aed; border-radius: 8px; opacity: 0; transition: opacity 0.5s;">
                    <strong style="color: #581c87;">ğŸ† Unsloth Wins!</strong> 5x faster, 7.5x less memory, same quality.<br>
                    <span style="font-size: 12px; color: #6b21a8;">Standard: 10 hrs on 8Ã—A100 ($360) | Unsloth: 2 hrs on RTX 4090 ($0) ğŸ’°</span>
                </div>
            `;
            
            // Simulate training race
            let elapsedSec = 0;
            const totalStandard = 600; // 10 hours = 600 units
            const totalUnsloth = 120;  // 2 hours = 120 units
            const updateInterval = 50; // Update every 50ms for smooth animation
            
            const losses = [2.5, 2.1, 1.8, 1.5, 1.2, 1.0, 0.8, 0.65, 0.52, 0.45, 0.42];
            
            const interval = setInterval(() => {
                elapsedSec++;
                
                // Update progress bars
                const standardProgress = Math.min((elapsedSec / totalStandard) * 100, 100);
                const unslothProgress = Math.min((elapsedSec / totalUnsloth) * 100, 100);
                
                document.getElementById('standardBar').style.width = standardProgress + '%';
                document.getElementById('unslothBar').style.width = unslothProgress + '%';
                
                // Update times
                const standardHours = Math.floor((elapsedSec / 60));
                const standardMins = Math.floor((elapsedSec % 60));
                document.getElementById('standardTime').textContent = 
                    `${standardHours}:${standardMins.toString().padStart(2, '0')} / 10:00`;
                
                const unslothMins = Math.floor((elapsedSec / 60) * 5);
                const unslothSecs = Math.floor(((elapsedSec / 60) * 5 % 1) * 60);
                if (unslothMins < 2 || (unslothMins === 2 && unslothSecs === 0)) {
                    document.getElementById('unslothTime').textContent = 
                        `${unslothMins}:${unslothSecs.toString().padStart(2, '0')} / 2:00`;
                } else {
                    document.getElementById('unslothTime').textContent = '2:00 / 2:00 âœ“';
                }
                
                // Update metrics
                const lossIndex = Math.min(Math.floor((elapsedSec / totalUnsloth) * 10), 10);
                document.getElementById('currentLoss').textContent = losses[lossIndex].toFixed(3);
                document.getElementById('throughput').textContent = (250 + Math.random() * 50).toFixed(0);
                document.getElementById('gpuTemp').textContent = (65 + Math.random() * 15).toFixed(0) + 'Â°C';
                document.getElementById('progress').textContent = Math.floor(unslothProgress) + '%';
                
                // Update costs (AWS A100 = $3.60/hr)
                document.getElementById('standardCost').textContent = '$' + ((elapsedSec / 60) * 8 * 3.60).toFixed(2);
                document.getElementById('unslothCost').textContent = '$0.00'; // Local GPU
                
                // Unsloth finishes first
                if (elapsedSec >= totalUnsloth) {
                    document.getElementById('unslothBar').innerHTML = '<span style="color: white; font-size: 11px; font-weight: 600; padding-left: 10px;">âœ“ COMPLETE</span>';
                    document.getElementById('raceWinner').style.opacity = '1';
                }
                
                // Stop when standard finishes
                if (elapsedSec >= totalStandard) {
                    document.getElementById('standardBar').innerHTML = '<span style="color: white; font-size: 11px; font-weight: 600; padding-left: 10px;">âœ“ COMPLETE</span>';
                    clearInterval(interval);
                }
            }, updateInterval);
        }
        
        // Set initial toggle state
        window.addEventListener('DOMContentLoaded', () => {
            const toggle = document.getElementById('neuralFlowToggle');
            const toggleSwitch = document.querySelector('.toggle-switch');
            
            if (toggle) {
                toggle.checked = neuralFlowEnabled;
                
                // Set visual state
                if (toggleSwitch) {
                    if (neuralFlowEnabled) {
                        toggleSwitch.classList.add('active');
                    } else {
                        toggleSwitch.classList.remove('active');
                    }
                }
                
                if (neuralFlowEnabled) {
                    setTimeout(startNeuralFlow, 500);
                }
            }
        });
    </script>
</body>
</html>