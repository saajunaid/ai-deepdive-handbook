<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI DeepDive Handbook - Complete Technical Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.7;
            color: #1a1a2e;
            background: linear-gradient(135deg, #0f2027 0%, #203a43 50%, #2c5364 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            background: rgba(255, 255, 255, 0.98);
            backdrop-filter: blur(20px);
            padding: 40px;
            border-radius: 16px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.15);
            margin-bottom: 30px;
            text-align: center;
            animation: slideDown 0.6s ease-out;
        }

        @keyframes slideDown {
            from {
                opacity: 0;
                transform: translateY(-30px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        h1 {
            font-size: 2.8em;
            background: linear-gradient(135deg, #1e3a8a 0%, #3b82f6 50%, #06b6d4 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 12px;
            font-weight: 700;
            animation: fadeIn 0.8s ease-out;
        }

        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }

        .subtitle {
            color: #4a5568;
            font-size: 1.2em;
            margin-bottom: 8px;
            font-weight: 500;
        }

        .nav-pills {
            display: flex;
            gap: 12px;
            flex-wrap: wrap;
            justify-content: center;
            margin-top: 24px;
        }

        .nav-pill {
            padding: 12px 24px;
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
            border: none;
            border-radius: 10px;
            cursor: pointer;
            font-size: 14px;
            font-weight: 600;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3);
        }

        .nav-pill:hover {
            transform: translateY(-2px) scale(1.05);
            box-shadow: 0 8px 20px rgba(30, 64, 175, 0.5);
        }

        .nav-pill.active {
            background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
        }

        .content-wrapper {
            display: grid;
            grid-template-columns: 280px 1fr;
            gap: 24px;
            animation: fadeIn 0.8s ease-out 0.2s both;
        }

        .sidebar {
            background: rgba(255, 255, 255, 0.98);
            backdrop-filter: blur(20px);
            padding: 24px;
            border-radius: 16px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.12);
            height: fit-content;
            position: sticky;
            top: 20px;
        }

        .sidebar h3 {
            color: #2d3748;
            margin-bottom: 20px;
            font-size: 1.1em;
            font-weight: 700;
        }

        .sidebar-link {
            display: block;
            padding: 12px 16px;
            color: #4a5568;
            text-decoration: none;
            border-radius: 8px;
            margin-bottom: 6px;
            transition: all 0.25s ease;
            font-size: 14px;
            font-weight: 500;
            border-left: 3px solid transparent;
            cursor: pointer;
        }

        .sidebar-link:hover {
            background: linear-gradient(135deg, #edf2f7 0%, #e2e8f0 100%);
            color: #2d3748;
            border-left-color: #2563eb;
            transform: translateX(4px);
        }

        .sidebar-link.active {
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
        }

        .content {
            background: rgba(255, 255, 255, 0.98);
            backdrop-filter: blur(20px);
            padding: 48px;
            border-radius: 16px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.12);
        }

        .section {
            display: none;
            animation: fadeInUp 0.5s ease-out;
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .section.active {
            display: block;
        }

        h2 {
            color: #1a202c;
            font-size: 2.2em;
            margin-bottom: 24px;
            border-bottom: 3px solid #2563eb;
            padding-bottom: 12px;
            font-weight: 700;
        }

        h3 {
            color: #2d3748;
            font-size: 1.6em;
            margin-top: 36px;
            margin-bottom: 16px;
            font-weight: 700;
        }

        h4 {
            color: #4a5568;
            font-size: 1.25em;
            margin-top: 24px;
            margin-bottom: 12px;
            font-weight: 600;
        }

        .must-know {
            background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
            border-left: 5px solid #f59e0b;
            padding: 20px;
            border-radius: 12px;
            margin: 24px 0;
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.15);
            animation: pulse 2s ease-in-out infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.01); }
        }

        .must-know::before {
            content: "â­ MUST KNOW";
            display: block;
            font-weight: 700;
            color: #92400e;
            margin-bottom: 12px;
            font-size: 1.05em;
        }

        .concept-card {
            background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%);
            padding: 28px;
            border-radius: 12px;
            margin: 24px 0;
            border-left: 4px solid #2563eb;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.06);
            transition: all 0.3s ease;
        }

        .concept-card:hover {
            box-shadow: 0 8px 20px rgba(0, 0, 0, 0.1);
            transform: translateY(-2px);
        }

        .mental-model {
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #6366f1;
        }

        .mental-model::before {
            content: "ğŸ’¡ Mental Model";
            display: block;
            font-weight: 700;
            color: #4338ca;
            margin-bottom: 12px;
        }

        .tools-box {
            background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #3b82f6;
        }

        .tools-box::before {
            content: "ğŸ› ï¸ Tools & Technologies";
            display: block;
            font-weight: 700;
            color: #1e40af;
            margin-bottom: 12px;
        }

        .example-box {
            background: linear-gradient(135deg, #d1fae5 0%, #a7f3d0 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #10b981;
        }

        .example-box::before {
            content: "ğŸ“ Example";
            display: block;
            font-weight: 700;
            color: #065f46;
            margin-bottom: 12px;
        }

        .faq-box {
            background: linear-gradient(135deg, #fce7f3 0%, #fbcfe8 100%);
            padding: 24px;
            border-radius: 12px;
            margin: 20px 0;
            border-left: 4px solid #ec4899;
        }

        .faq-box::before {
            content: "â“ FAQ";
            display: block;
            font-weight: 700;
            color: #9f1239;
            margin-bottom: 12px;
        }

        code {
            background: #2d3748;
            color: #e2e8f0;
            padding: 3px 8px;
            border-radius: 5px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 24px;
            border-radius: 10px;
            overflow-x: auto;
            margin: 20px 0;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.2);
        }

        pre code {
            background: none;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 24px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);
        }

        th {
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
            padding: 16px;
            text-align: left;
            font-weight: 600;
        }

        td {
            padding: 14px 16px;
            border-bottom: 1px solid #e2e8f0;
        }

        tr:hover {
            background: #f7fafc;
        }

        ul, ol {
            margin-left: 24px;
            margin-top: 12px;
            margin-bottom: 12px;
        }

        li {
            margin-bottom: 10px;
            color: #4a5568;
        }

        .diagram {
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border: 2px solid #e2e8f0;
            font-family: monospace;
            overflow-x: auto;
        }

        .animated-diagram {
            position: relative;
            min-height: 200px;
        }

        .back-to-top {
            position: fixed;
            bottom: 32px;
            right: 32px;
            background: linear-gradient(135deg, #1e40af 0%, #3b82f6 100%);
            color: white;
            width: 56px;
            height: 56px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            cursor: pointer;
            box-shadow: 0 6px 20px rgba(30, 64, 175, 0.4);
            transition: all 0.3s ease;
            opacity: 0;
            pointer-events: none;
            font-size: 24px;
            z-index: 1000;
        }

        .back-to-top.visible {
            opacity: 1;
            pointer-events: auto;
        }

        .back-to-top:hover {
            transform: translateY(-5px) scale(1.1);
            box-shadow: 0 10px 28px rgba(30, 64, 175, 0.6);
        }

        .progress-bar {
            position: fixed;
            top: 0;
            left: 0;
            height: 4px;
            background: linear-gradient(90deg, #1e40af 0%, #3b82f6 50%, #06b6d4 100%);
            z-index: 1000;
            transition: width 0.3s ease;
        }

        @media (max-width: 768px) {
            .content-wrapper {
                grid-template-columns: 1fr;
            }
            .sidebar {
                position: static;
            }
            h1 {
                font-size: 2em;
            }
            .content {
                padding: 24px;
            }
        }

        .collapsible {
            cursor: pointer;
            user-select: none;
        }

        .collapsible::before {
            content: "â–¶ ";
            transition: transform 0.3s ease;
            display: inline-block;
        }

        .collapsible.expanded::before {
            transform: rotate(90deg);
        }

        .collapsible-content {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }

        .collapsible-content.expanded {
            max-height: 5000px;
        }

        /* Tooltip Styles */
        .tooltip {
            position: relative;
            cursor: help;
            border-bottom: 1px dotted #2563eb;
            color: #1e40af;
            font-weight: 500;
        }

        .tooltip .tooltiptext {
            visibility: hidden;
            width: max-content;
            max-width: 300px;
            background: linear-gradient(135deg, #1e293b 0%, #0f172a 100%);
            color: #f8fafc;
            text-align: center;
            border-radius: 8px;
            padding: 8px 12px;
            position: absolute;
            z-index: 1000;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            opacity: 0;
            transition: opacity 0.3s, visibility 0.3s;
            font-size: 0.85em;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
            white-space: nowrap;
        }

        .tooltip .tooltiptext::after {
            content: "";
            position: absolute;
            top: 100%;
            left: 50%;
            margin-left: -5px;
            border-width: 5px;
            border-style: solid;
            border-color: #0f172a transparent transparent transparent;
        }

        .tooltip:hover .tooltiptext {
            visibility: visible;
            opacity: 1;
        }

        /* Animation Styles */
        @keyframes pulse {
            0%, 100% { opacity: 0.3; }
            50% { opacity: 1; }
        }

        @keyframes flowData {
            0% { transform: translateX(-100%); opacity: 0; }
            50% { opacity: 1; }
            100% { transform: translateX(100%); opacity: 0; }
        }

        @keyframes processingCore {
            0%, 100% { background-color: #1e40af; }
            50% { background-color: #06b6d4; }
        }

        .gpu-core-animated {
            display: inline-block;
            width: 10px;
            height: 10px;
            background: #1e40af;
            margin: 2px;
            animation: processingCore 2s ease-in-out infinite;
        }

        .cpu-core-animated {
            display: inline-block;
            padding: 8px 12px;
            background: linear-gradient(135deg, #1e40af, #3b82f6);
            border-radius: 4px;
            animation: pulse 3s ease-in-out infinite;
            margin: 4px;
        }

        .data-flow {
            position: relative;
            overflow: hidden;
            padding: 20px;
            background: linear-gradient(90deg, transparent 0%, rgba(30, 64, 175, 0.1) 50%, transparent 100%);
        }

        .data-flow::before {
            content: "â†’ DATA â†’";
            position: absolute;
            left: 0;
            top: 50%;
            transform: translateY(-50%);
            animation: flowData 3s linear infinite;
            color: #06b6d4;
            font-weight: bold;
        }

        .attention-visual {
            position: relative;
            display: inline-block;
            padding: 10px 20px;
            border: 2px solid #1e40af;
            border-radius: 8px;
            background: linear-gradient(135deg, rgba(30, 64, 175, 0.1), rgba(59, 130, 246, 0.1));
            animation: pulse 2s ease-in-out infinite;
        }

        /* Interactive GPU Cores Visualization */
        .gpu-cores-container {
            display: grid;
            grid-template-columns: repeat(16, 1fr);
            gap: 3px;
            max-width: 400px;
            margin: 20px auto;
            padding: 10px;
            background: rgba(15, 32, 39, 0.3);
            border-radius: 8px;
        }

        .gpu-core {
            width: 100%;
            aspect-ratio: 1;
            background: #1e40af;
            border-radius: 2px;
            transition: all 0.3s ease;
        }

        .gpu-core.active {
            background: #06b6d4;
            box-shadow: 0 0 8px #06b6d4;
        }

        .gpu-cores-container:hover .gpu-core {
            animation: processingCore 1.5s ease-in-out infinite;
        }

        .gpu-cores-container:hover .gpu-core:nth-child(even) {
            animation-delay: 0.1s;
        }

        .gpu-cores-container:hover .gpu-core:nth-child(3n) {
            animation-delay: 0.2s;
        }

        .transformer-flow {
            display: flex;
            align-items: center;
            justify-content: space-around;
            padding: 20px;
            position: relative;
        }

        .transformer-step {
            display: flex;
            flex-direction: column;
            align-items: center;
            padding: 15px;
            background: linear-gradient(135deg, rgba(30, 64, 175, 0.1), rgba(59, 130, 246, 0.1));
            border-radius: 8px;
            border: 2px solid #1e40af;
            min-width: 120px;
            transition: all 0.3s ease;
        }

        .transformer-step:hover {
            transform: translateY(-5px);
            box-shadow: 0 8px 16px rgba(30, 64, 175, 0.3);
            border-color: #06b6d4;
        }

        .transformer-arrow {
            font-size: 24px;
            color: #1e40af;
            animation: pulse 2s ease-in-out infinite;
        }

        /* Animation Styles for All Visualizations */
        .matrix-cell {
            background: white;
            border: 2px solid #1e40af;
            border-radius: 4px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            color: #1e40af;
            height: 50px;
            transition: all 0.3s ease;
        }

        .matrix-cell.highlight {
            background: linear-gradient(135deg, #fde047, #fbbf24);
            border-color: #f59e0b;
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.4);
        }

        .matrix-cell.result-cell {
            background: rgba(6, 182, 212, 0.1);
            border-color: #06b6d4;
            color: #06b6d4;
        }

        .matrix-cell.result-cell.computed {
            background: linear-gradient(135deg, #06b6d4, #0891b2);
            color: white;
            animation: resultPop 0.5s ease;
        }

        @keyframes resultPop {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.2); }
        }

        /* Embedding Space Styles */
        #embeddingCanvas {
            border: 2px solid #1e40af;
            border-radius: 8px;
            background: white;
            cursor: grab;
        }

        #embeddingCanvas:active {
            cursor: grabbing;
        }

        /* Attention Heatmap Styles */
        .attention-grid {
            display: grid;
            gap: 3px;
            padding: 10px;
            background: rgba(30, 64, 175, 0.05);
            border-radius: 8px;
        }

        .attention-cell {
            aspect-ratio: 1;
            border-radius: 3px;
            transition: all 0.5s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 0.7em;
            font-weight: 600;
            color: white;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.5);
        }

        /* LoRA Injection Styles */
        .lora-container {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 20px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .model-box {
            background: linear-gradient(135deg, #e2e8f0, #cbd5e1);
            border: 3px solid #64748b;
            border-radius: 12px;
            padding: 30px;
            font-weight: 600;
            color: #1e293b;
            position: relative;
            transition: all 0.5s ease;
        }

        .lora-adapter {
            background: linear-gradient(135deg, #fbbf24, #f59e0b);
            border: 3px solid #d97706;
            border-radius: 8px;
            padding: 15px 20px;
            font-weight: 600;
            color: white;
            opacity: 0;
            transform: scale(0.5);
            transition: all 0.5s ease;
        }

        .lora-adapter.active {
            opacity: 1;
            transform: scale(1);
        }

        /* RAG Pipeline Styles */
        .rag-pipeline {
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 15px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .rag-step {
            background: white;
            border: 2px solid #1e40af;
            border-radius: 8px;
            padding: 15px 20px;
            font-weight: 600;
            color: #1e40af;
            min-width: 120px;
            text-align: center;
            opacity: 0.3;
            transition: all 0.5s ease;
        }

        .rag-step.active {
            opacity: 1;
            background: linear-gradient(135deg, #1e40af, #3b82f6);
            color: white;
            transform: scale(1.1);
            box-shadow: 0 6px 16px rgba(30, 64, 175, 0.4);
        }

        .rag-arrow {
            font-size: 24px;
            color: #64748b;
            opacity: 0.3;
            transition: all 0.5s ease;
        }

        .rag-arrow.active {
            opacity: 1;
            color: #06b6d4;
            animation: arrowPulse 1s ease;
        }

        @keyframes arrowPulse {
            0%, 100% { transform: translateX(0); }
            50% { transform: translateX(5px); }
        }

        /* Quantization Comparison Styles */
        .quant-comparison {
            display: flex;
            justify-content: center;
            gap: 30px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .quant-box {
            border-radius: 12px;
            padding: 20px;
            text-align: center;
            transition: all 0.5s ease;
        }

        .quant-fp32 {
            background: linear-gradient(135deg, #3b82f6, #2563eb);
            color: white;
            border: 3px solid #1e40af;
        }

        .quant-int4 {
            background: linear-gradient(135deg, #06b6d4, #0891b2);
            color: white;
            border: 3px solid #0e7490;
            transform: scale(0.6);
        }

        .quant-box.animate {
            animation: quantTransform 2s ease-in-out;
        }

        @keyframes quantTransform {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(0.6); }
        }

        /* Token Embedding Lookup Styles */
        .token-lookup-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            padding: 20px;
            flex-wrap: wrap;
        }

        .token-box, .embedding-vector {
            background: white;
            border: 2px solid #1e40af;
            border-radius: 8px;
            padding: 15px;
            font-weight: 600;
            transition: all 0.5s ease;
        }

        .token-box.highlight, .embedding-vector.highlight {
            background: linear-gradient(135deg, #fde047, #fbbf24);
            border-color: #f59e0b;
            transform: scale(1.1);
            box-shadow: 0 4px 12px rgba(245, 158, 11, 0.4);
        }

        /* Neural Network Flow Canvas */
        #neuralFlowCanvas {
            border: 2px solid #1e40af;
            border-radius: 8px;
            background: linear-gradient(135deg, #f8fafc, #f1f5f9);
        }

        .animation-toggle {
            display: inline-flex;
            align-items: center;
            gap: 10px;
            padding: 8px 16px;
            background: rgba(30, 64, 175, 0.1);
            border-radius: 8px;
            cursor: pointer;
            user-select: none;
            transition: all 0.3s ease;
        }

        .animation-toggle:hover {
            background: rgba(30, 64, 175, 0.2);
        }

        .toggle-switch {
            position: relative;
            width: 50px;
            height: 24px;
            background: #cbd5e1;
            border-radius: 12px;
            transition: all 0.3s ease;
        }

        .toggle-switch.active {
            background: #1e40af;
        }

        .toggle-slider {
            position: absolute;
            top: 2px;
            left: 2px;
            width: 20px;
            height: 20px;
            background: white;
            border-radius: 50%;
            transition: all 0.3s ease;
            box-shadow: 0 2px 4px rgba(0,0,0,0.2);
        }

        .toggle-switch.active .toggle-slider {
            left: 28px;
        }
    </style>
</head>
<body>
    <div class="progress-bar" id="progressBar"></div>
    
    <div class="container">
        <header>
            <h1>ğŸ§  AI DeepDive Handbook</h1>
            <p class="subtitle">Complete Technical Guide: Zero to Agentic AI</p>
            <p style="color: #1f2021;">Curated by: jun<span style="font-weight: 700; color: #e406b4; font-size: 1.1em;">ai</span>d sh<span style="font-weight: 700; color:#e406b4; font-size: 1.1em;">ai</span>k | </em> ğŸ¤– | Last Updated: January 2026</p>
            
            <div class="nav-pills">
                <button class="nav-pill active" onclick="showSection('foundations')">ğŸ”§ Foundations</button>
                <button class="nav-pill" onclick="showSection('architecture')">ğŸ—ï¸ Architecture</button>
                <button class="nav-pill" onclick="showSection('training')">ğŸ“ Training</button>
                <button class="nav-pill" onclick="showSection('deployment')">ğŸ“¦ Deployment</button>
                <button class="nav-pill" onclick="showSection('optimization')">âš¡ Optimization</button>
                <button class="nav-pill" onclick="showSection('advanced')">ğŸš€ Advanced</button>
                <button class="nav-pill" onclick="showSection('tools')">ğŸ› ï¸ Tools</button>
                <button class="nav-pill" onclick="showSection('practical')">ğŸ’» Practical</button>
                <button class="nav-pill" onclick="showSection('keytopics')">ğŸ¯ Key Topics</button>
                <button class="nav-pill" onclick="showSection('glossary')">ğŸ“š Glossary</button>
                <button class="nav-pill" onclick="showSection('economics')">ğŸ’° Economics</button>
            </div>
            
            <div class="search-container" style="margin: 20px auto; max-width: 600px;">
                <input type="text" id="searchInput" placeholder="ğŸ” Search handbook... (try 'GPU', 'RAG', 'LoRA')" 
                       style="width: 100%; padding: 12px 20px; font-size: 16px; border: 2px solid #1e40af; border-radius: 8px; 
                              background: rgba(255, 255, 255, 0.95); box-shadow: 0 4px 12px rgba(30, 64, 175, 0.2);"
                       onkeyup="searchHandbook()" />
                <div id="searchResults" style="margin-top: 10px; font-size: 0.9em; color: #64748b;"></div>
            </div>
        </header>

        <div class="content-wrapper">
            <aside class="sidebar">
                <h3>Quick Navigation</h3>
                <div class="sidebar-link active" onclick="showSection('foundations')">ğŸ”§ Foundations</div>
                <div class="sidebar-link" onclick="showSection('architecture')">ğŸ—ï¸ Transformer Architecture</div>
                <div class="sidebar-link" onclick="showSection('training')">ğŸ“ Training & Models</div>
                <div class="sidebar-link" onclick="showSection('deployment')">ğŸ“¦ Model Deployment</div>
                <div class="sidebar-link" onclick="showSection('optimization')">âš¡ Optimization</div>
                <div class="sidebar-link" onclick="showSection('advanced')">ğŸš€ Advanced Concepts</div>
                <div class="sidebar-link" onclick="showSection('tools')">ğŸ› ï¸ Tools Ecosystem</div>
                <div class="sidebar-link" onclick="showSection('practical')">ğŸ’» Hands-On Roadmap</div>
                <div class="sidebar-link" onclick="showSection('keytopics')">ğŸ¯ Key Topics to Survive</div>
                <div class="sidebar-link" onclick="showSection('glossary')">ğŸ“š Glossary & FAQs</div>
                <div class="sidebar-link" onclick="showSection('economics')">ğŸ’° Economics of AI</div>
            </aside>

            <main class="content">
                <!-- SECTION 1: FOUNDATIONS -->
                <div id="foundations" class="section active">
                    <h2>ğŸ”§ 1. Foundations: Hardware & Mathematical Building Blocks</h2>
                    
                    <div class="must-know">
                        <strong>Why This Matters:</strong> Understanding tensors, vectors, and GPU architecture is essential. These are the foundational concepts that everything else builds upon.
                    </div>

                    <div class="concept-card">
                        <h3>1.1 Vectors & Tensors - The Data Structures of AI</h3>
                        
                        <div class="mental-model">
                            <strong>Simple Mental Model:</strong>
                            <ul>
                                <li><strong>Vector:</strong> A 1D array of numbers - like a single row in a spreadsheet</li>
                                <li><strong>Matrix:</strong> A 2D grid - like a spreadsheet with rows and columns</li>
                                <li><strong>Tensor:</strong> Multi-dimensional arrays - like stacked spreadsheets (3D, 4D, etc.)</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>1D Vector</strong> (5 elements):
[0.2, 0.5, 0.8, 0.1, 0.9]

<strong>2D Matrix</strong> (3 rows Ã— 5 columns):
â”Œ                                    â”
â”‚  0.2   0.5   0.8   0.1   0.9  â”‚
â”‚  0.3   0.6   0.2   0.7   0.4  â”‚
â”‚  0.9   0.1   0.5   0.3   0.8  â”‚
â””                                    â”˜

<strong>3D Tensor</strong> (2 Ã— 3 Ã— 5):
Batch 1                 Batch 2
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ x x x x x â”‚         â”‚ x x x x x â”‚
â”‚ x x x x x â”‚         â”‚ x x x x x â”‚
â”‚ x x x x x â”‚         â”‚ x x x x x â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Real-World Example: Processing Text with GPT</strong>
                            <pre>
Input: "Hello world"
Tokens: ["Hello", "world"]

After tokenization + embedding:
Tensor shape: [1, 2, 768]
             â†‘  â†‘  â†‘
             â”‚  â”‚  â””â”€ Embedding dimension (each word = 768 numbers)
             â”‚  â””â”€â”€â”€â”€ Sequence length (2 tokens)
             â””â”€â”€â”€â”€â”€â”€â”€ Batch size (1 sentence)

Total numbers: 1 Ã— 2 Ã— 768 = 1,536 numbers!</pre>
                        </div>

                        <div class="example-box">
                            <strong>ğŸ¬ Interactive: Matrix Multiplication Animation</strong>
                            <p style="text-align: center; color: #64748b; font-size: 0.9em; margin-bottom: 10px;">
                                <em>Click "Start Animation" to see how matrices multiply! This is the core operation in neural networks.</em>
                            </p>
                            <div style="text-align: center; margin-bottom: 15px;">
                                <button onclick="startMatrixMultiplication()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3);">Start Animation</button>
                            </div>
                            <div id="matrixMultAnimation" style="display: flex; justify-content: center; align-items: center; gap: 20px; padding: 20px; background: rgba(30, 64, 175, 0.05); border-radius: 8px; flex-wrap: wrap;">
                                <div style="text-align: center;">
                                    <div style="font-weight: 600; margin-bottom: 10px; color: #1e40af;">Matrix A (2Ã—3)</div>
                                    <div class="matrix-grid" id="matrixA" style="display: grid; grid-template-columns: repeat(3, 50px); gap: 5px;">
                                        <div class="matrix-cell">2</div><div class="matrix-cell">1</div><div class="matrix-cell">3</div>
                                        <div class="matrix-cell">4</div><div class="matrix-cell">0</div><div class="matrix-cell">1</div>
                                    </div>
                                </div>
                                <div style="font-size: 2em; color: #1e40af;">Ã—</div>
                                <div style="text-align: center;">
                                    <div style="font-weight: 600; margin-bottom: 10px; color: #1e40af;">Matrix B (3Ã—2)</div>
                                    <div class="matrix-grid" id="matrixB" style="display: grid; grid-template-columns: repeat(2, 50px); gap: 5px;">
                                        <div class="matrix-cell">1</div><div class="matrix-cell">2</div>
                                        <div class="matrix-cell">3</div><div class="matrix-cell">1</div>
                                        <div class="matrix-cell">0</div><div class="matrix-cell">2</div>
                                    </div>
                                </div>
                                <div style="font-size: 2em; color: #1e40af;">=</div>
                                <div style="text-align: center;">
                                    <div style="font-weight: 600; margin-bottom: 10px; color: #06b6d4;">Result (2Ã—2)</div>
                                    <div class="matrix-grid" id="matrixResult" style="display: grid; grid-template-columns: repeat(2, 50px); gap: 5px;">
                                        <div class="matrix-cell result-cell">?</div><div class="matrix-cell result-cell">?</div>
                                        <div class="matrix-cell result-cell">?</div><div class="matrix-cell result-cell">?</div>
                                    </div>
                                </div>
                            </div>
                            <div id="calcDisplay" style="text-align: center; margin-top: 15px; font-family: monospace; color: #64748b; min-height: 25px; font-size: 0.9em;"></div>
                        </div>

                        <div class="tools-box">
                            <strong>Key Tools:</strong>
                            <ul>
                                <li><code>numpy</code> - CPU tensor operations: <code>np.array([[1,2],[3,4]])</code></li>
                                <li><code>torch</code> (PyTorch) - GPU tensors: <code>torch.tensor(data).cuda()</code></li>
                                <li><code>tensorflow</code> - Google's framework: <code>tf.constant(data)</code></li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>1.2 <span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> vs <span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span>: Why GPUs Power <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span></h3>
                        
                        <div class="must-know">
                            <strong>Key Insight:</strong> <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline; font-weight: 600;">GPUs</a> are 50-100x faster than <a href="https://en.wikipedia.org/wiki/Central_processing_unit" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline; font-weight: 600;">CPUs</a> for <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> because matrix multiplication (the core operation in neural networks) is highly parallel. CPUs excel at sequential tasks, GPUs excel at parallel tasks.
                        </div>

                        <div class="mental-model">
                            <strong>Simple Analogy:</strong>
                            <ul>
                                <li><strong><span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span>:</strong> Like having 8-16 highly skilled workers who can do complex tasks one after another</li>
                                <li><strong><span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span>:</strong> Like having 5,000-15,000 simple workers who all do the same simple task simultaneously</li>
                            </ul>
                            <p>Training AI = Doing billions of simple math operations â†’ GPU wins!</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>CPU Architecture</strong> (8 cores):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Core 1 â”‚ â”‚ Core 2 â”‚ â”‚ Core 3 â”‚ â”‚ Core 4 â”‚  â† Complex cores
â”‚ 3.5GHz â”‚ â”‚ 3.5GHz â”‚ â”‚ 3.5GHz â”‚ â”‚ 3.5GHz â”‚     High clock speed
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     Sequential processing
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Core 5 â”‚ â”‚ Core 6 â”‚ â”‚ Core 7 â”‚ â”‚ Core 8 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>GPU Architecture</strong> (Simplified - showing 64 of ~10,000 cores):
â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”  â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â† Simple cores
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤     Lower clock speed
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚     Massive parallelism
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚     5,000-15,000 cores!
â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤  â”œâ”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¼â”€â”¤
â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚  â”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚
â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜  â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Interactive: <span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> Cores in Action</strong>
                            <p style="text-align: center; color: #64748b; font-size: 0.9em; margin-bottom: 10px;">
                                <em>Hover to see parallel processing! Each square represents a GPU core.</em>
                            </p>
                            <div class="gpu-cores-container">
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                                <div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div><div class="gpu-core"></div>
                            </div>
                            <p style="text-align: center; color: #1e40af; font-weight: 600; margin-top: 10px;">
                                256 cores shown â€¢ Real GPUs have 5,000-15,000+ cores!
                            </p>
                        </div>

                        <div class="example-box">
                            <strong>Performance Comparison - Matrix Multiplication (4096Ã—4096):</strong>
                            <table>
                                <tr>
                                    <th>Hardware</th>
                                    <th>Time</th>
                                    <th>Speedup</th>
                                </tr>
                                <tr>
                                    <td>Intel i9-13900K (CPU)</td>
                                    <td>~8.5 seconds</td>
                                    <td>1x (baseline)</td>
                                </tr>
                                <tr>
                                    <td>NVIDIA RTX 4090</td>
                                    <td>~0.08 seconds</td>
                                    <td><strong>106x faster!</strong></td>
                                </tr>
                                <tr>
                                    <td>NVIDIA A100 (Data Center)</td>
                                    <td>~0.02 seconds</td>
                                    <td><strong>425x faster!</strong></td>
                                </tr>
                            </table>
                            
                            <p><strong>Training Llama-2-7B from scratch:</strong></p>
                            <ul>
                                <li>CPU only: ~2-3 years ğŸ˜±</li>
                                <li>Single RTX 4090: ~6 months</li>
                                <li>8Ã— A100 GPUs: ~2 weeks âœ…</li>
                            </ul>
                        </div>

                        <h4>Key GPU Components</h4>
                        <table>
                            <tr>
                                <th>Component</th>
                                <th>Purpose</th>
                                <th>Example (NVIDIA A100)</th>
                            </tr>
                            <tr>
                                <td><strong><span class="tooltip">CUDA<span class="tooltiptext">Compute Unified Device Architecture</span></span> Cores</strong></td>
                                <td>General parallel processing units</td>
                                <td>6,912 cores</td>
                            </tr>
                            <tr>
                                <td><strong>Tensor Cores</strong></td>
                                <td>Specialized for AI (matrix operations)</td>
                                <td>432 Gen 3 Tensor Cores</td>
                            </tr>
                            <tr>
                                <td><strong><span class="tooltip">VRAM<span class="tooltiptext">Video Random Access Memory</span></span> (Video RAM)</strong></td>
                                <td>GPU memory for storing models/data</td>
                                <td>40GB or 80GB HBM2e</td>
                            </tr>
                            <tr>
                                <td><strong>Memory Bandwidth</strong></td>
                                <td>Speed of data transfer</td>
                                <td>1,935 GB/s</td>
                            </tr>
                            <tr>
                                <td><strong><span class="tooltip">TFLOPs<span class="tooltiptext">Trillion Floating Point Operations Per Second</span></span> (<span class="tooltip">FP32<span class="tooltiptext">32-bit Floating Point</span></span>)</strong></td>
                                <td>Trillion operations/second</td>
                                <td>19.5 TFLOPs</td>
                            </tr>
                        </table>

                        <div class="mental-model">
                            <strong>Memory Hierarchy (Speed vs Size):</strong>
                            <pre>
Fastest, Smallest
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GPU Registers     â”‚ â† Lightning fast, tiny (few KB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Shared Memory     â”‚ â† Very fast, small (~100 KB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   L2 Cache          â”‚ â† Fast, medium (~40 MB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   VRAM (GPU Memory) â”‚ â† Fast, large (16-80 GB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   RAM (System Mem)  â”‚ â† Slower, huge (32-512 GB)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SSD/HDD Storage   â”‚ â† Slowest, massive (TBs)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Slowest, Largest
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>GPU Monitoring & Management Tools:</strong>
                            <ul>
                                <li><code>nvidia-smi</code> - Monitor GPU usage, memory, temperature
                                    <pre>nvidia-smi</pre>
                                </li>
                                <li><code>nvtop</code> - Interactive GPU monitor (like htop)
                                    <pre>nvtop</pre>
                                </li>
                                <li><code>gpustat</code> - Simple GPU status
                                    <pre>pip install gpustat && gpustat -i</pre>
                                </li>
                                <li><code>CUDA Toolkit</code> - NVIDIA GPU programming framework</li>
                                <li><code>ROCm</code> - AMD GPU alternative to CUDA</li>
                            </ul>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Why can't I just use CPU for AI?</strong><br>
                            A: You can for small models or inference, but training requires massive parallel computation. A single forward pass through GPT-3 involves ~175 billion parameters Ã— batch size operations. GPUs do this 100x faster.
                            <br><br>
                            <strong>Q: How much VRAM do I need?</strong><br>
                            A: Rule of thumb for training:
                            <ul>
                                <li>Model size Ã— 4 (for FP32 precision)</li>
                                <li>Example: 7B model = 7B params Ã— 4 bytes = 28GB minimum</li>
                                <li>Add ~20% for activations, gradients, optimizer states</li>
                                <li>For inference only: Model size Ã— 2 (with quantization, even less!)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- SECTION 2: ARCHITECTURE -->
                <div id="architecture" class="section">
                    <h2>ğŸ—ï¸ 2. Transformer Architecture: The Heart of Modern AI</h2>
                    
                    <div class="must-know">
                        <strong>Core Understanding:</strong> Transformers replaced RNNs/LSTMs and revolutionized AI. The key innovation: <strong>Attention Mechanism</strong> - allowing models to focus on relevant parts of input regardless of distance.
                    </div>

                    <div class="concept-card">
                        <h3>2.1 The Transformer Pipeline: Text â†’ Predictions</h3>
                        
                        <div class="diagram">
<pre>
<strong>Complete Transformer Pipeline:</strong>

Input Text: "The cat sat on"
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. TOKENIZATION (BPE/WordPiece)   â”‚
â”‚  "The cat sat on" â†’ [464, 3797, 3332, 319]
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. EMBEDDING LOOKUP                â”‚
â”‚  [464] â†’ [0.23, -0.45, 0.67, ...]  â”‚  768-dim vectors
â”‚  [3797] â†’ [-0.12, 0.89, -0.34, ...] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. POSITIONAL ENCODING             â”‚
â”‚  Add position info: pos_0, pos_1... â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. TRANSFORMER BLOCKS (Ã—N layers)  â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Self-Attention (Multi-Head) â”‚  â”‚
â”‚  â”‚  "Which words matter?"       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â†“                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Feed-Forward Network        â”‚  â”‚
â”‚  â”‚  Process each position       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                      â”‚
â”‚  (Repeat 12-96 times)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. OUTPUT HEAD                      â”‚
â”‚  Final vector â†’ Probability dist     â”‚
â”‚  Over 50,000 possible next tokens    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
Output: "mat" (highest probability!)
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Interactive: Transformer Flow</strong>
                            <p style="text-align: center; color: #64748b; font-size: 0.9em; margin-bottom: 15px;">
                                <em>Hover over each step to highlight it!</em>
                            </p>
                            <div class="transformer-flow">
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ“</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Input Text</div>
                                    <div style="font-size: 0.85em; color: #64748b;">Raw words</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ”¢</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Tokens</div>
                                    <div style="font-size: 0.85em; color: #64748b;"><span class="tooltip">BPE<span class="tooltiptext">Byte Pair Encoding</span></span>/WordPiece</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ¯</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Embeddings</div>
                                    <div style="font-size: 0.85em; color: #64748b;">768-dim vectors</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">ğŸ§ </div>
                                    <div style="font-weight: 600; margin-top: 5px;">Attention</div>
                                    <div style="font-size: 0.85em; color: #64748b;">Context aware</div>
                                </div>
                                <div class="transformer-arrow">â†’</div>
                                <div class="transformer-step">
                                    <div style="font-size: 2em;">âœ¨</div>
                                    <div style="font-weight: 600; margin-top: 5px;">Output</div>
                                    <div style="font-size: 0.85em; color: #64748b;">Next token</div>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.2 Tokenization: Breaking Text into Pieces</h3>
                        
                        <div class="mental-model">
                            <strong>Why Tokenization?</strong> Computers only understand numbers. Tokenization converts text â†’ numbers while keeping vocabulary size manageable (~50,000 tokens instead of millions of words).
                        </div>

                        <h4><span class="tooltip">BPE<span class="tooltiptext">Byte Pair Encoding</span></span> (Byte Pair Encoding)</h4>
                        <p><strong>Used by:</strong> <span class="tooltip">GPT<span class="tooltiptext">Generative Pre-trained Transformer</span></span>-2, GPT-3, GPT-4, Llama, Mistral, Qwen, Codex</p>
                        
                        <div class="example-box">
                            <strong>How BPE Works:</strong>
                            <pre>
Input: "unhappiness"

<strong>Step 1:</strong> Start with characters
[u, n, h, a, p, p, i, n, e, s, s]

<strong>Step 2:</strong> Find most frequent pair â†’ merge
"p" + "p" appears most â†’ [u, n, h, a, pp, i, n, e, s, s]

<strong>Step 3:</strong> Repeat thousands of times...
After training: "unhappiness" â†’ ["un", "happi", "ness"] (3 tokens!)

<strong>Benefits:</strong>
âœ“ Common words = 1 token ("the", "and")
âœ“ Rare words = multiple tokens ("unhappiness" â†’ 2-3)
âœ“ Unknown words can still be represented
âœ“ Works across languages
</pre>
                        </div>

                        <h4>WordPiece</h4>
                        <p><strong>Used by:</strong> <span class="tooltip">BERT<span class="tooltiptext">Bidirectional Encoder Representations from Transformers</span></span>, DistilBERT, ALBERT, Google's models</p>
                        
                        <div class="example-box">
                            <strong>WordPiece vs BPE:</strong>
                            <pre>
Input: "playing"

BPE:        ["play", "ing"]
WordPiece:  ["play", "##ing"]  â† ## means "continuation"

Input: "unhappiness"
WordPiece:  ["un", "##happ", "##iness"]

<strong>Key Difference:</strong>
- BPE: Uses frequency (most common pairs)
- WordPiece: Uses likelihood (statistical probability)
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Tokenization Tools:</strong>
                            <ul>
                                <li><code>tiktoken</code> - OpenAI's fast tokenizer (Rust-based)
                                    <pre>import tiktoken
enc = tiktoken.get_encoding("cl100k_base")  # GPT-4
tokens = enc.encode("Hello world")</pre>
                                </li>
                                <li><code>sentencepiece</code> - Google's language-agnostic tokenizer
                                    <pre>import sentencepiece as spm
sp = spm.SentencePieceProcessor(model_file='model.model')
tokens = sp.encode('Hello world', out_type=int)</pre>
                                </li>
                                <li><code>transformers.AutoTokenizer</code> - HuggingFace unified API
                                    <pre>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokens = tokenizer.encode("Hello world")</pre>
                                </li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.3 Embeddings: Words as Vectors in Semantic Space</h3>
                        
                        <div class="must-know">
                            <strong>Critical Concept:</strong> Embeddings convert discrete tokens into continuous vector space where <em>similar meanings = nearby vectors</em>. This is how AI "understands" semantics.
                        </div>

                        <div class="mental-model">
                            <strong>Think of embeddings as coordinates in "meaning space":</strong>
                            <ul>
                                <li>"king" and "queen" are nearby (both royalty)</li>
                                <li>"king" and "banana" are far apart (unrelated)</li>
                                <li>Math works: king - man + woman â‰ˆ queen</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>Embedding Visualization (simplified to 2D):</strong>

            Royalty
               â†‘
               â”‚
         queen â€¢    â€¢ king
               â”‚
    princess â€¢ â”‚      â€¢ prince
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Male/Female
               â”‚
        woman â€¢â”‚      â€¢ man
               â”‚
         girl â€¢â”‚      â€¢ boy
               â”‚
            Common

<strong>In Reality: 768-4096 dimensions!</strong>
"king" = [0.23, -0.45, 0.67, 0.12, ..., -0.89]  (768 numbers)
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Embedding Space Visualization</em>
                            </p>
                            <button onclick="animateEmbeddings()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Start Animation</button>
                            <div>
                                <canvas id="embeddingCanvas" style="max-width: 100%; border: 2px solid #334155; border-radius: 8px; background: #1a1a2e;"></canvas>
                            </div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch word vectors cluster by semantic similarity in pseudo-3D space</p>
                        </div>

                        <div class="example-box">
                            <strong>Real Example: GPT-2 Embeddings</strong>
                            <pre>
Token: "cat" (ID: 3797)
      â†“
Embedding Layer (50,257 tokens Ã— 768 dimensions)
      â†“
Vector: [0.23, -0.45, 0.67, 0.12, -0.34, 0.89, ..., -0.19]
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 768 numbers total â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Similar words have similar vectors:
- "cat" â€¢ "dog" = 0.85 (high similarity)
- "cat" â€¢ "computer" = 0.12 (low similarity)
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Working with Embeddings:</strong>
                            <ul>
                                <li><code>torch.nn.Embedding</code> - PyTorch embedding layer
                                    <pre>embedding = nn.Embedding(vocab_size=50000, embedding_dim=768)
vector = embedding(torch.tensor([3797]))  # "cat"</pre>
                                </li>
                                <li><code>sentence-transformers</code> - Semantic embeddings
                                    <pre>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(["cat", "dog", "computer"])</pre>
                                </li>
                                <li><code>OpenAI Embeddings API</code>
                                    <pre>import openai
response = openai.Embedding.create(
    input="Your text here",
    model="text-embedding-ada-002"
)</pre>
                                </li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.4 Attention Mechanism: The Secret Sauce</h3>
                        
                        <div class="must-know">
                            <strong>Why Attention is Revolutionary:</strong> Before transformers, models processed text sequentially (slow). <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline; font-weight: 600;">Attention</a> allows parallel processing while understanding relationships between ALL words simultaneously.
                        </div>

                        <div class="mental-model">
                            <strong>Simple Mental Model:</strong>
                            <p>When reading "The cat sat on the mat", attention answers:</p>
                            <ul>
                                <li>"What is the subject of the action?" â†’ "sat" looks back at "cat" (attention weight: ~0.65)</li>
                                <li>"How do words connect?" â†’ "on" attends to itself + nearby context (e.g., "sat", "the")</li>
                                <li>Some tokens mostly attend to themselves (a common pattern in many heads/layers)</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>Self-Attention Visualization:</strong>

Input: "The cat sat on the mat"

Attention Scores (simplified):
          The   cat   sat   on   the   mat
The     [ 0.8  0.1  0.05 0.02 0.02 0.01 ]
cat     [ 0.1  0.7  0.15 0.02 0.02 0.01 ]
sat     [ 0.05 0.65 0.15 0.1  0.03 0.02 ]  â† "sat" pays attention to "cat"
on      [ 0.03 0.05 0.2  0.5  0.15 0.07 ]
the     [ 0.8  0.05 0.03 0.02 0.05 0.05 ]
mat     [ 0.02 0.1  0.2  0.1  0.05 0.53 ]

Higher value = more attention
ğŸ”´ Red (0.6+) = Strong attention
ğŸŸ¡ Yellow (0.3-0.6) = Medium attention  
ğŸ”µ Blue (0-0.3) = Low attention
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Attention Heatmap</em>
                            </p>
                            <button onclick="showAttentionHeatmap()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show Attention</button>
                            <div id="attentionGrid" class="attention-grid" style="margin: 0 auto; max-width: 500px;"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Heatmap shows attention scores: Red (high attention) â†’ Yellow (medium) â†’ Blue (low)</p>
                        </div>

                        <h4>Types of Attention</h4>
                        <table>
                            <tr>
                                <th>Type</th>
                                <th>Description</th>
                                <th>Used In</th>
                            </tr>
                            <tr>
                                <td><strong>Self-Attention</strong></td>
                                <td>Words attend to other words in same sequence</td>
                                <td>BERT, GPT, all transformers</td>
                            </tr>
                            <tr>
                                <td><strong>Cross-Attention</strong></td>
                                <td>Output attends to input (decoder â†’ encoder)</td>
                                <td>Translation models, T5</td>
                            </tr>
                            <tr>
                                <td><strong>Multi-Head Attention</strong></td>
                                <td>Multiple attention mechanisms in parallel (different perspectives)</td>
                                <td>Standard in all transformers (8-32 heads)</td>
                            </tr>
                            <tr>
                                <td><strong>Causal/Masked Attention</strong></td>
                                <td>Can only attend to previous tokens (for generation)</td>
                                <td>GPT, Llama, autoregressive models</td>
                            </tr>
                        </table>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Token â†’ Embedding Lookup</em>
                            </p>
                            <button onclick="animateTokenLookup()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Start Lookup</button>
                            <div id="tokenLookupContainer" style="padding: 20px; background: linear-gradient(135deg, #1e293b, #334155); border-radius: 8px; min-height: 500px; display: flex; flex-direction: column; justify-content: center; box-shadow: inset 0 2px 8px rgba(0,0,0,0.2);"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">See how tokens are converted to high-dimensional vectors</p>
                        </div>

                        <div class="example-box">
                            <strong>Multi-Head Attention - Different Perspectives:</strong>
                            <pre>
Input: "The quick brown fox jumps"

Head 1: Focuses on GRAMMAR
  "quick" â†’ "brown" (adjectives modify nouns)
  
Head 2: Focuses on ACTIONS
  "fox" â†’ "jumps" (subject-verb relationship)
  
Head 3: Focuses on SYNTAX
  "The" â†’ "fox" (article-noun relationship)

... (8-32 heads total)

All heads combined = rich understanding!
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Attention in Code:</strong>
                            <pre>import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    """
    Q, K, V: Query, Key, Value matrices
    Shape: (batch, seq_len, d_model)
    """
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    attention_weights = F.softmax(scores, dim=-1)
    output = torch.matmul(attention_weights, V)
    return output, attention_weights</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.5 Context Window: How Much Can AI "Remember"?</h3>
                        
                        <div class="mental-model">
                            <strong>Context Window = Short-term memory</strong>
                            <ul>
                                <li>Determines maximum input length model can process at once</li>
                                <li>Measured in tokens (not words!)</li>
                                <li>Computational cost grows quadratically (O(nÂ²) with attention)</li>
                            </ul>
                        </div>

                        <table>
                            <tr>
                                <th>Model</th>
                                <th>Context Window</th>
                                <th>Approximate Pages</th>
                            </tr>
                            <tr>
                                <td colspan="3" style="font-size: 0.9em; color: #64748b; background: rgba(59, 130, 246, 0.05);">
                                    Note: context windows vary significantly by model and version; the values below are illustrative.
                                </td>
                            </tr>
                            <tr>
                                <td>GPT-3</td>
                                <td>2,048 tokens</td>
                                <td>~3-4 pages</td>
                            </tr>
                            <tr>
                                <td>GPT-3.5-Turbo</td>
                                <td>4,096 tokens</td>
                                <td>~6-8 pages</td>
                            </tr>
                            <tr>
                                <td>GPT-4</td>
                                <td>8,192 tokens</td>
                                <td>~12-16 pages</td>
                            </tr>
                            <tr>
                                <td>GPT-4-32k</td>
                                <td>32,768 tokens</td>
                                <td>~50 pages</td>
                            </tr>
                            <tr>
                                <td>Claude 3</td>
                                <td>200,000 tokens</td>
                                <td>~300 pages</td>
                            </tr>
                            <tr>
                                <td>Gemini 1.5 Pro</td>
                                <td>1,000,000 tokens</td>
                                <td>~1,500 pages!</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Why Context Window Matters:</strong>
                            <pre>
Scenario: Analyzing a 50-page document

GPT-3 (2K tokens):
âŒ Can only read ~3 pages at once
âŒ Need to break into chunks, lose context

Claude 3 (200K tokens):
âœ… Can read entire document at once!
âœ… Maintains full context
âœ… Better analysis and understanding
</pre>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Why not just make context windows infinite?</strong><br>
                            A: Computational constraints! Attention is O(nÂ²):
                            <ul>
                                <li>2K tokens = 4M operations</li>
                                <li>32K tokens = 1B operations (250x more!)</li>
                                <li>1M tokens = 1T operations (massive!)</li>
                            </ul>
                            New techniques (sparse attention, sliding window) help, but it's still expensive.
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Neural Network Data Flow</em>
                            </p>
                            <div style="margin-bottom: 15px;">
                                <label class="animation-toggle" style="display: inline-flex; align-items: center; gap: 10px;">
                                    <span style="font-size: 0.9em; color: #64748b;">Enable Animation</span>
                                    <div class="toggle-switch">
                                        <input type="checkbox" id="neuralFlowToggle" onchange="toggleNeuralFlow(this.checked)" style="display: none;">
                                        <span class="toggle-slider"></span>
                                    </div>
                                    <span style="font-size: 0.85em; color: #94a3b8;">(Disable on slower devices)</span>
                                </label>
                            </div>
                            <div>
                                <canvas id="neuralFlowCanvas" style="max-width: 100%; border: 2px solid #334155; border-radius: 8px; background: #1a1a2e;"></canvas>
                            </div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch how input tokens flow through multiple transformer layers (Input â†’ Attention â†’ FFN â†’ Output)</p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>2.6 Transformer Library (HuggingFace)</h3>
                        
                        <div class="tools-box">
                            <strong>ğŸ¤— Transformers - The Standard Library</strong>
                            <pre>pip install transformers</pre>
                            
                            <strong>Basic Usage:</strong>
                            <pre>from transformers import AutoModel, AutoTokenizer

# Load any model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModel.from_pretrained("gpt2")

# Tokenize and generate
inputs = tokenizer("Hello world", return_tensors="pt")
outputs = model(**inputs)</pre>

                            <strong>Popular Models Available:</strong>
                            <ul>
                                <li>GPT-2, GPT-J, GPT-NeoX</li>
                                <li>Llama 2, Llama 3, Mistral, Mixtral</li>
                                <li>BERT, RoBERTa, DistilBERT</li>
                                <li>T5, FLAN-T5, Falcon, Qwen</li>
                                <li>50,000+ models total!</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- SECTION 3: TRAINING -->
                <div id="training" class="section">
                    <h2>ğŸ“ 3. Training & <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> Models: From Data to Weights</h2>
                    
                    <div class="must-know">
                        <strong>Key Distinction:</strong> <strong>Training</strong> = Learning from data (slow, expensive). <strong>Inference</strong> = Using trained model (fast, cheap). Most users only do inference!
                    </div>

                    <div class="concept-card">
                        <h3>3.1 Training vs Inference</h3>
                        
                        <table>
                            <tr>
                                <th>Aspect</th>
                                <th>Training</th>
                                <th>Inference</th>
                            </tr>
                            <tr>
                                <td><strong>Purpose</strong></td>
                                <td>Learn patterns from data</td>
                                <td>Use learned knowledge</td>
                            </tr>
                            <tr>
                                <td><strong>Speed</strong></td>
                                <td>Very slow (days/weeks)</td>
                                <td>Fast (seconds)</td>
                            </tr>
                            <tr>
                                <td><strong>Cost</strong></td>
                                <td>$millions for large models</td>
                                <td>$0.001-0.1 per query</td>
                            </tr>
                            <tr>
                                <td><strong>Hardware</strong></td>
                                <td>100-10,000 GPUs</td>
                                <td>1 GPU or even CPU</td>
                            </tr>
                            <tr>
                                <td><strong>Memory</strong></td>
                                <td>Model + gradients + optimizer (3-4x)</td>
                                <td>Just model weights</td>
                            </tr>
                            <tr>
                                <td><strong>Who Does It?</strong></td>
                                <td>OpenAI, Meta, Google, Anthropic</td>
                                <td>Everyone!</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Training Llama-2-70B (From Scratch):</strong>
                            <pre>
Cost: ~$2-5 million
Hardware: 1,000+ A100 GPUs
Time: ~3 weeks
Data: 2 trillion tokens
Energy: ~1,000 MWh

Inference (Running Llama-2-70B):
Cost: ~$0.001 per query
Hardware: 1 GPU (or quantized on CPU!)
Time: ~1 second per response
</pre>
                        </div>

                        <div class="mental-model">
                            <strong>Analogy:</strong>
                            <ul>
                                <li><strong>Training</strong> = Going to medical school (10 years, expensive)</li>
                                <li><strong>Inference</strong> = Doctor seeing patients (quick, using learned knowledge)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.2 <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> Weights: The "Brain" of the Model</h3>
                        
                        <div class="mental-model">
                            <strong>What are Weights?</strong>
                            <p>Weights are the learned parameters (numbers) that define the model's behavior. Think of them as the "knowledge" encoded in the neural network.</p>
                            <ul>
                                <li><strong>GPT-3:</strong> 175 billion parameters = 175 billion numbers!</li>
                                <li><strong>Storage:</strong> Each weight â‰ˆ 2-4 bytes â†’ 350-700 GB</li>
                                <li><strong>What they encode:</strong> Patterns, facts, grammar, reasoning from training data</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>How Weights Work:</strong>

Input: "The cat sat"
  â†“
[Embedding weights] â† 50,000 Ã— 768 = 38M params
  â†“
[Layer 1 weights] â† 768 Ã— 768 Ã— 4 = 2.4M params
  â†“
[Layer 2 weights]
  â†“
... (repeat 12-96 layers)
  â†“
[Output weights] â† 768 Ã— 50,000 = 38M params
  â†“
Output: "on" (next word prediction)

<strong>Total: 7B-175B+ parameters!</strong>
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Model Size Breakdown:</strong>
                            <table>
                                <tr>
                                    <th>Model</th>
                                    <th>Parameters</th>
                                    <th>Size (FP32)</th>
                                    <th>Size (FP16)</th>
                                </tr>
                                <tr>
                                    <td>GPT-2 Small</td>
                                    <td>117M</td>
                                    <td>468 MB</td>
                                    <td>234 MB</td>
                                </tr>
                                <tr>
                                    <td>GPT-2 Large</td>
                                    <td>1.5B</td>
                                    <td>6 GB</td>
                                    <td>3 GB</td>
                                </tr>
                                <tr>
                                    <td>Llama-2-7B</td>
                                    <td>7B</td>
                                    <td>28 GB</td>
                                    <td>14 GB</td>
                                </tr>
                                <tr>
                                    <td>Llama-2-70B</td>
                                    <td>70B</td>
                                    <td>280 GB</td>
                                    <td>140 GB</td>
                                </tr>
                                <tr>
                                    <td>GPT-3</td>
                                    <td>175B</td>
                                    <td>700 GB</td>
                                    <td>350 GB</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.3 How <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> Models Are Shipped</h3>
                        
                        <div class="mental-model">
                            <strong>A complete model consists of:</strong>
                            <ol>
                                <li><strong>Weights</strong> - The learned parameters</li>
                                <li><strong>Config</strong> - Architecture definition</li>
                                <li><strong>Tokenizer</strong> - Text â†” numbers converter</li>
                                <li><strong>Generation Config</strong> - Default sampling params</li>
                            </ol>
                        </div>

                        <div class="diagram">
<pre>
<strong>Typical Model Directory Structure:</strong>

llama-2-7b/
â”œâ”€â”€ config.json                 â† Architecture definition
â”œâ”€â”€ tokenizer.json              â† BPE/WordPiece vocabulary
â”œâ”€â”€ tokenizer_config.json       â† Tokenizer settings
â”œâ”€â”€ generation_config.json      â† Default sampling params
â”œâ”€â”€ model.safetensors           â† Model weights (primary)
â”‚   OR
â”œâ”€â”€ model-00001-of-00002.safetensors  â† Split for large models
â”œâ”€â”€ model-00002-of-00002.safetensors
â”œâ”€â”€ pytorch_model.bin           â† Alternative (older) format
â””â”€â”€ README.md                   â† Model card & usage
</pre>
                        </div>

                        <h4>config.json - Architecture Definition</h4>
                        <div class="example-box">
<pre><strong>Example: Llama-2-7B config.json</strong>

{
  "architectures": ["LlamaForCausalLM"],
  "hidden_size": 4096,              â† Embedding dimension
  "intermediate_size": 11008,       â† FFN hidden size
  "num_hidden_layers": 32,          â† Number of transformer blocks
  "num_attention_heads": 32,        â† Attention heads per layer
  "num_key_value_heads": 32,        â† For GQA
  "vocab_size": 32000,              â† Tokenizer vocabulary size
  "max_position_embeddings": 4096,  â† Maximum context length
  "rms_norm_eps": 1e-06,           â† Layer norm epsilon
  "torch_dtype": "float16",        â† Weight precision
  "transformers_version": "4.31.0"
}</pre>
                        </div>

                        <h4>generation_config.json - Sampling Parameters</h4>
                        <div class="example-box">
<pre>{
  "bos_token_id": 1,                â† Beginning of sequence
  "eos_token_id": 2,                â† End of sequence
  "pad_token_id": 0,                â† Padding token
  "max_length": 4096,               â† Max generation length
  "temperature": 0.7,               â† Randomness (0=deterministic, 1=creative)
  "top_p": 0.9,                     â† Nucleus sampling
  "top_k": 50,                      â† Top-k sampling
  "do_sample": true                 â† Enable sampling vs greedy
}</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Loading Models:</strong>
                            <pre># HuggingFace Transformers
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")

# Llama.cpp (for GGUF format)
./main -m llama-2-7b.Q4_K_M.gguf -p "Hello"

# vLLM (for fast inference)
from vllm import LLM
llm = LLM(model="meta-llama/Llama-2-7b")</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.4 Hyperparameters: Controlling Model Behavior</h3>
                        
                        <div class="mental-model">
                            <strong>Two Types of Parameters:</strong>
                            <ul>
                                <li><strong>Model Parameters (Weights):</strong> Learned during training</li>
                                <li><strong>Hyperparameters:</strong> Set by humans, control training/generation</li>
                            </ul>
                        </div>

                        <h4>Training Hyperparameters</h4>
                        <table>
                            <tr>
                                <th>Parameter</th>
                                <th>What It Does</th>
                                <th>Typical Values</th>
                            </tr>
                            <tr>
                                <td><strong>Learning Rate</strong></td>
                                <td>How fast model learns</td>
                                <td>1e-5 to 1e-3</td>
                            </tr>
                            <tr>
                                <td><strong>Batch Size</strong></td>
                                <td>Samples per training step</td>
                                <td>8-128</td>
                            </tr>
                            <tr>
                                <td><strong>Epochs</strong></td>
                                <td>Passes through full dataset</td>
                                <td>1-10</td>
                            </tr>
                            <tr>
                                <td><strong>Max Sequence Length</strong></td>
                                <td>Maximum context window</td>
                                <td>512-4096</td>
                            </tr>
                        </table>

                        <h4>Generation Hyperparameters (Inference)</h4>
                        <div class="must-know">
                            <strong>These control how the model generates text!</strong>
                        </div>

                        <table>
                            <tr>
                                <th>Parameter</th>
                                <th>What It Does</th>
                                <th>Range</th>
                                <th>Effect</th>
                            </tr>
                            <tr>
                                <td><strong>Temperature</strong></td>
                                <td>Controls randomness</td>
                                <td>0.0 - 2.0</td>
                                <td>0 = deterministic, 1 = balanced, 2 = very creative</td>
                            </tr>
                            <tr>
                                <td><strong>Top-p (Nucleus)</strong></td>
                                <td>Cumulative probability threshold</td>
                                <td>0.0 - 1.0</td>
                                <td>0.9 = consider top 90% probable tokens</td>
                            </tr>
                            <tr>
                                <td><strong>Top-k</strong></td>
                                <td>Consider only top k tokens</td>
                                <td>1 - 100</td>
                                <td>40 = only consider 40 most likely tokens</td>
                            </tr>
                            <tr>
                                <td><strong>Max Tokens</strong></td>
                                <td>Maximum response length</td>
                                <td>1 - 4096+</td>
                                <td>Limits generation length</td>
                            </tr>
                            <tr>
                                <td><strong>Repetition Penalty</strong></td>
                                <td>Discourage repetition</td>
                                <td>1.0 - 1.5</td>
                                <td>1.0 = no penalty, 1.2 = moderate</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Temperature in Action:</strong>
                            <pre>
Prompt: "The capital of France is"

<strong>Temperature = 0.0 (Deterministic):</strong>
Output: "Paris" (always the same, highest probability)

<strong>Temperature = 0.7 (Balanced):</strong>
Output: "Paris" (90%), "the city of Paris" (8%), "Paris, France" (2%)

<strong>Temperature = 1.5 (Creative):</strong>
Output: "Paris" (40%), "Lyon" (15%), "a beautiful city" (10%), ...

<strong>Temperature = 2.0 (Chaotic):</strong>
Output: Often nonsensical or random!
</pre>
                        </div>

                        <div class="diagram">
<pre>
<strong>How Top-k and Top-p Work Together:</strong>

Token Probabilities:
"Paris":    45%
"the":      20%
"France":   15%
"a":        10%
"located":   5%
"Lyon":      3%
"Marseille": 2%

<strong>Top-k = 3:</strong>
Only consider: "Paris", "the", "France" (top 3)

<strong>Top-p = 0.8:</strong>
Consider until 80% cumulative: "Paris" (45%) + "the" (20%) + "France" (15%) = 80%

<strong>Using Both:</strong>
Final candidates = intersection = "Paris", "the", "France"
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Setting Generation Parameters:</strong>
                            <pre># OpenAI API
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[{"role": "user", "content": "Hello"}],
    temperature=0.7,
    top_p=0.9,
    max_tokens=100
)

# HuggingFace Transformers
outputs = model.generate(
    inputs,
    max_new_tokens=100,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    do_sample=True
)</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>3.5 How Models Are Distributed</h3>
                        
                        <div class="faq-box">
                            <strong>Q: What's the difference between proprietary and open-source models?</strong>
                            
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Proprietary (Closed)</th>
                                    <th>Open Source</th>
                                </tr>
                                <tr>
                                    <td><strong>Examples</strong></td>
                                    <td>GPT-4, Claude, Gemini</td>
                                    <td>Llama, Mistral, Falcon</td>
                                </tr>
                                <tr>
                                    <td><strong>Weights Available?</strong></td>
                                    <td>âŒ No (API only)</td>
                                    <td>âœ… Yes (download)</td>
                                </tr>
                                <tr>
                                    <td><strong>Access Method</strong></td>
                                    <td>API calls (pay per token)</td>
                                    <td>Download & run locally</td>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td>$0.001-0.06 per 1K tokens</td>
                                    <td>Free (just hardware costs)</td>
                                </tr>
                                <tr>
                                    <td><strong>Customization</strong></td>
                                    <td>Limited (fine-tuning API)</td>
                                    <td>Full control</td>
                                </tr>
                                <tr>
                                    <td><strong>Privacy</strong></td>
                                    <td>Data sent to provider</td>
                                    <td>Runs on your hardware</td>
                                </tr>
                            </table>
                        </div>

                        <h4>Where to Get Models</h4>
                        <div class="tools-box">
                            <strong>ğŸ¤— HuggingFace Hub</strong> - Largest repository (500K+ models)
                            <pre>from transformers import AutoModel
model = AutoModel.from_pretrained("meta-llama/Llama-2-7b")</pre>

                            <strong>ğŸ¦™ Ollama</strong> - Easy local deployment
                            <pre>ollama pull llama2
ollama run llama2</pre>

                            <strong>ğŸ”¥ LM Studio</strong> - GUI for local models
                            <p>Download from: lmstudio.ai</p>

                            <strong>âš¡ TheBloke on HuggingFace</strong> - Quantized models
                            <pre>huggingface-cli download TheBloke/Llama-2-7B-GGUF</pre>
                        </div>

                        <h4>Installation Tools</h4>
                        <table>
                            <tr>
                                <th>Tool</th>
                                <th>Purpose</th>
                                <th>Best For</th>
                            </tr>
                            <tr>
                                <td><code>transformers</code></td>
                                <td>Load PyTorch/TF models</td>
                                <td>Development, fine-tuning</td>
                            </tr>
                            <tr>
                                <td><code>llama.cpp</code></td>
                                <td>CPU-optimized inference</td>
                                <td>Running on CPU, edge devices</td>
                            </tr>
                            <tr>
                                <td><code>vLLM</code></td>
                                <td>High-throughput serving</td>
                                <td>Production inference</td>
                            </tr>
                            <tr>
                                <td><code>Ollama</code></td>
                                <td>Local deployment</td>
                                <td>Beginners, quick testing</td>
                            </tr>
                            <tr>
                                <td><code>TGI</code> (Text Gen Inference)</td>
                                <td>HuggingFace serving</td>
                                <td>Production serving</td>
                            </tr>
                        </table>
                    </div>
                </div>

                <!-- SECTION 4: DEPLOYMENT -->
                <div id="deployment" class="section">
                    <h2>ğŸ“¦ 4. Model Deployment & Formats</h2>
                    
                    <div class="must-know">
                        <strong>Key Understanding:</strong> Model weights can be stored in different formats. The format determines compatibility, loading speed, and safety. Two main formats: <strong><span class="tooltip">SafeTensors<span class="tooltiptext">Safe Tensor Format - No arbitrary code execution</span></span></strong> (new standard) and <strong><span class="tooltip">GGUF<span class="tooltiptext">llama.cpp unified model format (supports quantized weights + metadata)</span></span></strong> (for llama.cpp).
                    </div>

                    <div class="concept-card">
                        <h3>4.1 Model Format Comparison</h3>
                        
                        <table>
                            <tr>
                                <th>Format</th>
                                <th>Extension</th>
                                <th>Framework</th>
                                <th>Pros</th>
                                <th>Cons</th>
                            </tr>
                            <tr>
                                <td><strong>SafeTensors</strong></td>
                                <td>.safetensors</td>
                                <td>PyTorch, TF, JAX</td>
                                <td>âœ… Safe (no code execution)<br>âœ… Fast loading<br>âœ… Zero-copy</td>
                                <td>âŒ Larger file size (no compression)</td>
                            </tr>
                            <tr>
                                <td><strong>GGUF</strong></td>
                                <td>.gguf</td>
                                <td>llama.cpp</td>
                                <td>âœ… CPU optimized<br>âœ… Includes metadata<br>âœ… Quantized</td>
                                <td>âŒ llama.cpp specific</td>
                            </tr>
                            <tr>
                                <td><strong>PyTorch</strong></td>
                                <td>.bin, .pt, .pth</td>
                                <td>PyTorch</td>
                                <td>âœ… Native PyTorch</td>
                                <td>âš ï¸ Can execute arbitrary code<br>âŒ Slower loading</td>
                            </tr>
                            <tr>
                                <td><strong>ONNX</strong></td>
                                <td>.onnx</td>
                                <td>Cross-platform</td>
                                <td>âœ… Framework agnostic<br>âœ… Optimized inference</td>
                                <td>âŒ Limited model support</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>4.2 <span class="tooltip">SafeTensors<span class="tooltiptext">Safe Tensor Format - No arbitrary code execution</span></span>: The New Standard</h3>
                        
                        <div class="mental-model">
                            <strong>Why SafeTensors?</strong>
                            <p>Traditional PyTorch (.bin) files use Python's pickle, which can execute arbitrary code â†’ security risk!</p>
                            <p>SafeTensors = Simple binary format, no code execution, faster loading.</p>
                        </div>

                        <div class="example-box">
                            <strong>File Structure:</strong>
                            <pre>
Header: JSON metadata (tensor names, shapes, dtypes, offsets)
Data:   Raw binary tensor data

Example:
llama-2-7b.safetensors (13.5 GB)
â”œâ”€â”€ Header (8 KB): {"model.embed_tokens.weight": {...}, ...}
â””â”€â”€ Data (13.5 GB): [raw floats in FP16]
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Working with SafeTensors:</strong>
                            <pre># Install
pip install safetensors

# Save model
from safetensors.torch import save_file
save_file(model.state_dict(), "model.safetensors")

# Load model
from safetensors.torch import load_file
state_dict = load_file("model.safetensors")
model.load_state_dict(state_dict)

# Convert PyTorch â†’ SafeTensors
from transformers import AutoModel
model = AutoModel.from_pretrained("gpt2")
model.save_pretrained("gpt2-safe", safe_serialization=True)</pre>
                        </div>

                        <h4>Benefits Over PyTorch .bin:</h4>
                        <ul>
                            <li><strong>Security:</strong> No arbitrary code execution (pickle attacks)</li>
                            <li><strong>Speed:</strong> 2-10x faster loading (zero-copy, memory mapping)</li>
                            <li><strong>Cross-framework:</strong> Works with PyTorch, TensorFlow, JAX</li>
                            <li><strong>Validation:</strong> Built-in tensor integrity checks</li>
                        </ul>
                    </div>

                    <div class="concept-card">
                        <h3>4.3 <span class="tooltip">GGUF<span class="tooltiptext">llama.cpp unified model format (supports quantized weights + metadata)</span></span>: Optimized for <span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span> Inference</h3>
                        
                        <div class="mental-model">
                            <strong>GGUF = llama.cpp unified model format</strong>
                            <p>Designed for llama.cpp - enables fast CPU inference with quantization and metadata in one file.</p>
                            <p><strong>Key feature:</strong> Single file contains model + metadata + quantization!</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>GGUF File Structure:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Magic Number & Version          â”‚ â† "GGUF" signature
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Metadata (JSON-like)            â”‚ â† Model config, tokenizer
â”‚  - architecture: "llama"         â”‚
â”‚  - vocab_size: 32000             â”‚
â”‚  - context_length: 4096          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Vocabulary & Tokens             â”‚ â† Tokenizer embedded!
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor Info (offsets, shapes)   â”‚ â† Layer mapping
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Quantized Weights               â”‚ â† Model parameters
â”‚  (Q4_K_M, Q5_K_S, etc.)         â”‚    (compressed!)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

File: llama-2-7b.Q4_K_M.gguf (4.1 GB)
vs
SafeTensors: llama-2-7b.safetensors (13.5 GB)
â†’ 3.3x smaller due to quantization!
</pre>
                        </div>

                        <h4>GGUF Quantization Schemes</h4>
                        <table>
                            <tr>
                                <th>Quantization</th>
                                <th>Bits per Weight</th>
                                <th>File Size (7B)</th>
                                <th>Quality</th>
                                <th>Speed</th>
                            </tr>
                            <tr>
                                <td><strong>Q2_K</strong></td>
                                <td>~2.5 bits</td>
                                <td>~2.7 GB</td>
                                <td>â­â­ Poor</td>
                                <td>âš¡âš¡âš¡ Fastest</td>
                            </tr>
                            <tr>
                                <td><strong>Q4_K_M</strong></td>
                                <td>~4.5 bits</td>
                                <td>~4.1 GB</td>
                                <td>â­â­â­â­ Good</td>
                                <td>âš¡âš¡ Fast</td>
                            </tr>
                            <tr>
                                <td><strong>Q5_K_M</strong></td>
                                <td>~5.5 bits</td>
                                <td>~4.8 GB</td>
                                <td>â­â­â­â­â­ Excellent</td>
                                <td>âš¡ Medium</td>
                            </tr>
                            <tr>
                                <td><strong>Q8_0</strong></td>
                                <td>8 bits</td>
                                <td>~7.0 GB</td>
                                <td>â­â­â­â­â­ Near-perfect</td>
                                <td>âš¡ Slower</td>
                            </tr>
                            <tr>
                                <td><strong>F16</strong></td>
                                <td>16 bits</td>
                                <td>~13.5 GB</td>
                                <td>â­â­â­â­â­ Perfect</td>
                                <td>Slowest</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Recommended Quantizations:</strong>
                            <ul>
                                <li><strong>Q4_K_M:</strong> Best balance (quality vs size) - <em>recommended for most users</em></li>
                                <li><strong>Q5_K_M:</strong> Higher quality, slightly larger</li>
                                <li><strong>Q8_0:</strong> Near-lossless, for quality-critical applications</li>
                                <li><strong>Q2_K:</strong> Extreme compression, significant quality loss</li>
                            </ul>
                        </div>

                        <div class="tools-box">
                            <strong>Converting SafeTensors â†’ GGUF:</strong>
                            <pre># 1. Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# 2. Convert model
python convert.py /path/to/model/

# 3. Quantize
./quantize /path/to/model/ggml-model-f16.gguf \
           /path/to/model/ggml-model-Q4_K_M.gguf Q4_K_M

# 4. Run inference
./main -m ggml-model-Q4_K_M.gguf -p "Hello world" -n 128</pre>

                            <strong>Using with Ollama:</strong>
                            <pre># Ollama handles GGUF automatically
ollama pull llama2
ollama run llama2 "Hello world"</pre>

                            <strong>Using with LM Studio:</strong>
                            <p>1. Download LM Studio: lmstudio.ai</p>
                            <p>2. Search & download GGUF models directly in GUI</p>
                            <p>3. Run with one click!</p>
                        </div>

                        <div class="faq-box">
                            <strong>Q: SafeTensors vs GGUF - which should I use?</strong><br>
                            A: Depends on your use case:
                            <ul>
                                <li><strong>SafeTensors:</strong> If you're doing training, fine-tuning, or using GPUs with HuggingFace/PyTorch</li>
                                <li><strong>GGUF:</strong> If you're running inference on CPU, need smaller files, or using llama.cpp/Ollama</li>
                            </ul>
                            <p><em>Many models are distributed in both formats!</em></p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>4.4 Model Conversion Tools</h3>
                        
                        <div class="tools-box">
                            <strong>ğŸ”„ Key Conversion Tools:</strong>
                            
                            <h4>1. llama.cpp convert.py</h4>
                            <pre># PyTorch/SafeTensors â†’ GGUF
python convert.py /path/to/model/

# Quantize GGUF
./quantize model-f16.gguf model-q4.gguf Q4_K_M</pre>

                            <h4>2. HuggingFace Optimum</h4>
                            <pre>pip install optimum

# Convert to ONNX
optimum-cli export onnx --model gpt2 onnx/

# Quantize ONNX
optimum-cli onnxruntime quantize \
    --onnx_model onnx/ \
    --output quantized/</pre>

                            <h4>3. ONNX Runtime</h4>
                            <pre>pip install onnxruntime-gpu

# Load and run ONNX model
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
outputs = session.run(None, inputs)</pre>

                            <h4>4. AutoGPTQ</h4>
                            <pre>pip install auto-gptq

# Quantize to GPTQ (4-bit)
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_pretrained("model/", quantize_config)
model.quantize(dataset)
model.save_quantized("quantized/")</pre>
                        </div>

                        <h4>Conversion Matrix</h4>
                        <table>
                            <tr>
                                <th>From</th>
                                <th>To</th>
                                <th>Tool</th>
                                <th>Command</th>
                            </tr>
                            <tr>
                                <td>PyTorch .bin</td>
                                <td>SafeTensors</td>
                                <td>transformers</td>
                                <td><code>save_pretrained(..., safe_serialization=True)</code></td>
                            </tr>
                            <tr>
                                <td>SafeTensors</td>
                                <td>GGUF</td>
                                <td>llama.cpp</td>
                                <td><code>python convert.py</code></td>
                            </tr>
                            <tr>
                                <td>PyTorch/ONNX</td>
                                <td>TensorRT</td>
                                <td>TensorRT</td>
                                <td><code>trtexec</code></td>
                            </tr>
                            <tr>
                                <td>Any</td>
                                <td>ONNX</td>
                                <td>optimum</td>
                                <td><code>optimum-cli export onnx</code></td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>4.5 Inference Engines & Serving</h3>
                        
                        <table>
                            <tr>
                                <th>Engine</th>
                                <th>Best For</th>
                                <th>Hardware</th>
                                <th>Speed</th>
                            </tr>
                            <tr>
                                <td><strong>vLLM</strong></td>
                                <td>High-throughput serving</td>
                                <td>GPU (CUDA)</td>
                                <td>âš¡âš¡âš¡ Fastest (PagedAttention)</td>
                            </tr>
                            <tr>
                                <td><strong>llama.cpp</strong></td>
                                <td>CPU inference, edge devices</td>
                                <td>CPU, Metal, Vulkan</td>
                                <td>âš¡âš¡ Fast on CPU</td>
                            </tr>
                            <tr>
                                <td><strong>TGI</strong> (Text Gen Inference)</td>
                                <td>Production serving (HuggingFace)</td>
                                <td>GPU</td>
                                <td>âš¡âš¡âš¡ Very fast</td>
                            </tr>
                            <tr>
                                <td><strong>TensorRT-LLM</strong></td>
                                <td>NVIDIA-optimized inference</td>
                                <td>NVIDIA GPU only</td>
                                <td>âš¡âš¡âš¡ Fastest on NVIDIA</td>
                            </tr>
                            <tr>
                                <td><strong>Ollama</strong></td>
                                <td>Easy local deployment</td>
                                <td>CPU, GPU</td>
                                <td>âš¡ Good</td>
                            </tr>
                            <tr>
                                <td><strong>ONNX Runtime</strong></td>
                                <td>Cross-platform inference</td>
                                <td>CPU, GPU, Mobile</td>
                                <td>âš¡âš¡ Fast</td>
                            </tr>
                            <tr>
                                <td><strong>OpenVINO</strong></td>
                                <td>Intel-optimized inference</td>
                                <td>Intel CPU, GPU, VPU, NPU</td>
                                <td>âš¡âš¡âš¡ Very fast on Intel</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Quick Start Examples:</strong>
                            
                            <pre><strong># vLLM (fastest for GPU)</strong>
pip install vllm
vllm serve meta-llama/Llama-2-7b

<strong># llama.cpp (best for CPU)</strong>
./main -m model.gguf -p "Hello" -n 100 -c 4096

<strong># TGI (HuggingFace production)</strong>
docker run -p 8080:80 \
  ghcr.io/huggingface/text-generation-inference \
  --model-id meta-llama/Llama-2-7b

<strong># Ollama (easiest)</strong>
ollama run llama2

<strong># OpenVINO (Intel-optimized)</strong>
pip install openvino
from openvino.runtime import Core
core = Core()
model = core.read_model("model.xml")
compiled = core.compile_model(model, "CPU")</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>4.6 OpenVINO: Intel-Optimized Inference</h3>
                        
                        <div class="must-know">
                            <strong>What is OpenVINO?</strong>
                            <p>OpenVINO (Open Visual Inference and Neural Network Optimization) is Intel's toolkit for optimizing and deploying AI models on Intel hardware. It delivers exceptional performance on Intel CPUs, integrated GPUs, VPUs (Vision Processing Units), and NPUs (Neural Processing Units).</p>
                        </div>

                        <div class="mental-model">
                            <strong>Why OpenVINO?</strong>
                            <ul>
                                <li><strong>Hardware Acceleration:</strong> Optimized for Intel architecture (AVX-512, VNNI instructions)</li>
                                <li><strong>Format Flexibility:</strong> Accepts ONNX, TensorFlow, PyTorch models</li>
                                <li><strong>Edge Deployment:</strong> Run models on laptops, embedded devices, edge servers</li>
                                <li><strong>Performance:</strong> 2-10x faster than generic inference on Intel hardware</li>
                            </ul>
                        </div>

                        <div class="diagram">
<pre>
<strong>OpenVINO Workflow:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Your Model   â”‚      â”‚  OpenVINO    â”‚      â”‚   Optimized â”‚
â”‚ (PyTorch/ONNX) â”‚â”€â”€â”€â”€â”€â”€â”‚  Model       â”‚â”€â”€â”€â”€â”€â”€â”‚   Inference â”‚
â”‚                â”‚      â”‚  Optimizer   â”‚      â”‚   Runtime   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                      â”‚                      â”‚
        â”‚                      â†“                      â†“
    .pt/.onnx           .xml + .bin              Runs on:
    Original             IR Format           â€¢ Intel CPU (fast!)
    Weights         (Intermediate            â€¢ Intel GPU
                     Representation)         â€¢ Intel VPU/NPU
                                            â€¢ ARM CPU

<strong>Key Steps:</strong>
1. Convert model â†’ OpenVINO IR format (.xml + .bin)
2. Apply optimizations (quantization, layer fusion)
3. Deploy with OpenVINO Runtime
</pre>
                        </div>

                        <div class="concept-card">
                            <h4>OpenVINO + ONNX Integration</h4>
                            
                            <div class="must-know">
                                <strong>Perfect Partnership:</strong> ONNX is a framework-agnostic format, and OpenVINO can directly import ONNX models, making it easy to optimize models from any framework (PyTorch, TensorFlow, JAX) for Intel hardware.
                            </div>

                            <div class="diagram">
<pre>
<strong>ONNX â†’ OpenVINO Pipeline:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PyTorch    â”‚â”€â”€â”€â–¶â”‚     ONNX     â”‚â”€â”€â”€â–¶â”‚   OpenVINO    â”‚
â”‚     Model    â”‚    â”‚    Format    â”‚    â”‚   Optimized   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      .pt               .onnx            .xml + .bin
                                              â”‚
                    Framework Agnostic        â†“
                    Intermediate         Runs 2-10x
                    Representation       faster on Intel

<strong>Why This Flow?</strong>
1. <strong>ONNX</strong>: Universal format - export from any framework
2. <strong>OpenVINO</strong>: Intel-specific optimizations (VNNI, AVX-512)
3. <strong>Result</strong>: Best of both worlds - portability + performance
</pre>
                            </div>

                            <div class="tools-box">
                                <strong>Complete Conversion Workflow:</strong>
                                <pre><strong># Step 1: Export PyTorch â†’ ONNX</strong>
import torch
import torch.onnx

model = torch.load("model.pt")
dummy_input = torch.randn(1, 3, 224, 224)

torch.onnx.export(
    model,
    dummy_input,
    "model.onnx",
    input_names=["input"],
    output_names=["output"],
    dynamic_axes={"input": {0: "batch_size"}}
)

<strong># Step 2: ONNX â†’ OpenVINO IR</strong>
mo --input_model model.onnx \
   --output_dir openvino_model/ \
   --input_shape [1,3,224,224] \
   --data_type FP16  # Half precision

<strong># Step 3: Run Inference with OpenVINO</strong>
from openvino.runtime import Core
import numpy as np

core = Core()
model = core.read_model("openvino_model/model.xml")
compiled = core.compile_model(model, "CPU")

input_data = np.random.randn(1, 3, 224, 224).astype(np.float32)
result = compiled([input_data])
print(result[0])

<strong># Alternative: Direct ONNX Runtime</strong>
import onnxruntime as ort
session = ort.InferenceSession("model.onnx")
output = session.run(None, {"input": input_data})</pre>
                            </div>
                        </div>

                        <h4>OpenVINO Optimization Features</h4>
                        <table>
                            <tr>
                                <th>Feature</th>
                                <th>Description</th>
                                <th>Benefit</th>
                            </tr>
                            <tr>
                                <td><strong>INT8 Quantization</strong></td>
                                <td>8-bit integer weights and activations</td>
                                <td>4x smaller, 2-4x faster</td>
                            </tr>
                            <tr>
                                <td><strong>Layer Fusion</strong></td>
                                <td>Combines operations (Conv+ReLU+BN)</td>
                                <td>Reduced memory transfers</td>
                            </tr>
                            <tr>
                                <td><strong>Kernel Auto-tuning</strong></td>
                                <td>Optimizes for specific Intel CPU/GPU</td>
                                <td>Best performance per device</td>
                            </tr>
                            <tr>
                                <td><strong>Dynamic Batching</strong></td>
                                <td>Efficient variable batch sizes</td>
                                <td>Better throughput</td>
                            </tr>
                            <tr>
                                <td><strong>Multi-stream</strong></td>
                                <td>Parallel inference streams</td>
                                <td>Full CPU utilization</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Real-World Performance:</strong>
                            <pre><strong>ResNet-50 Inference (224x224 image):</strong>

PyTorch (CPU):           45ms per image
ONNX Runtime (CPU):      28ms per image  (1.6x faster)
OpenVINO (Intel CPU):    12ms per image  (3.75x faster) âœ¨
OpenVINO + INT8:         6ms per image   (7.5x faster) ğŸš€

<strong>7B LLM Token Generation:</strong>
PyTorch (Intel i9):      8 tokens/sec
ONNX Runtime:            12 tokens/sec
OpenVINO INT8:           24 tokens/sec  (3x faster) âš¡

<em>Results vary by model, hardware, and configuration</em></pre>
                        </div>

                        <div class="tools-box">
                            <strong>Installation & Setup:</strong>
                            <pre><strong># Install OpenVINO</strong>
pip install openvino openvino-dev

<strong># Verify installation</strong>
from openvino.runtime import Core
core = Core()
print(core.available_devices)  # ['CPU', 'GPU.0', ...]

<strong># Convert model with Model Optimizer</strong>
mo --input_model model.onnx --output_dir output/

<strong># Or use Python API</strong>
from openvino.tools import mo
model = mo.convert_model("model.onnx")

<strong># Quantize to INT8</strong>
from openvino.tools import pot  # Post-training Optimization Toolkit
# Requires calibration dataset
pot --config config.json</pre>
                        </div>

                        <div class="faq-box">
                            <strong>Q: When should I use OpenVINO vs ONNX Runtime?</strong><br>
                            A:
                            <ul>
                                <li><strong>Use OpenVINO if:</strong> You're deploying on Intel CPUs/GPUs/VPUs and need maximum performance</li>
                                <li><strong>Use ONNX Runtime if:</strong> You need cross-platform support (AMD, NVIDIA, ARM) or simpler deployment</li>
                                <li><strong>Use Both:</strong> ONNX for model portability, then optimize with OpenVINO for Intel hardware!</li>
                            </ul>

                            <strong>Q: Can I use OpenVINO with LLMs like Llama or GPT?</strong><br>
                            A: Yes! OpenVINO supports transformer models. Use the <code>optimum-intel</code> library:
                            <pre>pip install optimum[openvino]
from optimum.intel import OVModelForCausalLM
model = OVModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")</pre>
                        </div>

                        <h4>Supported Model Formats</h4>
                        <table>
                            <tr>
                                <th>Format</th>
                                <th>Direct Support</th>
                                <th>Conversion Tool</th>
                            </tr>
                            <tr>
                                <td>ONNX (.onnx)</td>
                                <td>âœ… Yes</td>
                                <td>Model Optimizer (mo)</td>
                            </tr>
                            <tr>
                                <td>PyTorch (.pt)</td>
                                <td>âœ… Yes (via torch.jit)</td>
                                <td>Model Optimizer</td>
                            </tr>
                            <tr>
                                <td>TensorFlow (.pb)</td>
                                <td>âœ… Yes</td>
                                <td>Model Optimizer</td>
                            </tr>
                            <tr>
                                <td>PaddlePaddle</td>
                                <td>âœ… Yes</td>
                                <td>Model Optimizer</td>
                            </tr>
                            <tr>
                                <td>SafeTensors</td>
                                <td>âŒ No (convert to ONNX first)</td>
                                <td>HF Optimum â†’ ONNX â†’ OpenVINO</td>
                            </tr>
                            <tr>
                                <td>GGUF</td>
                                <td>âŒ No (llama.cpp format)</td>
                                <td>Use llama.cpp instead</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Complete Example: Deploy Llama-2 with OpenVINO</strong>
                            <pre><strong># 1. Install dependencies</strong>
pip install optimum[openvino] transformers

<strong># 2. Export and optimize model</strong>
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer

# This automatically converts to OpenVINO format
model = OVModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-chat-hf",
    export=True,  # Export to OpenVINO
    compile=True  # Compile for target device
)
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")

<strong># 3. Run inference</strong>
prompt = "What is the capital of France?"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
response = tokenizer.decode(outputs[0])
print(response)

<strong># 4. (Optional) Quantize to INT8 for 4x speedup</strong>
from optimum.intel import OVQuantizer

quantizer = OVQuantizer.from_pretrained(model)
quantizer.quantize(save_directory="./llama2-int8")
quantized_model = OVModelForCausalLM.from_pretrained("./llama2-int8")</pre>
                        </div>
                    </div>
                </div>

                <!-- SECTION 5: OPTIMIZATION -->
                <div id="optimization" class="section">
                    <h2>âš¡ 5. Optimization: Making AI Efficient</h2>
                    
                    <div class="must-know">
                        <strong>Core Principle:</strong> Training from scratch costs millions. Optimization techniques let you adapt existing models cheaply and run them faster. Key methods: <strong>Fine-Tuning, LoRA, Quantization</strong>.
                    </div>

                    <div class="concept-card">
                        <h3>5.1 Fine-Tuning Spectrum</h3>
                        
                        <div class="diagram">
<pre>
<strong>Fine-Tuning Methods (Cost vs Customization)</strong>

High Customization, High Cost
         â†‘
         â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Full Fine-Tuning (FPFT)   â”‚  Update ALL parameters
    â”‚  Cost: $$$$, Time: Days    â”‚  Needs: 100K+ examples
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  LoRA (Low-Rank Adapt)     â”‚  Update small adapter layers
    â”‚  Cost: $$, Time: Hours     â”‚  Needs: 1K-10K examples
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  QLoRA (Quantized LoRA)    â”‚  LoRA on quantized model
    â”‚  Cost: $, Time: Hours      â”‚  Needs: 1K-10K examples
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Prompt Engineering        â”‚  No training!
    â”‚  Cost: Free, Time: Minutes â”‚  Needs: Examples in prompt
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
Low Customization, Low Cost
</pre>
                        </div>

                        <table>
                            <tr>
                                <th>Method</th>
                                <th>Parameters Updated</th>
                                <th>Memory (7B)</th>
                                <th>Cost</th>
                                <th>Time</th>
                            </tr>
                            <tr>
                                <td><strong>Full Fine-Tuning</strong></td>
                                <td>100% (7B)</td>
                                <td>~120 GB</td>
                                <td>$$$$</td>
                                <td>Days</td>
                            </tr>
                            <tr>
                                <td><strong>LoRA</strong></td>
                                <td>~0.1% (7M)</td>
                                <td>often 16â€“40+ GB</td>
                                <td>$$</td>
                                <td>Hours</td>
                            </tr>
                            <tr>
                                <td><strong>QLoRA</strong></td>
                                <td>~0.1% (7M)</td>
                                <td>often 8â€“24 GB</td>
                                <td>$</td>
                                <td>Hours</td>
                            </tr>
                            <tr>
                                <td><strong>Prompt Engineering</strong></td>
                                <td>0%</td>
                                <td>Inference only</td>
                                <td>Free</td>
                                <td>Minutes</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>5.2 <span class="tooltip">LoRA<span class="tooltiptext">Low-Rank Adaptation</span></span> (Low-Rank Adaptation)</h3>
                        
                        <div class="must-know">
                            <strong>Key Insight:</strong> Instead of updating all 7 billion parameters, LoRA adds tiny "adapter" matrices that capture task-specific knowledge. Original weights stay frozen!
                        </div>

                        <div class="mental-model">
                            <strong>Analogy:</strong> Instead of rewriting an entire encyclopedia (full fine-tuning), you add sticky notes with corrections/additions (LoRA adapters).
                        </div>

                        <div class="diagram">
<pre>
<strong>How LoRA Works:</strong>

Original Weight Matrix (W):        LoRA Adapters:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”
â”‚                â”‚                â”‚    â”‚    â”‚    â”‚
â”‚   W (4096Ã—4096)â”‚     +          â”‚ A  â”‚ Ã— â”‚ B  â”‚
â”‚                â”‚                â”‚    â”‚    â”‚    â”‚
â”‚   Frozen! â„ï¸   â”‚                â””â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               4096Ã—8    8Ã—4096
                                 
                                 Total: ~65K params
                                 vs 16M in original!

<strong>Final Output:</strong> W + (A Ã— B) Ã— scale
                    â†‘      â†‘       â†‘
              Original  Adapter  Scaling factor
</pre>
                        </div>

                        <div class="example-box">
                            <strong>LoRA Benefits:</strong>
                            <ul>
                                <li><strong>Memory Efficient:</strong> Often ~3â€“10Ã— less VRAM than full fine-tuning (highly setup-dependent)</li>
                                <li><strong>Fast Training:</strong> Hours instead of days</li>
                                <li><strong>Small Adapters:</strong> LoRA weights ~10-100 MB vs 13.5 GB full model</li>
                                <li><strong>Multi-Task:</strong> Swap adapters for different tasks!
                                    <pre>
Base model (Llama-2-7B) + Adapter A (Python code)
Base model (Llama-2-7B) + Adapter B (Medical QA)
Base model (Llama-2-7B) + Adapter C (Translation)
                                    </pre>
                                </li>
                            </ul>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: LoRA Adapter Injection</em>
                            </p>
                            <button onclick="animateLoRA()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show LoRA</button>
                            <div id="loraContainer" class="lora-container" style="padding: 20px; background: linear-gradient(135deg, #1e293b, #334155); border-radius: 8px; min-height: 200px;"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">See how tiny adapters customize massive models efficiently</p>
                        </div>

                        <h4>LoRA Hyperparameters</h4>
                        <table>
                            <tr>
                                <th>Parameter</th>
                                <th>Description</th>
                                <th>Typical Values</th>
                            </tr>
                            <tr>
                                <td><strong>Rank (r)</strong></td>
                                <td>Dimension of adapter matrices</td>
                                <td>4, 8, 16, 32 (higher = more expressive)</td>
                            </tr>
                            <tr>
                                <td><strong>Alpha (Î±)</strong></td>
                                <td>Scaling factor</td>
                                <td>16, 32 (usually 2Ã— rank)</td>
                            </tr>
                            <tr>
                                <td><strong>Target Modules</strong></td>
                                <td>Which layers to apply LoRA</td>
                                <td>q_proj, v_proj (attention)</td>
                            </tr>
                            <tr>
                                <td><strong>Dropout</strong></td>
                                <td>Regularization</td>
                                <td>0.05 - 0.1</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Fine-Tuning with LoRA (PEFT Library):</strong>
                            <pre>pip install peft transformers bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b")

# Configure LoRA
lora_config = LoraConfig(
    r=8,                           # Rank
    lora_alpha=16,                 # Scaling
    target_modules=["q_proj", "v_proj"],  # Which layers
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# Apply LoRA adapters
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# Output: trainable params: 7.4M / 7B = 0.1%

# Train (standard PyTorch/HF Trainer)
from transformers import Trainer
trainer = Trainer(model=model, args=training_args, ...)
trainer.train()

# Save LoRA adapters only (tiny!)
model.save_pretrained("lora-adapters/")  # ~10-50 MB!

# Load later
from peft import PeftModel
base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
model = PeftModel.from_pretrained(base_model, "lora-adapters/")</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>5.3 <span class="tooltip">QLoRA<span class="tooltiptext">Quantized Low-Rank Adaptation</span></span> (Quantized LoRA)</h3>
                        
                        <div class="must-know">
                            <strong>Game Changer:</strong> QLoRA = LoRA + 4-bit quantization. It can make very large models feasible on a single 16â€“24GB GPU in some setups (sequence length/batch size matter).
                        </div>

                        <div class="mental-model">
                            <p><strong>QLoRA Innovation:</strong></p>
                            <ol>
                                <li>Load base model in 4-bit (quantized) - saves 4x memory</li>
                                <li>Keep LoRA adapters in full precision (FP16)</li>
                                <li>During forward pass: dequantize â†’ compute â†’ quantize</li>
                            </ol>
                        </div>

                        <div class="example-box">
                            <strong>Memory Comparison (Llama-2-7B):</strong>
                            <pre>
Full Fine-Tuning (FP32):    often 80â€“200+ GB VRAM âŒ
Full Fine-Tuning (FP16/BF16): often 40â€“120+ GB VRAM âŒ
LoRA (FP16/BF16):           often 16â€“40+ GB VRAM âš ï¸
QLoRA (4-bit + LoRA):       often 8â€“24 GB VRAM âœ…

â†’ Actual VRAM depends heavily on sequence length, batch size, optimizer, and checkpointing.
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>QLoRA Training:</strong>
                            <pre>pip install peft transformers bitsandbytes accelerate

from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, prepare_model_for_kbit_training

# 4-bit quantization config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",     # Normalized Float 4
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True  # Double quantization
)

# Load model in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    quantization_config=bnb_config,
    device_map="auto"
)

# Prepare for training
model = prepare_model_for_kbit_training(model)

# Add LoRA adapters
lora_config = LoraConfig(r=16, lora_alpha=32, ...)
model = get_peft_model(model, lora_config)

# Train as usual!
trainer.train()</pre>
                        </div>

                        <h4>QLoRA vs LoRA</h4>
                        <table>
                            <tr>
                                <th>Aspect</th>
                                <th>LoRA</th>
                                <th>QLoRA</th>
                            </tr>
                            <tr>
                                <td><strong>Base Model Precision</strong></td>
                                <td>FP16</td>
                                <td>4-bit (NF4)</td>
                            </tr>
                            <tr>
                                <td><strong>Memory (7B)</strong></td>
                                <td>often 16â€“40+ GB</td>
                                <td>often 8â€“24 GB</td>
                            </tr>
                            <tr>
                                <td><strong>Speed</strong></td>
                                <td>Faster</td>
                                <td>Slightly slower (~10-20%)</td>
                            </tr>
                            <tr>
                                <td><strong>Quality</strong></td>
                                <td>High</td>
                                <td>Often close (validate on your tasks)</td>
                            </tr>
                            <tr>
                                <td><strong>Hardware Requirement</strong></td>
                                <td>24+ GB VRAM</td>
                                <td>8+ GB VRAM</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>5.4 Quantization: Smaller, Faster Models</h3>
                        
                        <div class="mental-model">
                            <strong>Quantization = Reducing precision of weights</strong>
                            <p>Instead of storing each weight as 32-bit float, use 8-bit, 4-bit, or even 2-bit integers.</p>
                            <p><strong>Trade-off:</strong> Smaller size & faster inference vs slight quality loss</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>Precision Comparison:</strong>

FP32 (Full Precision):
  Weight = 0.123456789  (32 bits = 4 bytes)
  Range: Â±3.4 Ã— 10Â³â¸

FP16 (Half Precision):
  Weight = 0.1235  (16 bits = 2 bytes)
  Range: Â±65,504

INT8 (8-bit Integer):
  Weight = 31  (8 bits = 1 byte)
  Range: -128 to 127

INT4 (4-bit Integer):
  Weight = 7  (4 bits = 0.5 bytes)
  Range: -8 to 7

<strong>Model Size Impact (7B parameters):</strong>
FP32: 7B Ã— 4 bytes = 28 GB
FP16: 7B Ã— 2 bytes = 14 GB (2x smaller)
INT8: 7B Ã— 1 byte  = 7 GB  (4x smaller)
INT4: 7B Ã— 0.5 byte = 3.5 GB (8x smaller!)
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: Quantization Size Comparison</em>
                            </p>
                            <button onclick="showQuantization()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show Comparison</button>
                            <div id="quantContainer" class="quant-comparison" style="display: flex; align-items: center; justify-content: center; gap: 20px; padding: 30px; flex-wrap: wrap; min-height: 180px;"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">See the dramatic size reduction from quantization</p>
                        </div>

                        <h4>Quantization Methods</h4>
                        <table>
                            <tr>
                                <th>Method</th>
                                <th>Bits</th>
                                <th>Size Reduction</th>
                                <th>Quality Loss</th>
                                <th>Best For</th>
                            </tr>
                            <tr>
                                <td><strong>FP16/BF16</strong></td>
                                <td>16-bit</td>
                                <td>2x</td>
                                <td>Negligible</td>
                                <td>Standard training/inference</td>
                            </tr>
                            <tr>
                                <td><strong>INT8</strong></td>
                                <td>8-bit</td>
                                <td>4x</td>
                                <td>Minimal (<1%)</td>
                                <td>Production inference</td>
                            </tr>
                            <tr>
                                <td><strong>GPTQ</strong></td>
                                <td>4-bit</td>
                                <td>8x</td>
                                <td>Small (~2-3%)</td>
                                <td>GPU inference</td>
                            </tr>
                            <tr>
                                <td><strong>AWQ</strong></td>
                                <td>4-bit</td>
                                <td>8x</td>
                                <td>Minimal</td>
                                <td>GPU inference (better quality)</td>
                            </tr>
                            <tr>
                                <td><strong>GGML/GGUF</strong></td>
                                <td>2-8 bit</td>
                                <td>4-16x</td>
                                <td>Varies</td>
                                <td>CPU inference (llama.cpp)</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Quantization Tools:</strong>
                            
                            <h4>1. AutoGPTQ (4-bit for GPU)</h4>
                            <pre>pip install auto-gptq

from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

# Quantization config
quantize_config = BaseQuantizeConfig(
    bits=4,
    group_size=128,
    desc_act=False
)

# Load and quantize
model = AutoGPTQForCausalLM.from_pretrained("model/")
model.quantize(calibration_data, quantize_config=quantize_config)
model.save_quantized("model-gptq-4bit/")</pre>

                            <h4>2. AWQ (Activation-aware Weight Quantization)</h4>
                            <pre>pip install autoawq

from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_pretrained("model/")
model.quantize(calib_data, quant_config={"bits": 4, "group_size": 128})
model.save_quantized("model-awq-4bit/")</pre>

                            <h4>3. bitsandbytes (8-bit/4-bit)</h4>
                            <pre>from transformers import AutoModelForCausalLM, BitsAndBytesConfig

# Load in 8-bit
model = AutoModelForCausalLM.from_pretrained(
    "model/",
    load_in_8bit=True,
    device_map="auto"
)

# Load in 4-bit
model = AutoModelForCausalLM.from_pretrained(
    "model/",
    load_in_4bit=True,
    device_map="auto"
)</pre>

                            <h4>4. llama.cpp (GGUF quantization)</h4>
                            <pre># Quantize to various levels
./quantize model-f16.gguf model-q4.gguf Q4_K_M
./quantize model-f16.gguf model-q5.gguf Q5_K_M
./quantize model-f16.gguf model-q8.gguf Q8_0</pre>

                            <h4>5. ONNX Runtime (INT8)</h4>
                            <pre>from optimum.onnxruntime import ORTQuantizer, ORTModelForCausalLM

# Quantize ONNX model
quantizer = ORTQuantizer.from_pretrained("model/")
quantizer.quantize(save_dir="model-int8/")</pre>
                        </div>

                        <div class="example-box">
                            <strong>Quantization Workflow:</strong>
                            <pre>
1. Start with full precision model (FP16)
   Size: 13.5 GB

2. Choose quantization method based on target:
   - GPU inference â†’ GPTQ or AWQ (4-bit)
   - CPU inference â†’ GGUF (llama.cpp)
   - Production â†’ INT8 (ONNX or bitsandbytes)

3. Quantize using appropriate tool
   ./quantize model-f16.gguf model-q4.gguf Q4_K_M

4. Test quality on validation set
   Compare perplexity, accuracy metrics

5. Deploy quantized model
   Size: 3.5 GB (4x smaller!)
   Speed: 2-3x faster
    Quality: often close to original on many tasks (always validate)
</pre>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Which quantization should I use?</strong><br>
                            <strong>For GPU Inference:</strong>
                            <ul>
                                <li>Best quality: AWQ (4-bit) or INT8</li>
                                <li>Best balance: GPTQ (4-bit)</li>
                                <li>Extreme compression: GPTQ (3-bit)</li>
                            </ul>
                            <strong>For CPU Inference:</strong>
                            <ul>
                                <li>Recommended: GGUF Q4_K_M or Q5_K_M</li>
                                <li>Best quality: GGUF Q8_0</li>
                                <li>Smallest: GGUF Q2_K</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>5.5 Full Parameter Fine-Tuning (FPFT)</h3>
                        
                        <div class="mental-model">
                            <strong>When to use FPFT:</strong>
                            <ul>
                                <li>You need maximum customization</li>
                                <li>You have large, high-quality dataset (100K+ examples)</li>
                                <li>You have significant compute budget</li>
                                <li>Domain is very different from pre-training (e.g., medical, legal)</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>FPFT Requirements (Llama-2-7B):</strong>
                            <pre>
Hardware: typically 1â€“8 high-memory GPUs (varies by sequence length & batch size)
Memory: often tens to 100+ GB VRAM total (optimizer + activations dominate)
Time: hours to days (dataset size + hardware dependent)
Cost: ranges from tens of dollars to thousands+ (hardware + time dependent)
Data: 100K-1M high-quality examples

<strong>Why so expensive?</strong>
- Must store: Model (14GB) + Gradients (14GB) + Optimizer states (28GB)
- Must update all 7 billion parameters
- Requires multiple epochs over large dataset
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>FPFT with Transformers:</strong>
                            <pre>from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,  # Effective batch = 16
    learning_rate=2e-5,
    fp16=True,                      # Use FP16 to save memory
    gradient_checkpointing=True,    # Trade compute for memory
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="epoch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset
)

trainer.train()</pre>
                        </div>

                        <table>
                            <tr>
                                <th>Technique</th>
                                <th>When to Use</th>
                                <th>Cost</th>
                                <th>Quality</th>
                            </tr>
                            <tr>
                                <td><strong>Prompt Engineering</strong></td>
                                <td>Quick prototyping, few examples</td>
                                <td>Free</td>
                                <td>â­â­â­</td>
                            </tr>
                            <tr>
                                <td><strong>QLoRA</strong></td>
                                <td>Best starting point for most</td>
                                <td>$</td>
                                <td>â­â­â­â­</td>
                            </tr>
                            <tr>
                                <td><strong>LoRA</strong></td>
                                <td>More resources, better quality</td>
                                <td>$$</td>
                                <td>â­â­â­â­â­</td>
                            </tr>
                            <tr>
                                <td><strong>Full Fine-Tuning</strong></td>
                                <td>Maximum customization, large dataset</td>
                                <td>$$$$</td>
                                <td>â­â­â­â­â­</td>
                            </tr>
                        </table>
                    </div>
                </div>

                <!-- SECTION 6: ADVANCED -->
                <div id="advanced" class="section">
                    <h2>ğŸš€ 6. Advanced Concepts: Beyond Basic LLMs</h2>
                    
                    <div class="must-know">
                        <strong>Evolution of AI:</strong> LLMs alone have limitations (hallucination, outdated knowledge, no tools). Advanced techniques extend capabilities: RAG (external knowledge), RLHF (human alignment), Agents (autonomous action).
                    </div>

                    <div class="concept-card">
                        <h3>6.1 <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> (Retrieval-Augmented Generation)</h3>
                        
                        <div class="mental-model">
                            <strong>The Problem RAG Solves:</strong>
                            <ul>
                                <li>LLMs have fixed knowledge (from training data)</li>
                                <li>Can't access recent information</li>
                                <li>Can't access private/proprietary data</li>
                                <li>Hallucinate when uncertain</li>
                            </ul>
                            <p><strong>RAG Solution:</strong> Retrieve relevant documents â†’ Augment prompt â†’ Generate answer</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>RAG Pipeline:</strong>

User Query: "What is our Q4 revenue?"
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. EMBEDDING: Query â†’ Vector           â”‚
â”‚  "Q4 revenue" â†’ [0.23, -0.45, 0.67, ...]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. VECTOR DATABASE: Similarity Search   â”‚
â”‚  Find most similar documents             â”‚
â”‚                                           â”‚
â”‚  Documents (embedded):                   â”‚
â”‚  â€¢ Q4 Financial Report [0.24, -0.44, ...] â† 95% match!â”‚
â”‚  â€¢ Q3 Revenue Summary [0.12, -0.67, ...] â† 72% match  â”‚
â”‚  â€¢ Annual Report [0.45, 0.23, ...]      â† 68% match  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. RETRIEVAL: Get top K documents       â”‚
â”‚  Retrieved: Q4 Financial Report          â”‚
â”‚  Content: "Q4 revenue was $2.3M..."     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. AUGMENTATION: Build prompt           â”‚
â”‚  Context: [Q4 Financial Report content]  â”‚
â”‚  Question: "What is our Q4 revenue?"     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. GENERATION: LLM generates answer     â”‚
â”‚  Output: "Q4 revenue was $2.3M,          â”‚
â”‚  up 15% from Q3..."                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: RAG Pipeline Flow</em>
                            </p>
                            <button onclick="animateRAG()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Start RAG Flow</button>
                            <div id="ragPipeline" class="rag-pipeline" style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 10px; padding: 20px; background: linear-gradient(135deg, #f8fafc, #e2e8f0); border-radius: 8px; min-height: 150px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch the complete RAG workflow from query to response</p>
                        </div>

                        <div class="example-box">
                            <strong>RAG vs Fine-Tuning:</strong>
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>RAG</th>
                                    <th>Fine-Tuning</th>
                                </tr>
                                <tr>
                                    <td><strong>Knowledge Update</strong></td>
                                    <td>âœ… Real-time (update documents)</td>
                                    <td>âŒ Requires retraining</td>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td>ğŸ’° Low (vector DB + inference)</td>
                                    <td>ğŸ’°ğŸ’°ğŸ’° High (GPU training)</td>
                                </tr>
                                <tr>
                                    <td><strong>Setup Time</strong></td>
                                    <td>âš¡ Minutes to hours</td>
                                    <td>ğŸŒ Hours to days</td>
                                </tr>
                                <tr>
                                    <td><strong>Transparency</strong></td>
                                    <td>âœ… Can cite sources</td>
                                    <td>âŒ Knowledge in weights</td>
                                </tr>
                                <tr>
                                    <td><strong>Domain Adaptation</strong></td>
                                    <td>â­â­â­ Good for facts</td>
                                    <td>â­â­â­â­â­ Best for style/behavior</td>
                                </tr>
                            </table>
                        </div>

                        <h4>Vector Databases for RAG</h4>
                        <table>
                            <tr>
                                <th>Database</th>
                                <th>Type</th>
                                <th>Best For</th>
                                <th>Standout Feature</th>
                            </tr>
                            <tr>
                                <td><strong>Pinecone</strong></td>
                                <td>Managed</td>
                                <td>Production, scale</td>
                                <td>Fully managed, fast</td>
                            </tr>
                            <tr>
                                <td><strong>Weaviate</strong></td>
                                <td>Open source</td>
                                <td>Hybrid search</td>
                                <td>GraphQL API, modules</td>
                            </tr>
                            <tr>
                                <td><strong>Qdrant</strong></td>
                                <td>Open source</td>
                                <td>High performance</td>
                                <td>Rust-based, fast filtering</td>
                            </tr>
                            <tr>
                                <td><strong>ChromaDB</strong></td>
                                <td>Open source</td>
                                <td>Local development</td>
                                <td>Simple, embeds in app</td>
                            </tr>
                            <tr>
                                <td><strong>FAISS</strong></td>
                                <td>Library</td>
                                <td>Research, prototyping</td>
                                <td>Meta's library, flexible</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Building a RAG System:</strong>
                            <pre># 1. Install dependencies
pip install langchain chromadb openai sentence-transformers

# 2. Load and chunk documents
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

loader = TextLoader("docs.txt")
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Size of each chunk
    chunk_overlap=200       # Overlap to maintain context
)
chunks = text_splitter.split_documents(documents)

# 3. Create embeddings and store in vector DB
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings)

# 4. Create RAG chain
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever(k=3),  # Retrieve top 3
    return_source_documents=True
)

# 5. Query
result = qa_chain({"query": "What is our Q4 revenue?"})
print(result["result"])
print("Sources:", result["source_documents"])</pre>

                            <strong>Advanced: Semantic Search Visualization</strong>
                            <pre># How semantic search works
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# Documents in database
docs = [
    "The cat sat on the mat",
    "Python programming tutorial",
    "Feline resting on carpet"
]

# Embed documents
doc_embeddings = model.encode(docs)

# Query
query = "kitten on rug"
query_embedding = model.encode(query)

# Calculate cosine similarity
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity([query_embedding], doc_embeddings)

print(similarities)
# Output: [[0.82, 0.23, 0.88]]  â† doc 3 is most similar!</pre>
                        </div>

                        <div class="diagram">
<pre>
<strong>Semantic Search in Vector Space (2D visualization):</strong>

            |
     "cat"  â€¢ "feline"
            |  â€¢ "kitten" â† Query
            |
     "dog"  â€¢
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            |
            | â€¢ "Python"
            |     â€¢ "code"
            |

Distance in vector space = Semantic similarity!
"kitten" is close to "cat", "feline" (high similarity)
"kitten" is far from "Python", "code" (low similarity)
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.2 <span class="tooltip">RLHF<span class="tooltiptext">Reinforcement Learning from Human Feedback</span></span> (Reinforcement Learning from Human Feedback)</h3>
                        
                        <div class="mental-model">
                            <strong>The Alignment Problem:</strong>
                            <p>Pre-trained LLMs predict next tokens, but that doesn't mean helpful/safe/aligned responses!</p>
                            <p><strong>RLHF</strong> teaches models what humans actually want through feedback.</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>RLHF Pipeline (3 Stages):</strong>

Stage 1: Supervised Fine-Tuning (SFT)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Human demonstrations                 â”‚
â”‚ Q: "How do I bake bread?"           â”‚
â”‚ A: [High-quality human answer]      â”‚
â”‚                                      â”‚
â”‚ Train model to mimic humans          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Stage 2: Reward Model Training
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Humans rank multiple outputs         â”‚
â”‚ Q: "Explain quantum physics"         â”‚
â”‚ Output A: [detailed] â­â­â­â­â­          â”‚
â”‚ Output B: [brief] â­â­â­               â”‚
â”‚ Output C: [wrong] â­                  â”‚
â”‚                                      â”‚
â”‚ Train reward model to predict rating â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Stage 3: Reinforcement Learning (PPO)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM generates response               â”‚
â”‚ â†’ Reward model scores it             â”‚
â”‚ â†’ Update LLM to maximize reward      â”‚
â”‚                                      â”‚
â”‚ Repeat thousands of times            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Aligned Model! ğŸ¯
</pre>
                        </div>

                        <div class="example-box">
                            <strong>RLHF in Action (GPT-4):</strong>
                            <pre>
<strong>Before RLHF:</strong>
User: "How do I hack into a website?"
Model: "Here's a step-by-step guide..." âŒ

<strong>After RLHF:</strong>
User: "How do I hack into a website?"
Model: "I can't help with that. However, I can explain ethical security testing..." âœ…

<strong>Impact:</strong>
- More helpful, harmless, honest responses
- Follows instructions better
- Reduces harmful content
- Better conversation flow
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>RLHF Libraries:</strong>
                            <ul>
                                <li><strong>trl</strong> (Transformer Reinforcement Learning) - HuggingFace
                                    <pre>pip install trl</pre>
                                </li>
                                <li><strong>RL4LMs</strong> - Research framework</li>
                                <li><strong>DeepSpeed-Chat</strong> - Microsoft's RLHF</li>
                            </ul>

                            <pre>from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from transformers import AutoTokenizer

# Load model with value head for RL
model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# PPO config
config = PPOConfig(batch_size=8, learning_rate=1.41e-5)

# Create trainer
ppo_trainer = PPOTrainer(config, model, ref_model=None, tokenizer=tokenizer)

# Training loop
for batch in dataloader:
    query_tensors = batch["input_ids"]
    response_tensors = ppo_trainer.generate(query_tensors, ...)
    rewards = compute_rewards(response_tensors)  # From reward model
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.3 AI Agents: From Passive to Active</h3>
                        
                        <div class="must-know">
                            <strong>Key Distinction:</strong>
                            <ul>
                                <li><strong>LLM:</strong> Takes input â†’ Generates output (passive)</li>
                                <li><strong>Agent:</strong> Observes â†’ Reasons â†’ Takes Actions â†’ Repeats (active)</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>Agent = LLM + Tools + Memory + Planning</strong>
                        </div>

                        <div class="diagram">
<pre>
<strong>Agent Architecture:</strong>

          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚      Environment            â”‚
          â”‚  (External World, APIs)     â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†‘           â†“
            Observation   Action
                 â”‚           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
          â”‚       AGENT              â”‚
          â”‚                          â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚   Perception       â”‚ â”‚ â† Process observations
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚           â†“              â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚   LLM (Brain)      â”‚ â”‚ â† Reasoning
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚     â†“           â†‘        â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚Toolsâ”‚    â”‚ Memory â”‚ â”‚ â† Context
          â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â”‚     â†“                   â”‚
          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
          â”‚  â”‚   Action Selection â”‚ â”‚ â† Choose what to do
          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Agent in Action - Travel Planner:</strong>
                            <pre>
User: "Plan a 3-day trip to Paris"

Agent Workflow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. PERCEIVE: User wants Paris trip, 3 days  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PLAN: Break into sub-tasks               â”‚
â”‚    - Find flights                            â”‚
â”‚    - Book hotel                              â”‚
â”‚    - Create itinerary                        â”‚
â”‚    - Check weather                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ACT: Execute with tools                   â”‚
â”‚    Tool: search_flights("Paris", "3 days")   â”‚
â”‚    Result: Found flights $450                â”‚
â”‚                                               â”‚
â”‚    Tool: search_hotels("Paris", "3 nights")  â”‚
â”‚    Result: Found hotel $300/night            â”‚
â”‚                                               â”‚
â”‚    Tool: get_weather("Paris", "next week")   â”‚
â”‚    Result: Sunny, 22Â°C                       â”‚
â”‚                                               â”‚
â”‚    Tool: create_itinerary(...)               â”‚
â”‚    Result: Day-by-day plan                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. RESPOND: Synthesize results               â”‚
â”‚    "Here's your Paris trip plan:             â”‚
â”‚     - Flights: $450 ...                      â”‚
â”‚     - Hotel: $900 ...                        â”‚
â”‚     - Itinerary: [detailed plan]"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <h4>Types of Agents</h4>
                        <table>
                            <tr>
                                <th>Type</th>
                                <th>Description</th>
                                <th>Example</th>
                            </tr>
                            <tr>
                                <td><strong>ReAct Agent</strong></td>
                                <td>Reason + Act iteratively</td>
                                <td>LangChain agents, most common</td>
                            </tr>
                            <tr>
                                <td><strong>Plan-and-Execute</strong></td>
                                <td>Plan first, then execute steps</td>
                                <td>BabyAGI, AutoGPT</td>
                            </tr>
                            <tr>
                                <td><strong>Multi-Agent</strong></td>
                                <td>Multiple agents collaborate</td>
                                <td>AutoGen, CrewAI</td>
                            </tr>
                            <tr>
                                <td><strong>Tool-Use Agent</strong></td>
                                <td>Specialized in using external tools</td>
                                <td>Function calling (GPT-4)</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Building Agents:</strong>
                            
                            <h4>1. LangChain ReAct Agent</h4>
                            <pre>from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI

# Define tools
tools = [
    Tool(
        name="Calculator",
        func=lambda x: eval(x),
        description="Useful for math calculations"
    ),
    Tool(
        name="Wikipedia",
        func=search_wikipedia,
        description="Search Wikipedia for information"
    )
]

# Create agent
agent = initialize_agent(
    tools,
    OpenAI(temperature=0),
    agent="zero-shot-react-description",
    verbose=True
)

# Run
result = agent.run("What is 25% of 180, and who invented calculus?")</pre>

                            <h4>2. AutoGen (Multi-Agent)</h4>
                            <pre>from autogen import AssistantAgent, UserProxyAgent

# Create agents
assistant = AssistantAgent("assistant")
user_proxy = UserProxyAgent("user")

# Start conversation
user_proxy.initiate_chat(
    assistant,
    message="Analyze this dataset and create visualization"
)</pre>

                            <h4>3. CrewAI (Role-Based Agents)</h4>
                            <pre>from crewai import Agent, Task, Crew

# Define agents
researcher = Agent(
    role="Researcher",
    goal="Find information about AI",
    tools=[search_tool]
)

writer = Agent(
    role="Writer",
    goal="Write blog post",
    tools=[writing_tool]
)

# Create crew
crew = Crew(agents=[researcher, writer], tasks=[task1, task2])
result = crew.kickoff()</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.4 Agentic <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> & <span class="tooltip">MCP<span class="tooltiptext">Model Context Protocol</span></span> (Model Context Protocol)</h3>
                        
                        <div class="mental-model">
                            <strong>Agentic AI = Next Evolution</strong>
                            <p>Not just answering questions, but autonomously completing complex tasks end-to-end.</p>
                        </div>

                        <h4>What is MCP (Model Context Protocol)?</h4>
                        <div class="must-know">
                            <strong>MCP</strong> is an open protocol (by Anthropic) that standardizes how AI models connect to external data sources and tools.
                            <p><strong>Problem it solves:</strong> Every AI app reinvents how to connect models to tools. MCP provides a universal interface.</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>MCP Architecture:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          AI Model (Claude, GPT)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†• MCP Protocol
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            MCP Server                     â”‚
â”‚  (Standardized interface for resources)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“          â†“          â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Databaseâ”‚ â”‚   API   â”‚ â”‚  Files  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>MCP Components:</strong>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Resources: Data sources (files, DBs) â”‚
â”‚ 2. Tools: Actions model can take        â”‚
â”‚ 3. Prompts: Templates for common tasks   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>MCP Benefits:</strong>
                            <ul>
                                <li><strong>Standardization:</strong> One protocol, many tools</li>
                                <li><strong>Security:</strong> Controlled access to resources</li>
                                <li><strong>Composability:</strong> Mix and match MCP servers</li>
                                <li><strong>Flexibility:</strong> Add new tools without changing model code</li>
                            </ul>

                            <strong>Example MCP Servers:</strong>
                            <pre>
mcp-server-filesystem   â†’ Access local files
mcp-server-postgres     â†’ Query databases
mcp-server-github       â†’ GitHub API
mcp-server-web-search   â†’ Web search
mcp-server-slack        â†’ Slack integration
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Using MCP with Claude Desktop:</strong>
                            <pre># 1. Install MCP server
npm install -g @anthropic/mcp-server-filesystem

# 2. Configure Claude Desktop (claude_desktop_config.json)
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["@anthropic/mcp-server-filesystem", "/path/to/allowed/files"]
    }
  }
}

# 3. Claude can now access files!
User: "Read the file analysis.txt"
Claude: [Uses MCP to read file] â†’ Provides answer</pre>

                            <strong>Building Custom MCP Server:</strong>
                            <pre>import { MCPServer } from "@anthropic/mcp";

const server = new MCPServer({
  name: "my-database-server",
  version: "1.0.0"
});

// Define a tool
server.addTool({
  name: "query_db",
  description: "Query the database",
  inputSchema: {
    type: "object",
    properties: {
      query: { type: "string" }
    }
  },
  handler: async (input) => {
    const result = await db.query(input.query);
    return result;
  }
});

server.start();</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>6.5 Memory Architectures for Agents</h3>
                        
                        <div class="mental-model">
                            <strong>Why Memory Matters:</strong>
                            <p>LLMs are stateless - they forget everything after each request. Agents need memory for:</p>
                            <ul>
                                <li>Maintaining conversation context</li>
                                <li>Learning from past interactions</li>
                                <li>Building long-term knowledge</li>
                            </ul>
                        </div>

                        <h4>Types of Memory</h4>
                        <table>
                            <tr>
                                <th>Type</th>
                                <th>Duration</th>
                                <th>Purpose</th>
                                <th>Implementation</th>
                            </tr>
                            <tr>
                                <td><strong>Short-Term</strong></td>
                                <td>Single conversation</td>
                                <td>Context window</td>
                                <td>Recent messages in prompt</td>
                            </tr>
                            <tr>
                                <td><strong>Working Memory</strong></td>
                                <td>Current task</td>
                                <td>Track progress</td>
                                <td>Variables, state dict</td>
                            </tr>
                            <tr>
                                <td><strong>Long-Term</strong></td>
                                <td>Persistent</td>
                                <td>Historical knowledge</td>
                                <td>Vector DB, SQL database</td>
                            </tr>
                            <tr>
                                <td><strong>Episodic</strong></td>
                                <td>Past interactions</td>
                                <td>Learn from experience</td>
                                <td>Summarized conversations</td>
                            </tr>
                        </table>

                        <div class="diagram">
<pre>
<strong>Memory Architecture:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Short-Term (Context Window)         â”‚
â”‚   Last 10 messages, current conversation   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ Store summaries
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Working Memory (Task State)          â”‚
â”‚   Current goals, progress, variables       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ Persist important info
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Long-Term Memory (Vector DB)         â”‚
â”‚   All past knowledge, searchable by query  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Implementing Memory:</strong>
                            <pre>from langchain.memory import ConversationBufferMemory, VectorStoreMemory
from langchain.chains import ConversationChain

# Short-term memory
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory)

conversation.predict(input="Hi!")
conversation.predict(input="What did I just say?")  # Remembers!

# Long-term vector memory
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

vectorstore = FAISS.from_texts([], OpenAIEmbeddings())
memory = VectorStoreMemory(vectorstore=vectorstore)

# Automatically stores and retrieves relevant context</pre>
                        </div>
                    </div>
                </div>

                <!-- SECTION 6.6: TOOLS ECOSYSTEM -->
                <div id="tools" class="section">
                    <h2>ğŸ› ï¸ 6.6. Essential Tools Ecosystem</h2>
                    
                    <div class="must-know">
                        <strong>Why This Matters:</strong> The <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> landscape has hundreds of tools. This section focuses on the most popular, actively maintained tools that professionals actually use in production.
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ“ Model Training & Fine-Tuning</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Purpose</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td><strong>PyTorch</strong></td>
                                    <td>Deep learning framework</td>
                                    <td>Research, custom architectures, flexibility</td>
                                </tr>
                                <tr>
                                    <td><strong>Transformers (HuggingFace)</strong></td>
                                    <td>Pre-trained model library</td>
                                    <td>Quick start with SOTA models</td>
                                </tr>
                                <tr>
                                    <td><strong><span class="tooltip">PEFT<span class="tooltiptext">Parameter-Efficient Fine-Tuning</span></span></strong></td>
                                    <td><span class="tooltip">LoRA<span class="tooltiptext">Low-Rank Adaptation</span></span>, QLoRA implementations</td>
                                    <td>Memory-efficient fine-tuning</td>
                                </tr>
                                <tr>
                                    <td><strong>Unsloth</strong></td>
                                    <td>2-5x faster fine-tuning</td>
                                    <td>Fast <span class="tooltip">LoRA<span class="tooltiptext">Low-Rank Adaptation</span></span>/QLoRA on consumer <span class="tooltip">GPUs<span class="tooltiptext">Graphics Processing Units</span></span></td>
                                </tr>
                                <tr>
                                    <td><strong>Axolotl</strong></td>
                                    <td>One-stop fine-tuning tool</td>
                                    <td>Complete training pipeline with YAML configs</td>
                                </tr>
                                <tr>
                                    <td><strong>Accelerate</strong></td>
                                    <td>HuggingFace training library</td>
                                    <td>Multi-<span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span>, mixed precision, easy scaling</td>
                                </tr>
                                <tr>
                                    <td><strong>DeepSpeed</strong></td>
                                    <td>Distributed training optimizer</td>
                                    <td>Multi-<span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> training, ZeRO optimization</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: Fine-Tuning with Unsloth</strong>
                            <pre>
pip install unsloth

from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",
    max_seq_length=2048,
    load_in_4bit=True
)

# Add LoRA adapters
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"]
)

# Train 2x faster!
trainer.train()</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>âš¡ Inference Engines</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Format</th>
                                    <th>Speed</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td><strong>vLLM</strong></td>
                                    <td>SafeTensors</td>
                                    <td>âš¡âš¡âš¡</td>
                                    <td><span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> inference, production APIs</td>
                                </tr>
                                <tr>
                                    <td><strong>llama.cpp</strong></td>
                                    <td><span class="tooltip">GGUF<span class="tooltiptext">GPT-Generated Unified Format</span></span></td>
                                    <td>âš¡âš¡</td>
                                    <td><span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span> inference, consumer hardware</td>
                                </tr>
                                <tr>
                                    <td><strong>Ollama</strong></td>
                                    <td>GGUF</td>
                                    <td>âš¡âš¡</td>
                                    <td>Local development, easy setup</td>
                                </tr>
                                <tr>
                                    <td><strong><span class="tooltip">TGI<span class="tooltiptext">Text Generation Inference</span></span></strong></td>
                                    <td>SafeTensors</td>
                                    <td>âš¡âš¡âš¡</td>
                                    <td>Production GPU serving (HuggingFace)</td>
                                </tr>
                                <tr>
                                    <td><strong>LM Studio</strong></td>
                                    <td>GGUF</td>
                                    <td>âš¡âš¡</td>
                                    <td>Desktop app with <span class="tooltip">GUI<span class="tooltiptext">Graphical User Interface</span></span></td>
                                </tr>
                                <tr>
                                    <td><strong>TensorRT-LLM</strong></td>
                                    <td>SafeTensors</td>
                                    <td>âš¡âš¡âš¡</td>
                                    <td>NVIDIA optimization, maximum speed</td>
                                </tr>
                                <tr>
                                    <td><strong>ONNX Runtime</strong></td>
                                    <td>ONNX</td>
                                    <td>âš¡âš¡</td>
                                    <td>Cross-platform, production deployment</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: vLLM for Fast GPU Inference</strong>
                            <pre>
pip install vllm

from vllm import LLM, SamplingParams

# Load model
llm = LLM(model="meta-llama/Llama-2-7b-chat-hf")

# Configure generation
sampling = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)

# Generate (batched!)
outputs = llm.generate(["Tell me about AI"], sampling)

# 10-20x faster than vanilla HuggingFace!</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ”¢ Quantization Tools</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Method</th>
                                    <th>Output</th>
                                </tr>
                                <tr>
                                    <td><strong>llama.cpp quantize</strong></td>
                                    <td>K-quants (Q4_K_M, Q5_K_M)</td>
                                    <td>GGUF files</td>
                                </tr>
                                <tr>
                                    <td><strong>AutoGPTQ</strong></td>
                                    <td>GPTQ (4-bit)</td>
                                    <td>SafeTensors</td>
                                </tr>
                                <tr>
                                    <td><strong>AutoAWQ</strong></td>
                                    <td><span class="tooltip">AWQ<span class="tooltiptext">Activation-aware Weight Quantization</span></span> (4-bit)</td>
                                    <td>SafeTensors</td>
                                </tr>
                                <tr>
                                    <td><strong>bitsandbytes</strong></td>
                                    <td>8-bit, 4-bit on-the-fly</td>
                                    <td>Runtime quantization</td>
                                </tr>
                                <tr>
                                    <td><strong>Optimum</strong></td>
                                    <td>ONNX quantization</td>
                                    <td>HuggingFace optimization toolkit</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: Quantize to GGUF</strong>
                            <pre>
# 1. Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# 2. Convert HuggingFace to GGUF
python convert.py /path/to/model --outfile model-f16.gguf

# 3. Quantize
./quantize model-f16.gguf model-Q4_K_M.gguf Q4_K_M

# Result: 13GB â†’ 4GB model!</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ“š <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> & Vector Databases</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Category</th>
                                    <th>Tool</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td rowspan="2"><strong>RAG Frameworks</strong></td>
                                    <td>LangChain</td>
                                    <td>Mature ecosystem, lots of integrations</td>
                                </tr>
                                <tr>
                                    <td>LlamaIndex</td>
                                    <td>Data-focused, better indexing strategies</td>
                                </tr>
                                <tr>
                                    <td rowspan="3"><strong>Vector DBs</strong></td>
                                    <td>Pinecone</td>
                                    <td>Managed cloud service, scalable</td>
                                </tr>
                                <tr>
                                    <td>Qdrant</td>
                                    <td>Self-hosted, Rust-based, fast</td>
                                </tr>
                                <tr>
                                    <td>Chroma</td>
                                    <td>Embedded, easy local development</td>
                                </tr>
                                <tr>
                                    <td rowspan="2"><strong>Additional DBs</strong></td>
                                    <td>Weaviate</td>
                                    <td>Hybrid search (vector + keyword)</td>
                                </tr>
                                <tr>
                                    <td>FAISS</td>
                                    <td>Facebook's library, research/prototyping</td>
                                </tr>
                                <tr>
                                    <td><strong>Embeddings</strong></td>
                                    <td>Sentence Transformers</td>
                                    <td>Local embedding models</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: RAG with LangChain + Chroma</strong>
                            <pre>
pip install langchain chromadb sentence-transformers

from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Load documents
loader = TextLoader("docs.txt")
documents = loader.load()

# Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=500)
chunks = splitter.split_documents(documents)

# Create embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Store in Chroma
vectorstore = Chroma.from_documents(chunks, embeddings)

# Query
results = vectorstore.similarity_search("What is AI?", k=3)</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ¤– Agent Frameworks</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Framework</th>
                                    <th>Type</th>
                                    <th>Best For</th>
                                </tr>
                                <tr>
                                    <td><strong>LangChain Agents</strong></td>
                                    <td>ReAct, Tools</td>
                                    <td>Quick prototypes, <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span> chaining</td>
                                </tr>
                                <tr>
                                    <td><strong>AutoGen</strong></td>
                                    <td>Multi-agent</td>
                                    <td>Agent collaboration, conversations</td>
                                </tr>
                                <tr>
                                    <td><strong>CrewAI</strong></td>
                                    <td>Role-based</td>
                                    <td>Task delegation, team workflows</td>
                                </tr>
                                <tr>
                                    <td><strong>LangGraph</strong></td>
                                    <td>Graph-based</td>
                                    <td>Complex workflows, state management</td>
                                </tr>
                                <tr>
                                    <td><strong>Semantic Kernel</strong></td>
                                    <td>Microsoft SDK</td>
                                    <td>Enterprise .NET/Python apps</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Quick Start: Simple Agent with LangChain</strong>
                            <pre>
from langchain.agents import create_react_agent, Tool
from langchain_openai import ChatOpenAI

# Define tools
def calculator(query):
    # âš ï¸ Demo only: never use eval() on untrusted input.
    return eval(query)

tools = [Tool(name="Calculator", func=calculator, 
              description="For math calculations")]

# Create agent
llm = ChatOpenAI(model="gpt-4")
agent = create_react_agent(llm, tools)

# Run
result = agent.invoke("What is 25 * 34?")
# Agent thinks: "I need Calculator tool"
# Runs: calculator("25 * 34")
# Returns: "850"</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸŒ Model Hosting Platforms</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Platform</th>
                                    <th>Type</th>
                                    <th>Pricing</th>
                                </tr>
                                <tr>
                                    <td><strong>HuggingFace</strong></td>
                                    <td>Model hub, free hosting</td>
                                    <td>Free tier + paid plans (varies)</td>
                                </tr>
                                <tr>
                                    <td><strong>Replicate</strong></td>
                                    <td><span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> for open models</td>
                                    <td>Pay per second</td>
                                </tr>
                                <tr>
                                    <td><strong>Together AI</strong></td>
                                    <td>Fast inference API</td>
                                    <td>Varies (priced per token / model)</td>
                                </tr>
                                <tr>
                                    <td><strong>Groq</strong></td>
                                    <td>Lightning fast LPUs</td>
                                    <td>Free tier + paid</td>
                                </tr>
                                <tr>
                                    <td><strong>Modal</strong></td>
                                    <td>Serverless GPU compute</td>
                                    <td>Varies (per-second GPU pricing)</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ’» Development Tools</h3>
                        
                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Purpose</th>
                                </tr>
                                <tr>
                                    <td><strong>Jupyter Notebook</strong></td>
                                    <td>Interactive development, experimentation</td>
                                </tr>
                                <tr>
                                    <td><strong>Weights & Biases</strong></td>
                                    <td>Experiment tracking, model metrics</td>
                                </tr>
                                <tr>
                                    <td><strong>TensorBoard</strong></td>
                                    <td>Visualization of training metrics</td>
                                </tr>
                                <tr>
                                    <td><strong>MLflow</strong></td>
                                    <td><span class="tooltip">ML<span class="tooltiptext">Machine Learning</span></span> lifecycle management, model registry</td>
                                </tr>
                                <tr>
                                    <td><strong>Docker</strong></td>
                                    <td>Containerization, deployment</td>
                                </tr>
                                <tr>
                                    <td><strong>FastAPI</strong></td>
                                    <td>Build production APIs quickly</td>
                                </tr>
                                <tr>
                                    <td><strong>Streamlit</strong></td>
                                    <td>Quick <span class="tooltip">UI<span class="tooltiptext">User Interface</span></span> prototypes for demos</td>
                                </tr>
                                <tr>
                                    <td><strong>Gradio</strong></td>
                                    <td>ML model demos and sharing</td>
                                </tr>
                            </table>
                        </div>
                    </div>

                    <div class="must-know">
                        <strong>ğŸ’¡ Pro Tip:</strong> Don't try to learn everything at once! Start with PyTorch + Transformers for training, Ollama for local inference, and LangChain for basic agents. Add specialized tools as you need them.
                    </div>
                </div>

                <!-- SECTION 7: PRACTICAL -->
                <div id="practical" class="section">
                    <h2>ğŸ’» 7. Hands-On Roadmap: Zero to Agentic AI</h2>
                    
                    <div class="must-know">
                        <strong>Learning Path:</strong> This roadmap takes you from beginner to building production AI agents. Each phase builds on the previous one. Estimated timeline: 3-6 months with consistent practice.
                    </div>

                    <div class="concept-card">
                        <h3>Phase 1: Foundations (Weeks 1-2)</h3>
                        
                        <h4>Goal: Understand basics of Python, ML, and tensors</h4>
                        
                        <div class="example-box">
                            <strong>Week 1: Python & NumPy</strong>
                            <pre>
# Learn NumPy for tensor operations
import numpy as np

# Create tensors
vector = np.array([1, 2, 3])
matrix = np.array([[1, 2], [3, 4]])
tensor = np.zeros((2, 3, 4))  # 3D tensor

# Matrix operations
A = np.random.rand(3, 3)
B = np.random.rand(3, 3)
C = np.dot(A, B)  # Matrix multiplication

<strong>Practice Projects:</strong>
1. Implement basic linear algebra operations
2. Build a simple neural network from scratch (no libraries)
3. Understand backpropagation manually
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Week 2: PyTorch Basics</strong>
                            <pre>
import torch
import torch.nn as nn

# Tensors on GPU
x = torch.tensor([1, 2, 3]).cuda()

# Simple neural network
class SimpleNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        return self.fc2(x)

model = SimpleNet()

<strong>Practice Projects:</strong>
1. MNIST digit classification
2. Image classification with CNN
3. Understand training loops, loss functions
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Resources:</strong>
                            <ul>
                                <li>Fast.ai - Practical Deep Learning (free)</li>
                                <li>PyTorch Tutorials - pytorch.org</li>
                                <li>3Blue1Brown - Neural Networks YouTube series</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 2: LLMs & Transformers (Weeks 3-4)</h3>
                        
                        <h4>Goal: Understand and use pre-trained LLMs</h4>
                        
                        <div class="example-box">
                            <strong>Week 3: HuggingFace Transformers</strong>
                            <pre>
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model
tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# Generate text
inputs = tokenizer("The future of AI is", return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))

<strong>Practice Projects:</strong>
1. Text generation with different models
2. Compare tokenizers (BPE vs WordPiece)
3. Experiment with generation parameters (temperature, top-p)
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Week 4: Understanding Attention</strong>
                            <pre>
# Visualize attention weights
from transformers import AutoModel
import torch

model = AutoModel.from_pretrained("bert-base-uncased", output_attentions=True)
tokens = tokenizer("The cat sat on the mat", return_tensors="pt")

with torch.no_grad():
    outputs = model(**tokens)
    attentions = outputs.attentions  # Attention weights from all layers

# Plot attention patterns
import matplotlib.pyplot as plt
# Visualize which words attend to which

<strong>Practice Projects:</strong>
1. Build transformer from scratch (simplified)
2. Visualize attention patterns
3. Compare BERT vs GPT architectures
</pre>
                        </div>

                        <div class="tools-box">
                            <strong>Resources:</strong>
                            <ul>
                                <li>Andrej Karpathy - GPT from Scratch (YouTube)</li>
                                <li>The Illustrated Transformer (Jay Alammar)</li>
                                <li>HuggingFace Course - huggingface.co/course</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 3: Fine-Tuning & Optimization (Weeks 5-6)</h3>
                        
                        <h4>Goal: Customize models for your use case</h4>
                        
                        <div class="example-box">
                            <strong>Week 5: LoRA Fine-Tuning</strong>
                            <pre>
from peft import LoraConfig, get_peft_model
from transformers import Trainer, TrainingArguments

# Load base model
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")

# Add LoRA adapters
lora_config = LoraConfig(r=8, lora_alpha=16, target_modules=["q_proj", "v_proj"])
model = get_peft_model(model, lora_config)

# Prepare dataset
from datasets import load_dataset
dataset = load_dataset("your-dataset")

# Train
training_args = TrainingArguments(...)
trainer = Trainer(model=model, args=training_args, train_dataset=dataset)
trainer.train()

# Save adapters
model.save_pretrained("my-lora-adapters/")

<strong>Practice Projects:</strong>
1. Fine-tune for specific domain (medical, legal, code)
2. Compare LoRA vs QLoRA
3. Measure before/after performance
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Week 6: Quantization & Deployment</strong>
                            <pre>
# Quantize model with llama.cpp
./quantize model-f16.gguf model-q4.gguf Q4_K_M

# Deploy with Ollama
ollama create mymodel -f Modelfile
ollama run mymodel

# Or deploy with vLLM for production
from vllm import LLM
llm = LLM(model="mymodel", quantization="awq")
output = llm.generate("Hello", sampling_params=...)

<strong>Practice Projects:</strong>
1. Quantize models at different levels (Q4, Q5, Q8)
2. Benchmark inference speed
3. Deploy API endpoint with FastAPI
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 4: RAG & Vector Databases (Weeks 7-8)</h3>
                        
                        <h4>Goal: Build knowledge-enhanced AI applications</h4>
                        
                        <div class="example-box">
                            <strong>Complete RAG System</strong>
                            <pre>
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# 1. Load documents
loader = DirectoryLoader('./docs', glob="**/*.txt")
documents = loader.load()

# 2. Split into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = text_splitter.split_documents(documents)

# 3. Create embeddings and store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory="./chroma_db")

# 4. Create QA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(temperature=0),
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# 5. Query
result = qa_chain({"query": "What is the revenue?"})
print(result["result"])
print("Sources:", result["source_documents"])

<strong>Practice Projects:</strong>
1. Build RAG system for your documents
2. Experiment with different chunking strategies
3. Compare vector databases (Pinecone, Qdrant, Chroma)
4. Implement hybrid search (semantic + keyword)
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 5: AI Agents (Weeks 9-10)</h3>
                        
                        <h4>Goal: Build autonomous AI agents with tool use</h4>
                        
                        <div class="example-box">
                            <strong>LangChain ReAct Agent</strong>
                            <pre>
from langchain.agents import initialize_agent, Tool
from langchain.llms import OpenAI
import requests

# Define custom tools
def search_web(query):
    # Use SerpAPI or similar
    api_key = "your-api-key"
    response = requests.get(f"https://serpapi.com/search?q={query}&api_key={api_key}")
    return response.json()["organic_results"][0]["snippet"]

def calculate(expression):
    return eval(expression)

def get_weather(city):
    # Use OpenWeatherMap API
    return f"Weather in {city}: Sunny, 22Â°C"

tools = [
    Tool(name="Search", func=search_web, description="Search the web"),
    Tool(name="Calculator", func=calculate, description="Perform calculations"),
    Tool(name="Weather", func=get_weather, description="Get weather info")
]

# Create agent
agent = initialize_agent(
    tools,
    OpenAI(temperature=0),
    agent="zero-shot-react-description",
    verbose=True
)

# Run complex query
result = agent.run(
    "What's the weather in Paris, and how much would 5 nights cost if hotels are $150/night?"
)

<strong>Practice Projects:</strong>
1. Build agent with custom tools (API calls, database queries)
2. Implement ReAct, Plan-and-Execute patterns
3. Create multi-agent system with AutoGen
4. Build personal assistant agent
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>Phase 6: Agentic AI & MCP (Weeks 11-12)</h3>
                        
                        <h4>Goal: Build production-ready agentic systems</h4>
                        
                        <div class="example-box">
                            <strong>MCP Server Setup</strong>
                            <pre>
# 1. Create custom MCP server
import { MCPServer } from "@anthropic/mcp";

const server = new MCPServer({
  name: "my-company-server",
  version: "1.0.0"
});

// Add database tool
server.addTool({
  name: "query_sales_db",
  description: "Query our sales database",
  inputSchema: {
    type: "object",
    properties: {
      query: { type: "string", description: "SQL query" }
    },
    required: ["query"]
  },
  handler: async (input) => {
    const result = await db.query(input.query);
    return { results: result.rows };
  }
});

// Add file system access
server.addResource({
  uri: "file:///company/reports",
  name: "Company Reports",
  description: "Access to company report files"
});

server.start();

<strong>Practice Projects:</strong>
1. Build MCP server for your company's data
2. Create agent that uses multiple MCP servers
3. Implement security and access control
4. Deploy production agent system
</pre>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ¯ Capstone Projects</h3>
                        
                        <div class="example-box">
                            <strong>Beginner:</strong>
                            <ul>
                                <li>ğŸ“ Personal document Q&A chatbot (RAG)</li>
                                <li>ğŸ’¬ Customer support chatbot with fine-tuning</li>
                                <li>ğŸ“Š Data analysis assistant</li>
                            </ul>

                            <strong>Intermediate:</strong>
                            <ul>
                                <li>ğŸ¤– Research assistant agent (web search + summarization)</li>
                                <li>ğŸ“§ Email automation agent</li>
                                <li>ğŸ” Code review assistant</li>
                            </ul>

                            <strong>Advanced:</strong>
                            <ul>
                                <li>ğŸ¢ Enterprise knowledge management system</li>
                                <li>ğŸ¤ Multi-agent software development team</li>
                                <li>ğŸ¯ Autonomous business analyst (data + insights + reports)</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- SECTION 7.5: KEY TOPICS TO SURVIVE AI ERA -->
                <div id="keytopics" class="section">
                    <h2>ğŸ¯ 7.5. Key Topics to Survive the AI Era</h2>
                    
                    <div class="must-know">
                        <strong>Reality Check:</strong> You don't need to know everything! But these 10 concepts are non-negotiable if you want to stay relevant in the <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span>-powered future.
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ”¥ The Essential 10</h3>
                        
                        <div class="example-box">
                            <h4>1. Prompt Engineering (Start Here!)</h4>
                            <p><strong>Why:</strong> It's the #1 skill employers want right now. Everyone needs to communicate effectively with <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span>.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>Zero-shot, few-shot, chain-of-thought prompting</li>
                                <li>System prompts vs user prompts</li>
                                <li>Temperature, top-p, max tokens control</li>
                            </ul>
                            <p><strong>Reality:</strong> Master this in 1 week. Practice daily with ChatGPT/<span class="tooltip">Claude<span class="tooltiptext">Anthropic's AI Assistant</span></span>.</p>
                        </div>

                        <div class="example-box">
                            <h4>2. <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> (Retrieval-Augmented Generation)</h4>
                            <p><strong>Why:</strong> 80% of enterprise <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> use cases involve RAG. It's how you make <span class="tooltip">LLMs<span class="tooltiptext">Large Language Models</span></span> work with your private data.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>How embeddings convert text to vectors</li>
                                <li>Vector databases (Pinecone, Chroma)</li>
                                <li>Document chunking strategies</li>
                                <li>Semantic search basics</li>
                            </ul>
                            <p><strong>Reality:</strong> Build 1 RAG system with your company docs. That's itâ€”you're hireable.</p>
                        </div>

                        <div class="example-box">
                            <h4>3. Fine-Tuning vs Prompting (Know When to Use Each)</h4>
                            <p><strong>Why:</strong> Saves you 100x costs. Most problems don't need fine-tuning!</p>
                            <p><strong>Decision Tree:</strong></p>
                            <ul>
                                <li>New knowledge â†’ Use <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span></li>
                                <li>New style/format â†’ Fine-tune</li>
                                <li>New task â†’ Start with prompting, fine-tune if needed</li>
                            </ul>
                            <p><strong>Reality:</strong> Try prompting + RAG first. Fine-tune only if accuracy still poor.</p>
                        </div>

                        <div class="example-box">
                            <h4>4. <span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> Usage (OpenAI, Anthropic, etc.)</h4>
                            <p><strong>Why:</strong> You need to integrate <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> into real applications, not just chat.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>Making <span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> calls (Python requests, official SDKs)</li>
                                <li>Streaming responses for better UX</li>
                                <li>Cost management (batching, caching)</li>
                                <li>Error handling and retries</li>
                            </ul>
                            <p><strong>Reality:</strong> Build 3-4 small apps. Instant confidence boost.</p>
                        </div>

                        <div class="example-box">
                            <h4>5. Vector Embeddings (The Math Behind <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> "Understanding")</h4>
                            <p><strong>Why:</strong> Core to search, recommendations, RAG, similarity matching.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>What embeddings are (text â†’ numbers)</li>
                                <li>Cosine similarity for finding similar items</li>
                                <li>When to use sentence-transformers vs <span class="tooltip">OpenAI<span class="tooltiptext">OpenAI Embeddings API</span></span> embeddings</li>
                            </ul>
                            <p><strong>Reality:</strong> You don't need a PhD. Just understand: similar meaning â†’ similar vectors.</p>
                        </div>

                        <div class="example-box">
                            <h4>6. Model Selection (Choosing the Right <span class="tooltip">LLM<span class="tooltiptext">Large Language Model</span></span>)</h4>
                            <p><strong>Why:</strong> Wrong model = wasted money or poor results.</p>
                            <p><strong>Quick Guide:</strong></p>
                            <ul>
                                <li><strong>GPT-4-class models:</strong> Strong reasoning, typically higher cost (pricing varies)</li>
                                <li><strong>Smaller/faster models:</strong> Cheaper and low-latency, often â€œgood enoughâ€ for many apps</li>
                                <li><strong>Claude 3.5 Sonnet:</strong> Best for writing, coding</li>
                                <li><strong>Llama 3:</strong> Free, self-hosted, privacy</li>
                                <li><strong>Mistral:</strong> European, multilingual</li>
                            </ul>
                            <p><strong>Reality:</strong> Start with GPT-3.5. Upgrade to GPT-4 only if accuracy matters.</p>
                        </div>

                        <div class="example-box">
                            <h4>7. AI Agents (The Future of Work)</h4>
                            <p><strong>Why:</strong> Agents are replacing simple workflows and automations.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>ReAct pattern (Reason + Act)</li>
                                <li>Tool calling / function calling</li>
                                <li>When agents fail (and fallbacks)</li>
                            </ul>
                            <p><strong>Reality:</strong> Build 1 simple agent (e.g., "Check email â†’ Summarize â†’ Reply"). That's enough to understand agents.</p>
                        </div>

                        <div class="example-box">
                            <h4>8. Context Windows & Token Limits</h4>
                            <p><strong>Why:</strong> You'll hit limits. Know how to handle them.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>What a token is (~4 characters)</li>
                                <li>Model limits (varies by model/version; some offer very large contexts)</li>
                                <li>Strategies: summarization, RAG, sliding windows</li>
                            </ul>
                            <p><strong>Reality:</strong> Count tokens before sending (tiktoken library). Plan accordingly.</p>
                        </div>

                        <div class="example-box">
                            <h4>9. Inference vs Training (Know the Difference!)</h4>
                            <p><strong>Why:</strong> Saves you from overhyping or underestimating what's possible.</p>
                            <p><strong>Key Differences:</strong></p>
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Inference</th>
                                    <th>Training</th>
                                </tr>
                                <tr>
                                    <td>Cost</td>
                                    <td>Often cents or less per request (varies by model + tokens)</td>
                                    <td>Often thousands to millions+ (depends on dataset + compute)</td>
                                </tr>
                                <tr>
                                    <td>Time</td>
                                    <td>0.1-5 seconds</td>
                                    <td>Days to months</td>
                                </tr>
                                <tr>
                                    <td>Hardware</td>
                                    <td>1 <span class="tooltip">GPU<span class="tooltiptext">Graphics Processing Unit</span></span> or <span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span></td>
                                    <td>100s of <span class="tooltip">GPUs<span class="tooltiptext">Graphics Processing Units</span></span></td>
                                </tr>
                            </table>
                            <p><strong>Reality:</strong> 99% of developers only do inference. You're using pre-trained models.</p>
                        </div>

                        <div class="example-box">
                            <h4>10. Ethical <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> & Bias Awareness</h4>
                            <p><strong>Why:</strong> Legal liability + PR disasters. Companies care about this.</p>
                            <p><strong>What to Learn:</strong></p>
                            <ul>
                                <li>Models can hallucinate (make up facts)</li>
                                <li>Biases in training data persist in outputs</li>
                                <li>GDPR/privacy concerns with user data</li>
                                <li>When to add human-in-the-loop verification</li>
                            </ul>
                            <p><strong>Reality:</strong> Add disclaimers. Never trust <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> for critical decisions without human review.</p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ“Š Your Learning Priority Matrix</h3>
                        
                        <table>
                            <tr>
                                <th>If You're A...</th>
                                <th>Must Learn (Priority 1)</th>
                                <th>Should Learn (Priority 2)</th>
                                <th>Nice to Have (Priority 3)</th>
                            </tr>
                            <tr>
                                <td><strong>Software Developer</strong></td>
                                <td><span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> usage, RAG, Prompting</td>
                                <td>Agents, Vector DBs, Fine-tuning</td>
                                <td>Model training, Quantization</td>
                            </tr>
                            <tr>
                                <td><strong>Data Scientist</strong></td>
                                <td>Embeddings, Fine-tuning, RAG</td>
                                <td>LoRA, Agents, Model evaluation</td>
                                <td>RLHF, Distributed training</td>
                            </tr>
                            <tr>
                                <td><strong>Product Manager</strong></td>
                                <td>Prompting, Model selection, Cost management</td>
                                <td>RAG basics, Agent capabilities</td>
                                <td>Technical architecture</td>
                            </tr>
                            <tr>
                                <td><strong>Business Analyst</strong></td>
                                <td>Prompting, <span class="tooltip">ChatGPT<span class="tooltiptext">OpenAI's Conversational AI</span></span>/Claude mastery</td>
                                <td>RAG concepts, Use case identification</td>
                                <td>Everything else</td>
                            </tr>
                            <tr>
                                <td><strong>ML Engineer</strong></td>
                                <td>Fine-tuning, Quantization, Inference optimization</td>
                                <td>Agents, RLHF, Distributed training</td>
                                <td>Prompt engineering (already know)</td>
                            </tr>
                            <tr>
                                <td><strong>IT Architect</strong></td>
                                <td>Model selection, Deployment strategies, Cost optimization</td>
                                <td><span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> architecture, Scaling, Security</td>
                                <td>Fine-tuning details, Model training</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>ğŸ”§ Tools Ecosystem</h3>
                        
                        <div class="must-know">
                            <strong>Core Principle:</strong> Master the fundamentals first. Don't chase every shiny new tool. These 10 tools cover 90% of real-world <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> work. Deep dives below on key frameworks.
                        </div>

                        <div class="tools-box">
                            <table>
                                <tr>
                                    <th>Priority</th>
                                    <th>Tool</th>
                                    <th>Why Essential</th>
                                    <th>Time to Learn</th>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‡ #1</strong></td>
                                    <td><strong><a href="https://pytorch.org/docs/" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">Python + PyTorch</a></strong></td>
                                    <td>Foundation of all <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span> work. 95% of <span class="tooltip">ML<span class="tooltiptext">Machine Learning</span></span> code is Python.</td>
                                    <td>2-3 weeks basics</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‡ #2</strong></td>
                                    <td><strong><a href="https://huggingface.co/docs/transformers" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">HuggingFace Transformers</a></strong></td>
                                    <td>Access to 500K+ models. Industry standard.</td>
                                    <td>1 week</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‡ #3</strong></td>
                                    <td><strong><a href="https://platform.openai.com/docs/api-reference" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">OpenAI/<span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> SDKs</a></strong></td>
                                    <td>90% of production apps use APIs, not local models.</td>
                                    <td>2-3 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥ˆ #4</strong></td>
                                    <td><strong><a href="https://python.langchain.com/" target="_blank" rel="noopener noreferrer" style="color: #3b82f6; text-decoration: underline;">LangChain</a></strong></td>
                                    <td>Most popular <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span>/Agent framework. Job requirement.</td>
                                    <td>1 week</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥ˆ #5</strong></td>
                                    <td><strong>Chroma/Pinecone</strong></td>
                                    <td>Vector databases power every <span class="tooltip">RAG<span class="tooltiptext">Retrieval-Augmented Generation</span></span> system.</td>
                                    <td>3-4 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥ˆ #6</strong></td>
                                    <td><strong>Ollama</strong></td>
                                    <td>Easiest way to run models locally. Essential for development.</td>
                                    <td>1 day</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #7</strong></td>
                                    <td><strong>Unsloth/<span class="tooltip">PEFT<span class="tooltiptext">Parameter-Efficient Fine-Tuning</span></span></strong></td>
                                    <td>Fine-tuning on consumer hardware. 2x faster training.</td>
                                    <td>3-5 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #8</strong></td>
                                    <td><strong>vLLM/TGI</strong></td>
                                    <td>Production inference. 10x faster than vanilla serving.</td>
                                    <td>2-3 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #9</strong></td>
                                    <td><strong>llama.cpp</strong></td>
                                    <td><span class="tooltip">CPU<span class="tooltiptext">Central Processing Unit</span></span> inference, GGUF quantization. Powers many tools.</td>
                                    <td>2-3 days</td>
                                </tr>
                                <tr>
                                    <td><strong>ğŸ¥‰ #10</strong></td>
                                    <td><strong>Weights & Biases</strong></td>
                                    <td>Track experiments, share results. Industry standard monitoring.</td>
                                    <td>2 days</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Learning Path Recommendation:</strong>
                            <pre>
<strong>Month 1 - Essentials (Start Here!):</strong>
âœ“ Python basics â†’ PyTorch fundamentals
âœ“ HuggingFace Transformers (load & run models)
âœ“ OpenAI API (build 3-4 small apps)

<strong>Month 2 - RAG & Production:</strong>
âœ“ LangChain (chains, agents, memory)
âœ“ Chroma/Pinecone (vector search)
âœ“ Ollama (local development)

<strong>Month 3 - Advanced (When Needed):</strong>
âœ“ Unsloth/PEFT (if you need fine-tuning)
âœ“ vLLM (if deploying to production)
âœ“ llama.cpp (if targeting CPUs/edge devices)

<strong>Throughout - Monitoring:</strong>
âœ“ Weights & Biases for tracking experiments
</pre>
                        </div>

                        <div class="mental-model">
                            <strong>Reality Check - What You DON'T Need (Yet):</strong>
                            <ul>
                                <li>âŒ <strong>DeepSpeed, FSDP:</strong> Only for training from scratch (you won't)</li>
                                <li>âŒ <strong>TensorRT, ONNX:</strong> Only for extreme optimization (start simple)</li>
                                <li>âŒ <strong>Kubernetes, Ray:</strong> Only when scaling to 100K+ users</li>
                                <li>âŒ <strong>Custom CUDA kernels:</strong> Unless you're building foundational infrastructure</li>
                                <li>âœ… <strong>Focus:</strong> Start with the 10 tools above. Add others when you hit real limitations.</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-wandb">
                        <h3>ğŸ“Š 7.5.1 Deep Dive: Weights & Biases (W&B)</h3>
                        
                        <div class="must-know">
                            <strong>What is Weights & Biases?</strong> The industry-standard MLOps platform for experiment tracking, model management, and team collaboration. Think of it as "Git + Excel + Dashboards" for ML - version your experiments, compare results, and share insights instantly.
                        </div>

                        <div class="mental-model">
                            <strong>The Problem W&B Solves:</strong>
                            <p>Training AI models is iterative chaos:</p>
                            <ul>
                                <li>âŒ <strong>Version Hell:</strong> "Which hyperparameters did I use for that 94% accuracy run last week?"</li>
                                <li>âŒ <strong>Excel Madness:</strong> Tracking 50 experiments in spreadsheets is error-prone</li>
                                <li>âŒ <strong>Lost Work:</strong> Teammate trained a great model but forgot to save the config</li>
                                <li>âŒ <strong>Blind Debugging:</strong> Model failing but can't visualize what's happening</li>
                            </ul>
                            <p><strong>W&B provides:</strong> Automatic logging + beautiful visualizations + collaborative workspace + model registry + deployment tracking.</p>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(251, 191, 36, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #d97706; margin-bottom: 10px;">
                                <em>Interactive: Live Experiment Tracking Dashboard</em>
                            </p>
                            <button onclick="animateWandB()" style="padding: 10px 20px; background: linear-gradient(135deg, #f59e0b, #d97706); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(245, 158, 11, 0.3); margin-bottom: 15px;">â–¶ï¸ Simulate Training Run</button>
                            <div id="wandbDashboard" style="padding: 20px; background: linear-gradient(135deg, #fef3c7, #fde68a); border-radius: 8px; min-height: 400px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);">
                                <div style="text-align: center; color: #92400e; font-size: 14px; padding: 40px;">Click "Simulate Training Run" to watch live metrics, compare experiments, and see the final model selection</div>
                            </div>
                            <p style="font-size: 0.85em; color: #92400e; margin-top: 10px;">Watch how W&B tracks loss, accuracy, and GPU metrics in real-time across 3 competing experiments</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>W&B Workflow (5 Steps to Production):</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: Initialize Tracking                â”‚
â”‚  wandb.init(project="my-llm-finetuning")   â”‚
â”‚  â†’ Creates cloud workspace                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: Auto-Log Everything                â”‚
â”‚  wandb.log({"loss": 0.5, "lr": 2e-5})      â”‚
â”‚  â†’ Metrics, hyperparameters, system stats   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: Visualize Live                     â”‚
â”‚  Dashboard updates every 10 seconds         â”‚
â”‚  â†’ Line charts, GPU usage, cost tracking    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: Compare Experiments                â”‚
â”‚  Parallel coordinates plot shows best run   â”‚
â”‚  â†’ "Experiment 7 had lowest loss at epoch 3"â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: Save & Deploy Best Model           â”‚
â”‚  wandb.log_model(model, "production")       â”‚
â”‚  â†’ Team downloads from Model Registry       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>W&B Example: Track Fine-Tuning in 5 Lines</strong>
                            <pre>import wandb
from transformers import TrainingArguments

# Initialize tracking
wandb.init(
    project="llama-3-finetuning",
    config={
        "learning_rate": 2e-5,
        "epochs": 3,
        "batch_size": 4,
        "model": "meta-llama/Llama-3-8B"
    }
)

# Auto-integrate with HuggingFace Trainer
training_args = TrainingArguments(
    output_dir="./results",
    report_to="wandb",  # â† Automatic logging!
    logging_steps=10
)

# Train model (W&B logs everything automatically)
trainer = Trainer(model=model, args=training_args, train_dataset=dataset)
trainer.train()

# Log final model artifact
wandb.log_model(path="./results", name="llama-3-custom")
wandb.finish()

# ğŸ‰ Dashboard now shows: loss curves, GPU%, memory, throughput, model file</pre>
                            <p style="margin-top: 10px;"><strong>Without W&B:</strong> You'd manually save logs, plot graphs in matplotlib, and track configs in text files. 100x more work!</p>
                        </div>

                        <div class="tools-box">
                            <strong>W&B Key Features (Why Teams Pay for This):</strong>
                            <table>
                                <tr>
                                    <th>Feature</th>
                                    <th>Purpose</th>
                                    <th>Example</th>
                                </tr>
                                <tr>
                                    <td><strong>Experiment Tracking</strong></td>
                                    <td>Auto-log metrics & hyperparameters</td>
                                    <td>"Show me all runs with lr > 1e-5"</td>
                                </tr>
                                <tr>
                                    <td><strong>Model Registry</strong></td>
                                    <td>Version models like Git commits</td>
                                    <td>"Deploy model from Experiment #42"</td>
                                </tr>
                                <tr>
                                    <td><strong>Artifacts</strong></td>
                                    <td>Store datasets, configs, checkpoints</td>
                                    <td>"Download training dataset v3.2"</td>
                                </tr>
                                <tr>
                                    <td><strong>Sweeps</strong></td>
                                    <td>Automated hyperparameter search</td>
                                    <td>"Try 20 lr values, pick best"</td>
                                </tr>
                                <tr>
                                    <td><strong>Reports</strong></td>
                                    <td>Share results with stakeholders</td>
                                    <td>"Executive summary with graphs"</td>
                                </tr>
                                <tr>
                                    <td><strong>Alerts</strong></td>
                                    <td>Slack/email when training fails</td>
                                    <td>"Notify me if loss > 2.0"</td>
                                </tr>
                                <tr>
                                    <td><strong>System Metrics</strong></td>
                                    <td>GPU usage, CPU, memory, disk</td>
                                    <td>"Why is GPU at 40%? Memory leak?"</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Real-World Use Case: Debugging Training Issues</strong>
                            <pre><strong>Scenario:</strong> Your model loss plateaus at 1.2 after epoch 1.

<strong>W&B Dashboard Shows:</strong>
1. Loss curve: Flat after 1000 steps
2. Learning rate: Decayed to 1e-7 (too low!)
3. GPU utilization: 45% (data loading bottleneck)
4. Gradient norms: Exploding to 1e6 (need clipping)

<strong>Fix in 5 Minutes:</strong>
âœ“ Increase learning rate to 5e-5
âœ“ Add gradient clipping (max_norm=1.0)
âœ“ Increase dataloader workers from 2â†’8

<strong>Result:</strong> New run reaches 0.4 loss. W&B saved you 3 days of blind debugging!</pre>
                        </div>

                        <div class="tools-box">
                            <strong>W&B vs Alternatives:</strong>
                            <table>
                                <tr>
                                    <th>Tool</th>
                                    <th>Best For</th>
                                    <th>Collaboration</th>
                                    <th>Setup Time</th>
                                    <th>Cost</th>
                                </tr>
                                <tr>
                                    <td><strong>W&B</strong></td>
                                    <td>Teams, production projects</td>
                                    <td>â­â­â­â­â­ Cloud dashboards</td>
                                    <td>2 minutes</td>
                                    <td>Free up to 100GB</td>
                                </tr>
                                <tr>
                                    <td><strong>TensorBoard</strong></td>
                                    <td>Solo projects, PyTorch users</td>
                                    <td>â­â­ Local only</td>
                                    <td>5 minutes</td>
                                    <td>Free</td>
                                </tr>
                                <tr>
                                    <td><strong>MLflow</strong></td>
                                    <td>Self-hosted, on-prem</td>
                                    <td>â­â­â­â­ Requires server</td>
                                    <td>30 minutes</td>
                                    <td>Free (self-host)</td>
                                </tr>
                                <tr>
                                    <td><strong>Neptune.ai</strong></td>
                                    <td>Alternative to W&B</td>
                                    <td>â­â­â­â­ Similar features</td>
                                    <td>3 minutes</td>
                                    <td>Free up to 200GB</td>
                                </tr>
                                <tr>
                                    <td><strong>Excel + Manual Logs</strong></td>
                                    <td>Masochists ğŸ˜…</td>
                                    <td>â­ Email attachments</td>
                                    <td>Instant</td>
                                    <td>Free (+ your sanity)</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>W&B FAQs:</strong>
                            <ul>
                                <li><strong>Q: Is it free for personal projects?</strong><br>
                                    âœ… Yes! 100GB storage, unlimited experiments. Perfect for learning & portfolios.</li>
                                <li><strong>Q: Does it work offline?</strong><br>
                                    âœ… Yes. Logs locally, syncs when internet returns.</li>
                                <li><strong>Q: Can I self-host it?</strong><br>
                                    âš ï¸ Yes, but requires enterprise license ($$$). Use MLflow for free self-hosting.</li>
                                <li><strong>Q: Which is better: W&B or TensorBoard?</strong><br>
                                    ğŸ“Š <strong>TensorBoard:</strong> Great for solo PyTorch work, local debugging.<br>
                                    ğŸ“Š <strong>W&B:</strong> Better for teams, comparing experiments, production tracking.</li>
                                <li><strong>Q: Do I need it if I'm just learning?</strong><br>
                                    ğŸ’¡ Not at first. Start logging after you've trained 10+ models and realize you can't remember which config worked best.</li>
                                <li><strong>Q: How much does the paid tier cost?</strong><br>
                                    ğŸ’° ~$50/user/month for teams. Includes unlimited storage, advanced features, priority support.</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>When to Use W&B:</strong>
                            <ul>
                                <li>âœ… <strong>Fine-tuning models:</strong> Track loss curves across multiple runs</li>
                                <li>âœ… <strong>Hyperparameter tuning:</strong> Compare 50 experiments visually</li>
                                <li>âœ… <strong>Team collaboration:</strong> Share results without emailing screenshots</li>
                                <li>âœ… <strong>Production monitoring:</strong> Track deployed model performance</li>
                                <li>âœ… <strong>Research reproducibility:</strong> Log everything for papers</li>
                                <li>âš ï¸ <strong>Simple API calls:</strong> Overkill if you're just calling OpenAI API (no training)</li>
                                <li>âš ï¸ <strong>One-off scripts:</strong> Not worth setup for single-use code</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Getting Started (5-Minute Quickstart):</strong>
                            <pre><strong>Step 1: Install</strong>
pip install wandb

<strong>Step 2: Login (creates free account)</strong>
wandb login
# Opens browser â†’ Sign in with GitHub/Google

<strong>Step 3: Add to Your Code</strong>
import wandb

# Start tracking
wandb.init(project="my-first-project")

# Log metrics in your training loop
for epoch in range(10):
    loss = train_one_epoch()  # Your training code
    wandb.log({"loss": loss, "epoch": epoch})

# Done! View dashboard at https://wandb.ai/username/my-first-project

<strong>Step 4: Open Dashboard</strong>
âœ“ See live loss curve
âœ“ View hyperparameters
âœ“ Check GPU usage
âœ“ Compare with future runs</pre>
                        </div>

                        <div class="must-know">
                            <strong>ğŸ¯ Bottom Line:</strong> W&B is like having a senior ML engineer looking over your shoulder, showing you what's wrong and what's working. It's free for learning, and every AI company uses it (or wishes they did). <strong>Learn it once, use it forever.</strong>
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-langchain">
                        <h3>ğŸ¦œ 7.5.2 Deep Dive: LangChain Framework</h3>
                        
                        <div class="must-know">
                            <strong>What is LangChain?</strong> The most popular framework for building LLM applications. Think of it as "jQuery for AI" - it handles the repetitive plumbing so you can focus on logic.
                        </div>

                        <div class="mental-model">
                            <strong>The Problem LangChain Solves:</strong>
                            <p>Building AI apps requires chaining multiple operations:</p>
                            <ul>
                                <li>Prompt templates â†’ LLM call â†’ Parse output â†’ Store in memory â†’ Use in next prompt</li>
                                <li>Load documents â†’ Split â†’ Embed â†’ Store â†’ Search â†’ Retrieve â†’ Generate</li>
                                <li>User input â†’ Agent decides tool â†’ Execute â†’ Parse result â†’ Decide next action</li>
                            </ul>
                            <p><strong>LangChain provides:</strong> Pre-built components for each step + orchestration to chain them together.</p>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(59, 130, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #1e40af; margin-bottom: 10px;">
                                <em>Interactive: LangChain Pipeline Flow</em>
                            </p>
                            <button onclick="animateLangChain()" style="padding: 10px 20px; background: linear-gradient(135deg, #1e40af, #3b82f6); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(30, 64, 175, 0.3); margin-bottom: 15px;">Show LangChain Flow</button>
                            <div id="langchainPipeline" style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 10px; padding: 20px; background: linear-gradient(135deg, #f8fafc, #e2e8f0); border-radius: 8px; min-height: 150px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);"></div>
                            <p style="font-size: 0.85em; color: #64748b; margin-top: 10px;">Watch how LangChain chains components together for complex AI workflows</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>LangChain Core Concepts:</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CHAINS - Sequential Steps               â”‚
â”‚     Prompt â†’ LLM â†’ Output Parser â†’ Memory  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AGENTS - Dynamic Decision Making        â”‚
â”‚     LLM decides which tools to use & when   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. MEMORY - Context Persistence            â”‚
â”‚     Store conversation history across calls â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. RETRIEVERS - Smart Document Search      â”‚
â”‚     Vector DB + semantic search built-in    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>
                        </div>

                        <div class="example-box">
                            <strong>LangChain Example: Simple RAG in 10 Lines</strong>
                            <p style="margin: 8px 0 12px; font-size: 0.9em; color: #64748b;">Note: LangChainâ€™s Python APIs evolve quickly; depending on your version, some integrations/import paths may live in separate packages (e.g., <code>langchain-community</code>, <code>langchain-openai</code>). Treat this snippet as conceptual scaffolding and cross-check the current docs for exact imports.</p>
                            <pre>from langchain.document_loaders import TextLoader
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI

# Load & embed documents
loader = TextLoader('docs.txt')
docs = loader.load()
vectorstore = Chroma.from_documents(docs, OpenAIEmbeddings())

# Create RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=OpenAI(),
    retriever=vectorstore.as_retriever()
)

# Ask questions
qa_chain.run("What is the main topic?")</pre>
                            <p style="margin-top: 10px;"><strong>Without LangChain:</strong> Would take 100+ lines handling embeddings, chunking, retrieval, prompt formatting!</p>
                        </div>

                        <div class="tools-box">
                            <strong>LangChain Key Components:</strong>
                            <table>
                                <tr>
                                    <th>Component</th>
                                    <th>Purpose</th>
                                    <th>Example Use Case</th>
                                </tr>
                                <tr>
                                    <td><strong>PromptTemplate</strong></td>
                                    <td>Reusable prompt formatting</td>
                                    <td>"Summarize {document} for {audience}"</td>
                                </tr>
                                <tr>
                                    <td><strong>LLMChain</strong></td>
                                    <td>Prompt + LLM + Output</td>
                                    <td>Basic Q&A with formatting</td>
                                </tr>
                                <tr>
                                    <td><strong>RetrievalQA</strong></td>
                                    <td>RAG out of the box</td>
                                    <td>Chat with your documents</td>
                                </tr>
                                <tr>
                                    <td><strong>ConversationChain</strong></td>
                                    <td>Chatbot with memory</td>
                                    <td>Multi-turn conversations</td>
                                </tr>
                                <tr>
                                    <td><strong>Agent</strong></td>
                                    <td>LLM that uses tools</td>
                                    <td>Search web, run code, call APIs</td>
                                </tr>
                                <tr>
                                    <td><strong>VectorStore</strong></td>
                                    <td>Unified vector DB interface</td>
                                    <td>Swap Chroma/Pinecone/FAISS easily</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>When to Use LangChain?</strong>
                            <ul>
                                <li>âœ… <strong>Building RAG systems:</strong> Saves 80% of boilerplate code</li>
                                <li>âœ… <strong>Creating agents:</strong> Best framework for tool-using LLMs</li>
                                <li>âœ… <strong>Prototyping fast:</strong> Get from idea to demo in hours</li>
                                <li>âœ… <strong>Learning AI patterns:</strong> See best practices built-in</li>
                                <li>âš ï¸ <strong>Production at scale:</strong> Consider simpler alternatives for high-throughput</li>
                                <li>âš ï¸ <strong>Simple prompting:</strong> Overkill if you just need OpenAI API</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>LangChain vs Alternatives:</strong>
                            <ul>
                                <li><strong>LangChain:</strong> Most features, steepest learning curve, great for prototyping</li>
                                <li><strong>LlamaIndex:</strong> Better for RAG specifically, simpler API</li>
                                <li><strong>Raw APIs (OpenAI SDK):</strong> Maximum control, more code, best for production</li>
                                <li><strong>Semantic Kernel (Microsoft):</strong> Better C#/.NET integration</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-unsloth">
                        <h3>âš¡ 7.5.3 Deep Dive: Unsloth & PEFT</h3>
                        
                        <div class="must-know">
                            <strong>What is Unsloth?</strong> A high-performance fine-tuning library that's 2-5x faster than standard HuggingFace training. Combines custom CUDA kernels + memory optimization to fine-tune 7B+ models on consumer GPUs.<br>
                            <strong>What is PEFT?</strong> HuggingFace's Parameter-Efficient Fine-Tuning library. Instead of updating 7 billion weights, you update 10 million (LoRA adapters). Train on 1 GPU instead of 8.
                        </div>

                        <div class="mental-model">
                            <strong>The Problem Unsloth & PEFT Solve:</strong>
                            <p>Standard fine-tuning is prohibitively expensive:</p>
                            <ul>
                                <li>âŒ <strong>Memory:</strong> Llama-3-8B needs 120GB VRAM for full fine-tuning (8x A100 GPUs = $30K)</li>
                                <li>âŒ <strong>Speed:</strong> Training takes 10 hours on expensive cloud GPUs</li>
                                <li>âŒ <strong>Storage:</strong> Each fine-tuned model is 16GB (can't train 50 variants)</li>
                                <li>âŒ <strong>Accessibility:</strong> Only research labs can afford this</li>
                            </ul>
                            <p><strong>Unsloth + PEFT enable:</strong> Fine-tune Llama-3-8B on 16GB consumer GPU (RTX 4090) in 2 hours, saving only 100MB adapter weights.</p>
                        </div>

                        <div style="margin: 20px 0; padding: 20px; background: rgba(139, 92, 246, 0.05); border-radius: 8px; text-align: center;">
                            <p style="font-weight: 600; color: #7c3aed; margin-bottom: 10px;">
                                <em>Interactive: Fine-Tuning Speed Comparison</em>
                            </p>
                            <button onclick="animateUnsloth()" style="padding: 10px 20px; background: linear-gradient(135deg, #8b5cf6, #7c3aed); color: white; border: none; border-radius: 8px; cursor: pointer; font-weight: 600; box-shadow: 0 4px 12px rgba(139, 92, 246, 0.3); margin-bottom: 15px;">âš¡ Race: Standard vs Unsloth</button>
                            <div id="unslothRace" style="padding: 20px; background: linear-gradient(135deg, #faf5ff, #f3e8ff); border-radius: 8px; min-height: 450px; box-shadow: inset 0 2px 8px rgba(0,0,0,0.05);">
                                <div style="text-align: center; color: #581c87; font-size: 14px; padding: 40px;">Click "Race" to watch side-by-side speed comparison: Standard Training (10 hrs) vs Unsloth+PEFT (2 hrs)</div>
                            </div>
                            <p style="font-size: 0.85em; color: #6b21a8; margin-top: 10px;">Watch memory usage, training speed, and final model size differences</p>
                        </div>

                        <div class="diagram">
<pre>
<strong>How LoRA/PEFT Works (Technical Deep Dive):</strong>

Original Transformer Layer (Llama-3-8B):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Input: x (4096 dimensions)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  W_original: 4096Ã—4096 matrix                â”‚
â”‚  (16.7 million parameters) â† FROZEN!         â”‚
â”‚  Output: WÂ·x                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>Add LoRA Adapters (r=16 rank):</strong>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  A: 4096Ã—16 matrix (65K parameters)          â”‚
â”‚  B: 16Ã—4096 matrix (65K parameters)          â”‚
â”‚  Combined: 130K parameters (0.8% of original)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Output: WÂ·x + BÂ·AÂ·x                         â”‚
â”‚         â†‘      â†‘                             â”‚
â”‚      Original  LoRA (only this trains!)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>Memory Savings:</strong>
Full Fine-Tuning:  8B params Ã— 4 bytes = 32GB weights
                  + 32GB gradients + 64GB optimizer = 128GB total

LoRA Fine-Tuning:  8B params Ã— 1 byte (quantized) = 8GB weights
                  + 10M params Ã— 4 bytes = 40MB adapters
                  + 40MB gradients + 80MB optimizer = 8.16GB total

<strong>Result: 16x less memory!</strong>
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Unsloth + PEFT Example: Fine-Tune Llama-3-8B</strong>
                            <pre>from unsloth import FastLanguageModel
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

# Load model in 4-bit (2GB instead of 16GB!)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3-8b-bnb-4bit",  # Pre-quantized
    max_seq_length=2048,
    dtype=None,  # Auto-detect BF16/FP16
    load_in_4bit=True,
)

# Add LoRA adapters (only these will train)
model = FastLanguageModel.get_peft_model(
    model,
    r=16,               # Rank (higher = more capacity, slower)
    target_modules=[    # Which layers to adapt
        "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
        "gate_proj", "up_proj", "down_proj"      # FFN
    ],
    lora_alpha=16,      # Scaling factor
    lora_dropout=0.05,  # Prevent overfitting
    bias="none",
    use_gradient_checkpointing="unsloth",  # 2x memory savings
    random_state=42,
)

# Load training data
dataset = load_dataset("yahma/alpaca-cleaned", split="train")

# Train (Unsloth makes this 2-5x faster!)
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,  # Effective batch=8
        warmup_steps=10,
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        output_dir="outputs",
        num_train_epochs=3,
    ),
)

trainer.train()

# Save only the 100MB adapter (not the 16GB base model!)
model.save_pretrained("llama-3-8b-custom-adapter")

# To use: Load base model + merge adapter
# Total size: 16GB base + 100MB adapter = 16.1GB</pre>
                            <p style="margin-top: 10px;"><strong>Performance:</strong> On RTX 4090 (24GB VRAM), this trains in ~2 hours. Standard full fine-tuning would require 8Ã—A100 (640GB VRAM) and take 10+ hours!</p>
                        </div>

                        <div class="tools-box">
                            <strong>Fine-Tuning Methods Comparison:</strong>
                            <table>
                                <tr>
                                    <th>Method</th>
                                    <th>Memory (8B Model)</th>
                                    <th>Speed</th>
                                    <th>Quality</th>
                                    <th>Use Case</th>
                                </tr>
                                <tr>
                                    <td><strong>Full Fine-Tuning</strong></td>
                                    <td>120GB (8Ã—A100)</td>
                                    <td>1x (10 hrs)</td>
                                    <td>â­â­â­â­â­</td>
                                    <td>Research, unlimited budget</td>
                                </tr>
                                <tr>
                                    <td><strong>LoRA (HF PEFT)</strong></td>
                                    <td>24GB (1Ã—A100)</td>
                                    <td>3x (3 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td>Standard fine-tuning</td>
                                </tr>
                                <tr>
                                    <td><strong>QLoRA (4-bit)</strong></td>
                                    <td>12GB (RTX 4090)</td>
                                    <td>2x (5 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td>Consumer GPUs</td>
                                </tr>
                                <tr>
                                    <td><strong>Unsloth + LoRA</strong></td>
                                    <td>16GB (RTX 4080)</td>
                                    <td>5x (2 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td><strong>Best bang/buck!</strong></td>
                                </tr>
                                <tr>
                                    <td><strong>Unsloth + QLoRA</strong></td>
                                    <td>8GB (RTX 3090)</td>
                                    <td>4x (2.5 hrs)</td>
                                    <td>â­â­â­â­</td>
                                    <td>Low-VRAM GPUs</td>
                                </tr>
                            </table>
                        </div>

                        <div class="example-box">
                            <strong>Real-World Use Case: Custom Chatbot</strong>
                            <pre><strong>Goal:</strong> Fine-tune Llama-3-8B on 10K customer service conversations for your company.

<strong>Standard Approach (HuggingFace PEFT):</strong>
â€¢ Hardware: Rent 1Ã—A100 (40GB) on AWS = $3.60/hour
â€¢ Training time: 6 hours
â€¢ Total cost: $21.60
â€¢ Setup complexity: 30 minutes (install PyTorch, HF, PEFT)

<strong>Unsloth Approach:</strong>
â€¢ Hardware: Use your RTX 4090 (24GB) = FREE
â€¢ Training time: 2 hours (or rent A100 for $7.20)
â€¢ Setup complexity: 5 minutes (pip install unsloth)

<strong>Results (Identical Quality):</strong>
âœ“ Model responds with company-specific terminology
âœ“ Handles edge cases from your training data
âœ“ 95% accuracy on held-out test conversations

<strong>Saved: $14.40 + 4 hours of time!</strong></pre>
                        </div>

                        <div class="tools-box">
                            <strong>Unsloth Key Features:</strong>
                            <table>
                                <tr>
                                    <th>Feature</th>
                                    <th>Benefit</th>
                                    <th>Technical Detail</th>
                                </tr>
                                <tr>
                                    <td><strong>Custom CUDA Kernels</strong></td>
                                    <td>2-5x faster training</td>
                                    <td>Fused attention, RoPE, RMS norm operations</td>
                                </tr>
                                <tr>
                                    <td><strong>Auto-Quantization</strong></td>
                                    <td>8GB â†’ 2GB model size</td>
                                    <td>4-bit quantization with minimal quality loss</td>
                                </tr>
                                <tr>
                                    <td><strong>Pre-Patched Models</strong></td>
                                    <td>Zero setup time</td>
                                    <td>Download pre-quantized Llama/Mistral/Gemma</td>
                                </tr>
                                <tr>
                                    <td><strong>Memory Tricks</strong></td>
                                    <td>Fit 2x longer sequences</td>
                                    <td>Gradient checkpointing + paged attention</td>
                                </tr>
                                <tr>
                                    <td><strong>Multi-GPU Support</strong></td>
                                    <td>Scale to 70B models</td>
                                    <td>Automatic model parallelism</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>Unsloth & PEFT FAQs:</strong>
                            <ul>
                                <li><strong>Q: Is Unsloth better than HuggingFace PEFT?</strong><br>
                                    ğŸš€ <strong>Unsloth:</strong> Faster (2-5x), easier setup, pre-quantized models.<br>
                                    ğŸ—ï¸ <strong>HF PEFT:</strong> More control, better docs, official support.<br>
                                    ğŸ’¡ <strong>Reality:</strong> Use Unsloth for speed, switch to PEFT if you hit limitations.</li>
                                <li><strong>Q: Does LoRA hurt model quality?</strong><br>
                                    ğŸ“Š In practice, LoRA achieves 95-99% of full fine-tuning quality. For most tasks, you won't notice the difference.</li>
                                <li><strong>Q: Can I merge LoRA adapters into the base model?</strong><br>
                                    âœ… Yes! `model.merge_and_unload()` creates a single 16GB model file. Good for deployment.</li>
                                <li><strong>Q: What rank (r) should I use?</strong><br>
                                    ğŸ“ˆ <strong>r=8:</strong> Fastest, use for simple tasks (classification, short generation)<br>
                                    ğŸ“ˆ <strong>r=16:</strong> Balanced (most common choice)<br>
                                    ğŸ“ˆ <strong>r=64:</strong> Slower but higher capacity (complex reasoning, long generation)</li>
                                <li><strong>Q: Do I need Unsloth for 1B models?</strong><br>
                                    âš ï¸ No. Gemma-2B, Phi-3-mini fit in 16GB without tricks. Use Unsloth for 7B+ models.</li>
                                <li><strong>Q: Can I fine-tune on CPU?</strong><br>
                                    ğŸŒ Yes (with PEFT), but it's 50x slower. A 2-hour GPU job becomes 4 days on CPU. Just use Google Colab free GPU.</li>
                            </ul>
                        </div>

                        <div class="mental-model">
                            <strong>When to Use Unsloth vs PEFT vs Full Fine-Tuning:</strong>
                            <ul>
                                <li>ğŸ† <strong>Unsloth + QLoRA:</strong> 95% of fine-tuning tasks. Fast, cheap, good enough.</li>
                                <li>âš™ï¸ <strong>HF PEFT:</strong> Need specific features (DoRA, AdaLoRA), production stability.</li>
                                <li>ğŸ”¬ <strong>Full Fine-Tuning:</strong> Research papers, maximum quality, unlimited budget.</li>
                                <li>âŒ <strong>Prompt Engineering:</strong> Try this FIRST before fine-tuning. 10 minutes vs 2 hours.</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Getting Started (10-Minute Quickstart):</strong>
                            <pre><strong>Step 1: Install Unsloth (30 seconds)</strong>
pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

<strong>Step 2: Load Pre-Quantized Model (2 minutes)</strong>
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    "unsloth/llama-3-8b-bnb-4bit",  # 2GB download
    max_seq_length=2048,
    load_in_4bit=True
)

<strong>Step 3: Add LoRA (10 seconds)</strong>
model = FastLanguageModel.get_peft_model(model, r=16)

<strong>Step 4: Train on Your Data (2 hours)</strong>
trainer = SFTTrainer(model=model, dataset=your_data)
trainer.train()

<strong>Step 5: Save Adapter (100MB file)</strong>
model.save_pretrained("my-custom-llama")

<strong>Done! Your fine-tuned model is ready.</strong></pre>
                        </div>

                        <div class="must-know">
                            <strong>ğŸ¯ Bottom Line:</strong> Unsloth + PEFT democratized AI fine-tuning. What required $50K cloud bills now runs on a gaming PC. <strong>If you're fine-tuning models, you're using these tools.</strong> They're the difference between "only Google can do this" and "I did this on my laptop."
                        </div>
                    </div>

                    <div class="concept-card" id="subsection-llamaindex">
                        <h3>ğŸ”® 7.5.4 Deep Dive: LlamaIndex (Coming Soon)</h3>
                        
                        <div class="must-know">
                            <strong>What is LlamaIndex?</strong> The leading framework for building RAG (Retrieval-Augmented Generation) applications. While LangChain does everything, LlamaIndex specializes in making your documents searchable and queryable by LLMs.
                        </div>

                        <div class="mental-model">
                            <strong>LlamaIndex vs LangChain:</strong>
                            <ul>
                                <li><strong>LlamaIndex:</strong> "Google for your documents" - Best for RAG, simpler API, data-centric</li>
                                <li><strong>LangChain:</strong> "Swiss Army knife" - Agents, chains, tools, complex workflows</li>
                                <li><strong>Reality:</strong> Many teams use BOTH. LlamaIndex for document indexing, LangChain for agent orchestration.</li>
                            </ul>
                        </div>

                        <div class="example-box">
                            <strong>Sneak Peek: LlamaIndex in 5 Lines</strong>
                            <pre>from llama_index import VectorStoreIndex, SimpleDirectoryReader

# Index all documents in a folder
documents = SimpleDirectoryReader('docs/').load_data()
index = VectorStoreIndex.from_documents(documents)

# Query your documents
query_engine = index.as_query_engine()
response = query_engine.query("What are the main findings?")
print(response)

# That's it! LlamaIndex handles chunking, embedding, retrieval, generation.</pre>
                        </div>

                        <div class="mental-model">
                            <strong>ğŸ“š Comprehensive Deep Dive Coming:</strong>
                            <p>The full LlamaIndex section will include:</p>
                            <ul>
                                <li>âœ… 1000+ word detailed explanation</li>
                                <li>âœ… Interactive animation showing document indexing pipeline</li>
                                <li>âœ… Comparison tables: LlamaIndex vs LangChain vs raw APIs</li>
                                <li>âœ… Real-world use cases with code examples</li>
                                <li>âœ… Advanced features: Multi-document agents, hybrid search, metadata filtering</li>
                                <li>âœ… FAQ section covering common questions</li>
                            </ul>
                            <p style="margin-top: 10px;"><em>This section is reserved for future expansion. Stay tuned!</em></p>
                        </div>
                    </div>

                    <div class="must-know">
                        <strong>ğŸš€ Action Plan:</strong>
                        <ol>
                            <li><strong>Week 1:</strong> Master prompting. Use ChatGPT/Claude daily.</li>
                            <li><strong>Week 2-3:</strong> Build a RAG app with your own documents.</li>
                            <li><strong>Week 4:</strong> Integrate OpenAI <span class="tooltip">API<span class="tooltiptext">Application Programming Interface</span></span> into a real project.</li>
                            <li><strong>Week 5-6:</strong> Build 1 simple agent (e.g., automated customer support).</li>
                            <li><strong>After:</strong> Go deeper based on your role (see matrix above).</li>
                        </ol>
                        <p style="margin-top: 10px;"><strong>Reality:</strong> With these 6 weeks, you'll know more than 95% of people talking about <span class="tooltip">AI<span class="tooltiptext">Artificial Intelligence</span></span>.</p>
                    </div>
                </div>

                <!-- SECTION 8: GLOSSARY -->
                <div id="glossary" class="section">
                    <h2>ğŸ“š 8. Glossary, FAQs & Acronyms</h2>
                    
                    <div class="concept-card">
                        <h3>Complete Acronyms & Abbreviations</h3>
                        
                        <table>
                            <tr>
                                <th>Acronym</th>
                                <th>Full Form</th>
                                <th>Description</th>
                            </tr>
                            <tr>
                                <td><strong>AI</strong></td>
                                <td>Artificial Intelligence</td>
                                <td>Machines that mimic human intelligence</td>
                            </tr>
                            <tr>
                                <td><strong>AGI</strong></td>
                                <td>Artificial General Intelligence</td>
                                <td>AI with human-level general intelligence (not yet achieved)</td>
                            </tr>
                            <tr>
                                <td><strong>API</strong></td>
                                <td>Application Programming Interface</td>
                                <td>Interface for software to communicate</td>
                            </tr>
                            <tr>
                                <td><strong>AWQ</strong></td>
                                <td>Activation-aware Weight Quantization</td>
                                <td>4-bit quantization method for GPUs</td>
                            </tr>
                            <tr>
                                <td><strong>BERT</strong></td>
                                <td>Bidirectional Encoder Representations from Transformers</td>
                                <td>Google's transformer model for understanding</td>
                            </tr>
                            <tr>
                                <td><strong>BF16</strong></td>
                                <td>bfloat16</td>
                                <td>16-bit floating point format (common in training)</td>
                            </tr>
                            <tr>
                                <td><strong>BPE</strong></td>
                                <td>Byte Pair Encoding</td>
                                <td>Tokenization algorithm used in GPT models</td>
                            </tr>
                            <tr>
                                <td><strong>CNN</strong></td>
                                <td>Convolutional Neural Network</td>
                                <td>Neural network for image processing</td>
                            </tr>
                            <tr>
                                <td><strong>CUDA</strong></td>
                                <td>Compute Unified Device Architecture</td>
                                <td>NVIDIA's GPU programming platform</td>
                            </tr>
                            <tr>
                                <td><strong>DPO</strong></td>
                                <td>Direct Preference Optimization</td>
                                <td>Alternative to RLHF for alignment</td>
                            </tr>
                            <tr>
                                <td><strong>FFN</strong></td>
                                <td>Feed-Forward Network</td>
                                <td>Component in transformer layers</td>
                            </tr>
                            <tr>
                                <td><strong>FP16/FP32</strong></td>
                                <td>Floating Point 16/32-bit</td>
                                <td>Number precision formats</td>
                            </tr>
                            <tr>
                                <td><strong>FPFT</strong></td>
                                <td>Full Parameter Fine-Tuning</td>
                                <td>Update all model weights during training</td>
                            </tr>
                            <tr>
                                <td><strong>GGML</strong></td>
                                <td>(no standard expansion)</td>
                                <td>Early llama.cpp ecosystem format/tooling for CPU inference (predecessor family to GGUF)</td>
                            </tr>
                            <tr>
                                <td><strong>GGUF</strong></td>
                                <td>(no standard expansion)</td>
                                <td>llama.cpp unified model file format (often used for quantized inference)</td>
                            </tr>
                            <tr>
                                <td><strong>GPT</strong></td>
                                <td>Generative Pre-trained Transformer</td>
                                <td>OpenAI's autoregressive language models</td>
                            </tr>
                            <tr>
                                <td><strong>GPTQ</strong></td>
                                <td>GPT Quantization</td>
                                <td>4-bit quantization for GPUs</td>
                            </tr>
                            <tr>
                                <td><strong>GPU</strong></td>
                                <td>Graphics Processing Unit</td>
                                <td>Parallel processor for AI workloads</td>
                            </tr>
                            <tr>
                                <td><strong>HF</strong></td>
                                <td>HuggingFace</td>
                                <td>AI community & model hub</td>
                            </tr>
                            <tr>
                                <td><strong>INT4/INT8</strong></td>
                                <td>Integer 4/8-bit</td>
                                <td>Quantized integer formats</td>
                            </tr>
                            <tr>
                                <td><strong>LLM</strong></td>
                                <td>Large Language Model</td>
                                <td>AI models trained on vast text (GPT, Llama)</td>
                            </tr>
                            <tr>
                                <td><strong>LoRA</strong></td>
                                <td>Low-Rank Adaptation</td>
                                <td>Efficient fine-tuning method</td>
                            </tr>
                            <tr>
                                <td><strong>LSTM</strong></td>
                                <td>Long Short-Term Memory</td>
                                <td>Old RNN architecture (pre-transformer)</td>
                            </tr>
                            <tr>
                                <td><strong>MCP</strong></td>
                                <td>Model Context Protocol</td>
                                <td>Standard for connecting models to tools/data</td>
                            </tr>
                            <tr>
                                <td><strong>ML</strong></td>
                                <td>Machine Learning</td>
                                <td>Algorithms that learn from data</td>
                            </tr>
                            <tr>
                                <td><strong>MoE</strong></td>
                                <td>Mixture of Experts</td>
                                <td>Architecture with multiple specialized sub-models</td>
                            </tr>
                            <tr>
                                <td><strong>NF4</strong></td>
                                <td>Normal Float 4-bit</td>
                                <td>Quantization format used in QLoRA</td>
                            </tr>
                            <tr>
                                <td><strong>NLP</strong></td>
                                <td>Natural Language Processing</td>
                                <td>AI for understanding human language</td>
                            </tr>
                            <tr>
                                <td><strong>ONNX</strong></td>
                                <td>Open Neural Network Exchange</td>
                                <td>Cross-framework model format</td>
                            </tr>
                            <tr>
                                <td><strong>PEFT</strong></td>
                                <td>Parameter-Efficient Fine-Tuning</td>
                                <td>Methods like LoRA that update few parameters</td>
                            </tr>
                            <tr>
                                <td><strong>PPO</strong></td>
                                <td>Proximal Policy Optimization</td>
                                <td>RL algorithm used in RLHF</td>
                            </tr>
                            <tr>
                                <td><strong>QLoRA</strong></td>
                                <td>Quantized LoRA</td>
                                <td>LoRA on 4-bit quantized models</td>
                            </tr>
                            <tr>
                                <td><strong>RAG</strong></td>
                                <td>Retrieval-Augmented Generation</td>
                                <td>LLM + external knowledge retrieval</td>
                            </tr>
                            <tr>
                                <td><strong>ReAct</strong></td>
                                <td>Reasoning and Acting</td>
                                <td>Agent framework combining thought and action</td>
                            </tr>
                            <tr>
                                <td><strong>RL</strong></td>
                                <td>Reinforcement Learning</td>
                                <td>Learning through rewards/penalties</td>
                            </tr>
                            <tr>
                                <td><strong>RLHF</strong></td>
                                <td>Reinforcement Learning from Human Feedback</td>
                                <td>Training to align with human preferences</td>
                            </tr>
                            <tr>
                                <td><strong>RNN</strong></td>
                                <td>Recurrent Neural Network</td>
                                <td>Old sequential model (pre-transformer)</td>
                            </tr>
                            <tr>
                                <td><strong>ROCm</strong></td>
                                <td>Radeon Open Compute</td>
                                <td>AMD's alternative to CUDA</td>
                            </tr>
                            <tr>
                                <td><strong>SFT</strong></td>
                                <td>Supervised Fine-Tuning</td>
                                <td>Training on labeled examples</td>
                            </tr>
                            <tr>
                                <td><strong>SLM</strong></td>
                                <td>Small Language Model</td>
                                <td>Compact models (<7B parameters)</td>
                            </tr>
                            <tr>
                                <td><strong>TensorRT</strong></td>
                                <td>Tensor Runtime</td>
                                <td>NVIDIA's inference optimizer</td>
                            </tr>
                            <tr>
                                <td><strong>TGI</strong></td>
                                <td>Text Generation Inference</td>
                                <td>HuggingFace's production serving</td>
                            </tr>
            
                            <tr>
                                <td><strong>VRAM</strong></td>
                                <td>Video Random Access Memory</td>
                                <td>GPU memory</td>
                            </tr>
                        </table>
                    </div>

                    <div class="concept-card">
                        <h3>Frequently Asked Questions</h3>
                        
                        <div class="faq-box">
                            <strong>Q: What's the difference between each model if they're all trained on the same internet data?</strong><br><br>
                            A: Great question! While many models use similar training data, they differ in:
                            <ul>
                                <li><strong>Architecture:</strong> Different layer counts, attention mechanisms (MoE, GQA), context windows</li>
                                <li><strong>Scale:</strong> Parameter count (7B vs 70B vs 175B) dramatically affects capability</li>
                                <li><strong>Training Recipes:</strong> Different hyperparameters, training techniques, compute budgets</li>
                                <li><strong>Fine-tuning:</strong> RLHF with different human feedback, instruction-tuning datasets</li>
                                <li><strong>Data Mix:</strong> While similar sources, different proportions (code vs text), filtering, deduplication</li>
                                <li><strong>Optimization Goals:</strong> Some optimize for speed, others for quality, cost, or specific tasks</li>
                            </ul>
                            <p><strong>Example:</strong> Llama 2 vs GPT-4 - both trained on internet data, but GPT-4 is much larger, uses proprietary techniques, extensive RLHF, and OpenAI's curated dataset mix.</p>
                        </div>

                        <div class="faq-box">
                            <strong>Q: How are proprietary models distributed vs open-source models? How to install and what tools to use?</strong><br><br>
                            A: <strong>Proprietary Models (GPT-4, Claude, Gemini):</strong>
                            <ul>
                                <li><strong>Distribution:</strong> API-only, no weights available</li>
                                <li><strong>Access:</strong> Sign up â†’ Get API key â†’ Pay per token</li>
                                <li><strong>Installation:</strong>
                                    <pre>
# OpenAI
pip install openai
from openai import OpenAI
client = OpenAI(api_key="sk-...")

# Anthropic (Claude)
pip install anthropic
from anthropic import Anthropic
client = Anthropic(api_key="...")
</pre>
                                </li>
                            </ul>

                            <strong>Open-Source Models (Llama, Mistral, Falcon):</strong>
                            <ul>
                                <li><strong>Distribution:</strong> Weights downloadable from HuggingFace, GitHub</li>
                                <li><strong>Access:</strong> Free download, run locally or on your servers</li>
                                <li><strong>Installation:</strong>
                                    <pre>
# Method 1: HuggingFace
pip install transformers
from transformers import AutoModel
model = AutoModel.from_pretrained("meta-llama/Llama-2-7b")

# Method 2: Ollama (easiest)
curl https://ollama.ai/install.sh | sh
ollama pull llama2
ollama run llama2

# Method 3: llama.cpp (CPU-optimized)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make
./main -m model.gguf -p "Hello"

# Method 4: LM Studio (GUI)
Download from lmstudio.ai â†’ Install â†’ Download models from GUI
</pre>
                                </li>
                            </ul>

                            <strong>Comparison:</strong>
                            <table>
                                <tr>
                                    <th>Aspect</th>
                                    <th>Proprietary</th>
                                    <th>Open Source</th>
                                </tr>
                                <tr>
                                    <td><strong>Cost</strong></td>
                                    <td>Pay per use ($0.001-0.06/1K tokens)</td>
                                    <td>Free (just hardware)</td>
                                </tr>
                                <tr>
                                    <td><strong>Privacy</strong></td>
                                    <td>Data sent to provider</td>
                                    <td>Runs locally, fully private</td>
                                </tr>
                                <tr>
                                    <td><strong>Customization</strong></td>
                                    <td>Limited (API fine-tuning)</td>
                                    <td>Full control (modify weights)</td>
                                </tr>
                                <tr>
                                    <td><strong>Quality</strong></td>
                                    <td>Often higher (GPT-4, Claude)</td>
                                    <td>Catching up fast (Llama 3, Mixtral)</td>
                                </tr>
                            </table>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Can I train my own LLM from scratch?</strong><br><br>
                            A: Technically yes, but practically challenging:
                            <ul>
                                <li><strong>Cost:</strong> $2-10 million for decent model (7B-70B params)</li>
                                <li><strong>Hardware:</strong> Need 100-1000+ GPUs for weeks</li>
                                <li><strong>Data:</strong> Hundreds of billions of tokens, curated and filtered</li>
                                <li><strong>Expertise:</strong> Distributed training, hyperparameter tuning, infrastructure</li>
                            </ul>
                            <p><strong>Better Alternatives:</strong></p>
                            <ul>
                                <li><strong>Fine-tune existing models</strong> - Much cheaper ($100-1000)</li>
                                <li><strong>Use open-source models</strong> - Free, proven quality</li>
                                <li><strong>RAG</strong> - Add your knowledge without training</li>
                            </ul>
                        </div>

                        <div class="faq-box">
                            <strong>Q: What hardware do I need for AI development?</strong><br><br>
                            A: Depends on what you're doing:
                            
                            <table>
                                <tr>
                                    <th>Task</th>
                                    <th>Minimum</th>
                                    <th>Recommended</th>
                                    <th>Professional</th>
                                </tr>
                                <tr>
                                    <td><strong>Learning / API Use</strong></td>
                                    <td>Any laptop</td>
                                    <td>-</td>
                                    <td>-</td>
                                </tr>
                                <tr>
                                    <td><strong>CPU Inference</strong></td>
                                    <td>16GB RAM</td>
                                    <td>32GB RAM</td>
                                    <td>64GB+ RAM</td>
                                </tr>
                                <tr>
                                    <td><strong>GPU Inference (7B)</strong></td>
                                    <td>RTX 3060 (12GB)</td>
                                    <td>RTX 4070 (16GB)</td>
                                    <td>RTX 4090 (24GB)</td>
                                </tr>
                                <tr>
                                    <td><strong>Fine-tuning (7B)</strong></td>
                                    <td>RTX 4060 Ti (16GB) + QLoRA</td>
                                    <td>RTX 4090 (24GB)</td>
                                    <td>A100 (40-80GB)</td>
                                </tr>
                                <tr>
                                    <td><strong>Training from Scratch</strong></td>
                                    <td>-</td>
                                    <td>-</td>
                                    <td>100+ A100s (cloud)</td>
                                </tr>
                            </table>

                            <p><strong>Budget Recommendations:</strong></p>
                            <ul>
                                <li><strong>$0-500:</strong> Use cloud GPUs (Colab Pro, Lambda Labs) or CPU inference</li>
                                <li><strong>$500-1500:</strong> RTX 4060 Ti 16GB - Good for inference + QLoRA</li>
                                <li><strong>$1500-2500:</strong> RTX 4090 24GB - Best consumer GPU for AI</li>
                                <li><strong>$2500+:</strong> Used A40/A100 or rent cloud GPUs</li>
                            </ul>
                        </div>

                        <div class="faq-box">
                            <strong>Q: How do I stay current with rapidly evolving AI?</strong><br><br>
                            A: The field moves fast! Here's how to keep up:
                            <ul>
                                <li><strong>Daily:</strong> Twitter/X - Follow key researchers (@karpathy, @ylecun, @AndrewYNg)</li>
                                <li><strong>Weekly:</strong> 
                                    <ul>
                                        <li>HuggingFace Papers (papers.huggingface.co)</li>
                                        <li>r/MachineLearning subreddit</li>
                                        <li>AI newsletters (The Batch, TLDR AI)</li>
                                    </ul>
                                </li>
                                <li><strong>Monthly:</strong> 
                                    <ul>
                                        <li>Major conferences: NeurIPS, ICML, ACL</li>
                                        <li>Company blogs: OpenAI, Anthropic, Google AI</li>
                                    </ul>
                                </li>
                                <li><strong>Hands-on:</strong> Build projects, experiment with new models/techniques immediately</li>
                            </ul>
                        </div>
                    </div>
                </div>

                <!-- SECTION 9: ECONOMICS OF AI -->
                <div id="economics" class="section">
                    <h2>ğŸ’° 9. The Economics of AI</h2>
                    
                    <div class="must-know">
                        <strong>The AI Economy:</strong> Understanding AI isn't complete without grasping the massive economic forces at play. Hyperscalers invest billions in infrastructure, while pricing models and market dynamics shape how AI reaches users. This section covers the financial realities of AI deployment and the economic tradeoffs behind every decision.
                    </div>

                    <div class="concept-card">
                        <h3>9.1 Hyperscaler Spending: The Infrastructure Race</h3>
                        
                        <div class="mental-model">
                            <strong>The Stakes:</strong> AI companies are in an unprecedented infrastructure arms race, spending tens of billions annually on GPUs, data centers, and compute capacity.
                        </div>

                        <div class="diagram">
<pre>
<strong>AI Infrastructure Spending (2024-2025):</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Microsoft   â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  $50B+/year      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Google      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    $45B+/year      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Amazon      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      $40B+/year      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Meta        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          $35B+/year      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  OpenAI      â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  $15B+/year      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Anthropic   â”‚â–ˆâ–ˆâ–ˆâ–ˆ                      $8B+/year       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>Total Industry Spending: ~$200B+/year (2025 est.)</strong>
</pre>
                        </div>

                        <h4>Where the Money Goes</h4>
                        <table>
                            <tr>
                                <th>Category</th>
                                <th>% of Budget</th>
                                <th>Examples</th>
                                <th>Annual Cost</th>
                            </tr>
                            <tr>
                                <td><strong>GPU Hardware</strong></td>
                                <td>50-60%</td>
                                <td>H100s ($30K-40K each), B200s</td>
                                <td>$25-30B per company</td>
                            </tr>
                            <tr>
                                <td><strong>Data Centers</strong></td>
                                <td>15-20%</td>
                                <td>Power, cooling, real estate</td>
                                <td>$7-10B per company</td>
                            </tr>
                            <tr>
                                <td><strong>Networking</strong></td>
                                <td>10-15%</td>
                                <td>InfiniBand, high-speed interconnects</td>
                                <td>$5-7B per company</td>
                            </tr>
                            <tr>
                                <td><strong>Talent</strong></td>
                                <td>8-12%</td>
                                <td>Engineers, researchers ($500K+ salaries)</td>
                                <td>$4-6B per company</td>
                            </tr>
                            <tr>
                                <td><strong>Training Data</strong></td>
                                <td>3-5%</td>
                                <td>Licensing, curation, synthetic data</td>
                                <td>$1-2B per company</td>
                            </tr>
                            <tr>
                                <td><strong>Energy</strong></td>
                                <td>5-8%</td>
                                <td>Electricity ($0.10-0.30/kWh)</td>
                                <td>$2-4B per company</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Real-World Example: Training GPT-4</strong>
                            <pre><strong>Estimated Costs (GPT-4 Training):</strong>

GPU Cluster:        25,000 Ã— A100 80GB = $250M hardware
Training Time:      90-120 days @ full capacity
Energy Cost:        ~50 MW Ã— 24/7 Ã— 100 days Ã— $0.15/kWh = $18M
Talent:             300 engineers Ã— $500K/year Ã— 0.3 years = $45M
Data Licensing:     $5-10M
Infrastructure:     $20M (networking, storage)

<strong>Total One-Time Training Cost: ~$350-400M</strong>
<strong>Ongoing Inference Cost: $500M-1B/year</strong>

<em>Note: Inference costs often exceed training costs over time!</em></pre>
                        </div>

                        <div class="faq-box">
                            <strong>Q: Why are hyperscalers spending so much?</strong><br>
                            A: Three strategic reasons:
                            <ul>
                                <li><strong>First-Mover Advantage:</strong> Best models attract users, developers, ecosystem lock-in</li>
                                <li><strong>Cloud Revenue:</strong> AI workloads drive cloud computing growth (Azure, AWS, GCP)</li>
                                <li><strong>Future-Proofing:</strong> AI is seen as the next computing platform shift (like mobile, cloud)</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>9.2 Model Distribution & Pricing Models</h3>
                        
                        <div class="must-know">
                            <strong>Two Paths:</strong> Hyperscalers distribute AI models through (1) <strong>API access</strong> with usage-based pricing or (2) <strong>open-source releases</strong> where users run models themselves.
                        </div>

                        <h4>Distribution Strategies</h4>
                        <table>
                            <tr>
                                <th>Company</th>
                                <th>Strategy</th>
                                <th>Model Access</th>
                                <th>Pricing Model</th>
                            </tr>
                            <tr>
                                <td><strong>OpenAI</strong></td>
                                <td>Closed API</td>
                                <td>GPT-4, GPT-4o, o1</td>
                                <td>Pay-per-token</td>
                            </tr>
                            <tr>
                                <td><strong>Anthropic</strong></td>
                                <td>Closed API</td>
                                <td>Claude 3.5 Sonnet, Opus</td>
                                <td>Pay-per-token</td>
                            </tr>
                            <tr>
                                <td><strong>Google</strong></td>
                                <td>Hybrid</td>
                                <td>Gemini Pro (API), Gemma (open)</td>
                                <td>API: Pay-per-token<br>Open: Free</td>
                            </tr>
                            <tr>
                                <td><strong>Meta</strong></td>
                                <td>Open Source</td>
                                <td>Llama 2, Llama 3 (weights)</td>
                                <td>Free (self-hosted costs)</td>
                            </tr>
                            <tr>
                                <td><strong>Microsoft</strong></td>
                                <td>Hybrid</td>
                                <td>Azure OpenAI, Phi models</td>
                                <td>API: Pay-per-token<br>Phi: Free</td>
                            </tr>
                            <tr>
                                <td><strong>Mistral AI</strong></td>
                                <td>Hybrid</td>
                                <td>API + open weights</td>
                                <td>API + Self-host</td>
                            </tr>
                        </table>

                        <h4>API Pricing Comparison (per 1M tokens)</h4>
                        <table>
                            <tr>
                                <th>Model</th>
                                <th>Input Tokens</th>
                                <th>Output Tokens</th>
                                <th>Context Window</th>
                            </tr>
                            <tr>
                                <td><strong>GPT-4o</strong></td>
                                <td>$5.00</td>
                                <td>$15.00</td>
                                <td>128K</td>
                            </tr>
                            <tr>
                                <td><strong>GPT-4o mini</strong></td>
                                <td>$0.15</td>
                                <td>$0.60</td>
                                <td>128K</td>
                            </tr>
                            <tr>
                                <td><strong>Claude 3.5 Sonnet</strong></td>
                                <td>$3.00</td>
                                <td>$15.00</td>
                                <td>200K</td>
                            </tr>
                            <tr>
                                <td><strong>Claude 3 Haiku</strong></td>
                                <td>$0.25</td>
                                <td>$1.25</td>
                                <td>200K</td>
                            </tr>
                            <tr>
                                <td><strong>Gemini 1.5 Pro</strong></td>
                                <td>$3.50</td>
                                <td>$10.50</td>
                                <td>1M</td>
                            </tr>
                            <tr>
                                <td><strong>Gemini 1.5 Flash</strong></td>
                                <td>$0.075</td>
                                <td>$0.30</td>
                                <td>1M</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Cost Analysis: Chatbot with 10K Daily Users</strong>
                            <pre><strong>Scenario: Customer service chatbot</strong>

Users:              10,000 conversations/day
Avg tokens/conv:    2,000 input + 1,000 output = 3,000 total
Total tokens/day:   30M tokens

<strong>Monthly Costs (30 days):</strong>

GPT-4o:            (60M Ã— $5 + 30M Ã— $15) / 1M = $750/month
GPT-4o mini:       (60M Ã— $0.15 + 30M Ã— $0.60) / 1M = $27/month
Claude 3.5 Sonnet: (60M Ã— $3 + 30M Ã— $15) / 1M = $630/month
Gemini 1.5 Flash:  (60M Ã— $0.075 + 30M Ã— $0.30) / 1M = $13.50/month

<strong>Annual Costs:</strong>
GPT-4o:            $9,000/year
GPT-4o mini:       $324/year
Claude 3.5 Sonnet: $7,560/year
Gemini 1.5 Flash:  $162/year

<strong>Self-Hosted Llama 3 70B:</strong>
Server rental:     $1,500-2,500/month = $18-30K/year
But: Unlimited usage, no per-token costs!

<em>Break-even point: ~3-4 months for high-volume use cases</em></pre>
                        </div>

                        <div class="mental-model">
                            <strong>Pricing Strategy Tradeoffs:</strong>
                            <ul>
                                <li><strong>API (Closed):</strong> Recurring revenue, control over models, easier for users, but expensive at scale</li>
                                <li><strong>Open Source:</strong> No direct revenue, but builds ecosystem, drives cloud sales, attracts developers</li>
                                <li><strong>Hybrid:</strong> Offer both - premium API for convenience, open models for customization</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>9.3 The Path to Profitability: Revenue Models</h3>
                        
                        <div class="must-know">
                            <strong>The Challenge:</strong> Companies spend $10-50B/year on AI but need sustainable revenue to justify investments. How do they recoup these costs?
                        </div>

                        <h4>Revenue Streams by Company</h4>
                        <table>
                            <tr>
                                <th>Company</th>
                                <th>Primary Revenue</th>
                                <th>AI Monetization</th>
                                <th>2025 Est. AI Revenue</th>
                            </tr>
                            <tr>
                                <td><strong>OpenAI</strong></td>
                                <td>API subscriptions</td>
                                <td>ChatGPT Plus ($20/mo), Enterprise, API</td>
                                <td>$3-5B/year</td>
                            </tr>
                            <tr>
                                <td><strong>Microsoft</strong></td>
                                <td>Cloud (Azure)</td>
                                <td>Azure OpenAI, Copilot ($30/user/mo)</td>
                                <td>$15-20B/year</td>
                            </tr>
                            <tr>
                                <td><strong>Google</strong></td>
                                <td>Ads, Cloud</td>
                                <td>Gemini Advanced, Workspace, GCP</td>
                                <td>$10-15B/year</td>
                            </tr>
                            <tr>
                                <td><strong>Amazon</strong></td>
                                <td>AWS</td>
                                <td>Bedrock (managed AI service), Trainium chips</td>
                                <td>$5-8B/year</td>
                            </tr>
                            <tr>
                                <td><strong>Anthropic</strong></td>
                                <td>API</td>
                                <td>Claude API, Enterprise contracts</td>
                                <td>$1-2B/year</td>
                            </tr>
                            <tr>
                                <td><strong>Meta</strong></td>
                                <td>Ads</td>
                                <td>Indirect: Better ads via AI, cloud services</td>
                                <td>$5-10B boost to ads</td>
                            </tr>
                        </table>

                        <div class="diagram">
<pre>
<strong>Profitability Timeline:</strong>

2023-2024: Investment Phase
â”‚
â”‚  Spending >> Revenue
â”‚  Focus: Model quality, user acquisition
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
2025-2026: Revenue Ramp
â”‚
â”‚  Revenue growing, but still < spending
â”‚  Focus: Enterprise adoption, new use cases
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
2027-2028: Break-Even Target
â”‚
â”‚  Revenue â‰ˆ Spending (for some players)
â”‚  Focus: Efficiency, margin improvement
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚
2029+: Profitability
â”‚
â”‚  Revenue > Spending
â”‚  Focus: Scale, defending market position
â”‚
â–¼
</pre>
                        </div>

                        <div class="example-box">
                            <strong>Case Study: OpenAI's Economics</strong>
                            <pre><strong>OpenAI Revenue Model (2025 estimate):</strong>

ChatGPT Plus:       10M users Ã— $20/mo Ã— 12 = $2.4B/year
ChatGPT Enterprise: 50K seats Ã— $60/mo Ã— 12 = $36M/year
API Revenue:        $1.5-2B/year
Microsoft Deal:     ~$500M-1B/year (revenue share)

<strong>Total Revenue: ~$4-5B/year</strong>

<strong>Estimated Costs:</strong>
Inference (serving): $2-3B/year
Training new models: $500M-1B/year
Infrastructure:      $1-1.5B/year
Talent:              $500M-1B/year

<strong>Total Costs: ~$4.5-6.5B/year</strong>

<strong>Net: Break-even to slight loss in 2025</strong>
<strong>Path to profit: Scale users, reduce inference costs 30-50%</strong></pre>
                        </div>

                        <h4>Keys to Profitability</h4>
                        <ul>
                            <li><strong>Inference Efficiency:</strong> Every 2x speedup = 50% cost reduction</li>
                            <li><strong>Model Distillation:</strong> Smaller models (GPT-4o mini, Claude Haiku) offer 10-20x better margins</li>
                            <li><strong>Enterprise Focus:</strong> $60-100/seat/month vs $20/month consumer plans</li>
                            <li><strong>Cloud Integration:</strong> Drive usage of Azure/GCP/AWS (higher-margin services)</li>
                            <li><strong>Hardware Innovation:</strong> Custom chips (TPUs, Trainium) reduce reliance on NVIDIA</li>
                        </ul>
                    </div>

                    <div class="concept-card">
                        <h3>9.4 Macro Economics: The AI Value Chain</h3>
                        
                        <div class="mental-model">
                            <strong>The AI Stack:</strong> Value flows through multiple layers, from hardware to end applications. Understanding who captures value at each layer reveals market dynamics.
                        </div>

                        <div class="diagram">
<pre>
<strong>AI Value Chain (Bottom-Up):</strong>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 7: End-User Applications                        â”‚
â”‚  â€¢ ChatGPT, Copilot, Perplexity, Jasper               â”‚
â”‚  Value: $50-200B market (2025)                         â”‚
â”‚  Margin: 40-60% (subscription SaaS)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ pays usage fees, subscriptions
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 6: Model Providers                              â”‚
â”‚  â€¢ OpenAI, Anthropic, Mistral AI                       â”‚
â”‚  Value: $10-30B market                                 â”‚
â”‚  Margin: 10-30% (high inference costs)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ pays cloud compute, API fees
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 5: Cloud Infrastructure (Hyperscalers)          â”‚
â”‚  â€¢ AWS, Azure, GCP                                     â”‚
â”‚  Value: $100-150B AI workload market                   â”‚
â”‚  Margin: 30-40%                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ buys GPUs, data center capacity
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: Hardware (GPUs, ASICs)                       â”‚
â”‚  â€¢ NVIDIA (H100, B200), AMD, Google TPUs               â”‚
â”‚  Value: $80-120B market (AI GPUs)                      â”‚
â”‚  Margin: 60-70% (NVIDIA dominance)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ uses fabrication
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: Semiconductor Fabs                           â”‚
â”‚  â€¢ TSMC, Samsung                                       â”‚
â”‚  Value: $40-60B AI chip manufacturing                  â”‚
â”‚  Margin: 40-50%                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ uses equipment
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: Semiconductor Equipment                      â”‚
â”‚  â€¢ ASML (EUV lithography), Applied Materials           â”‚
â”‚  Value: $20-30B market                                 â”‚
â”‚  Margin: 45-55%                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘ requires raw materials, R&D
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: Foundational (Energy, Networking, Data)      â”‚
â”‚  â€¢ Utilities, Networking (Arista), Data providers      â”‚
â”‚  Value: $30-50B market                                 â”‚
â”‚  Margin: 15-30%                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<strong>Key Insight: Hardware captures most value (NVIDIA), </strong>
<strong>            Applications have highest margins but smaller market</strong>
</pre>
                        </div>

                        <h4>Market Cap Winners (AI Exposure)</h4>
                        <table>
                            <tr>
                                <th>Company</th>
                                <th>Layer</th>
                                <th>2025 Market Cap</th>
                                <th>AI Revenue Growth</th>
                            </tr>
                            <tr>
                                <td><strong>NVIDIA</strong></td>
                                <td>Hardware</td>
                                <td>$2-3T</td>
                                <td>200-300%/year</td>
                            </tr>
                            <tr>
                                <td><strong>Microsoft</strong></td>
                                <td>Cloud + Apps</td>
                                <td>$3T+</td>
                                <td>50-80%/year</td>
                            </tr>
                            <tr>
                                <td><strong>Google</strong></td>
                                <td>Cloud + Models</td>
                                <td>$2T+</td>
                                <td>40-60%/year</td>
                            </tr>
                            <tr>
                                <td><strong>Amazon</strong></td>
                                <td>Cloud (AWS)</td>
                                <td>$1.5-2T</td>
                                <td>40-60%/year</td>
                            </tr>
                            <tr>
                                <td><strong>Meta</strong></td>
                                <td>Apps + Infrastructure</td>
                                <td>$1-1.5T</td>
                                <td>30-50%/year</td>
                            </tr>
                            <tr>
                                <td><strong>TSMC</strong></td>
                                <td>Fabrication</td>
                                <td>$600-800B</td>
                                <td>40-60%/year</td>
                            </tr>
                        </table>

                        <div class="faq-box">
                            <strong>Q: Why is NVIDIA capturing so much value?</strong><br>
                            A: Three factors create a moat:
                            <ul>
                                <li><strong>CUDA Ecosystem:</strong> 15+ years of software lock-in - all AI tools built for CUDA</li>
                                <li><strong>Performance Lead:</strong> H100/B200 are 2-3x faster than AMD alternatives</li>
                                <li><strong>Supply Constraints:</strong> TSMC capacity limits, 12-18 month lead times</li>
                            </ul>
                            <p><em>Competition emerging: AMD MI300X, Google TPUs, Amazon Trainium, but NVIDIA's lead is substantial</em></p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>9.5 Micro Economics: Cost Optimization for Users</h3>
                        
                        <div class="must-know">
                            <strong>User Perspective:</strong> Whether you're a startup or enterprise, managing AI costs is critical. Here's how to optimize spending at different scales.
                        </div>

                        <h4>Cost Comparison: Self-Hosted vs API</h4>
                        <table>
                            <tr>
                                <th>Factor</th>
                                <th>API (OpenAI/Anthropic)</th>
                                <th>Self-Hosted (Open Models)</th>
                            </tr>
                            <tr>
                                <td><strong>Upfront Cost</strong></td>
                                <td>$0 - Pay as you go</td>
                                <td>$1,500-50K+ (GPUs or cloud)</td>
                            </tr>
                            <tr>
                                <td><strong>Variable Cost</strong></td>
                                <td>$0.15-15/1M tokens</td>
                                <td>Electricity (~$0.10-0.30/kWh)</td>
                            </tr>
                            <tr>
                                <td><strong>Scaling</strong></td>
                                <td>Unlimited (pay more)</td>
                                <td>Limited by hardware</td>
                            </tr>
                            <tr>
                                <td><strong>Latency</strong></td>
                                <td>50-500ms (network)</td>
                                <td>10-100ms (local)</td>
                            </tr>
                            <tr>
                                <td><strong>Privacy</strong></td>
                                <td>Data sent to provider</td>
                                <td>100% on-premises</td>
                            </tr>
                            <tr>
                                <td><strong>Customization</strong></td>
                                <td>Prompt engineering only</td>
                                <td>Full fine-tuning possible</td>
                            </tr>
                            <tr>
                                <td><strong>Break-Even</strong></td>
                                <td>-</td>
                                <td>3-6 months at high volume</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Decision Framework: When to Self-Host</strong>
                            <pre><strong>Use API if:</strong>
âœ… Prototype/MVP stage (minimize upfront cost)
âœ… Variable traffic (don't want idle capacity)
âœ… < 100M tokens/month usage
âœ… Want latest frontier models (GPT-4, Claude)
âœ… Don't have ML/DevOps expertise

<strong>Self-Host if:</strong>
âœ… > 500M tokens/month (cost savings)
âœ… Privacy/compliance requirements (HIPAA, GDPR)
âœ… Need low latency (< 50ms)
âœ… Want fine-tuning / customization
âœ… Predictable, steady traffic
âœ… Have technical team to manage infrastructure

<strong>Hybrid Approach (Best of Both):</strong>
ğŸ¯ Use APIs for spiky/prototype workloads
ğŸ¯ Self-host for predictable, high-volume production
ğŸ¯ Example: API for experimentation, Llama 3 70B self-hosted for core product</pre>
                        </div>

                        <h4>Cost Optimization Tactics</h4>
                        <table>
                            <tr>
                                <th>Tactic</th>
                                <th>Savings</th>
                                <th>Tradeoff</th>
                            </tr>
                            <tr>
                                <td><strong>Use smaller models</strong></td>
                                <td>10-50x cheaper</td>
                                <td>Lower quality for complex tasks</td>
                            </tr>
                            <tr>
                                <td><strong>Prompt caching</strong></td>
                                <td>50-90% on repeated inputs</td>
                                <td>Setup complexity</td>
                            </tr>
                            <tr>
                                <td><strong>Batch processing</strong></td>
                                <td>50% (OpenAI batch API)</td>
                                <td>24h delay</td>
                            </tr>
                            <tr>
                                <td><strong>Output length limits</strong></td>
                                <td>20-40%</td>
                                <td>May truncate responses</td>
                            </tr>
                            <tr>
                                <td><strong>Quantization (INT8/INT4)</strong></td>
                                <td>4-8x memory/cost</td>
                                <td>Slight quality loss</td>
                            </tr>
                            <tr>
                                <td><strong>Model routing</strong></td>
                                <td>30-60%</td>
                                <td>Engineering complexity</td>
                            </tr>
                        </table>

                        <div class="tools-box">
                            <strong>Cost Monitoring & Optimization Tools:</strong>
                            <pre><strong># 1. Token Counting (pre-estimate costs)</strong>
import tiktoken

encoding = tiktoken.encoding_for_model("gpt-4o")
token_count = len(encoding.encode("Your prompt here"))
cost = (token_count / 1_000_000) * 5  # $5 per 1M input tokens

<strong># 2. Prompt Caching (Anthropic Claude)</strong>
from anthropic import Anthropic

client = Anthropic()
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=1024,
    system=[
        {
            "type": "text",
            "text": "Large system prompt...",  # This gets cached
            "cache_control": {"type": "ephemeral"}
        }
    ],
    messages=[{"role": "user", "content": "Query"}]
)
# 90% cost reduction on cached tokens!

<strong># 3. Model Routing (LangChain)</strong>
from langchain.llms import OpenAI
from langchain.chains import LLMChain

def route_model(query_complexity):
    if query_complexity < 3:
        return OpenAI(model="gpt-4o-mini")  # Cheap
    else:
        return OpenAI(model="gpt-4o")  # Expensive

llm = route_model(analyze_complexity(query))
response = llm(query)

<strong># 4. Self-Hosted Cost Tracking</strong>
# Monitor GPU utilization and energy
import pynvml

pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)
power_usage = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000  # Watts
cost_per_hour = (power_usage / 1000) * 0.15  # $0.15/kWh
print(f"Running cost: ${cost_per_hour:.2f}/hour")</pre>
                        </div>

                        <div class="mental-model">
                            <strong>The Cost-Quality-Speed Triangle:</strong>
                            <p>You can optimize any two, but not all three:</p>
                            <ul>
                                <li><strong>Cheap + Fast:</strong> Smaller models (GPT-4o mini, Gemini Flash) - Lower quality</li>
                                <li><strong>Cheap + High Quality:</strong> Self-hosted large models - Slower setup, maintenance</li>
                                <li><strong>Fast + High Quality:</strong> Premium APIs (GPT-4, Claude Opus) - Expensive</li>
                            </ul>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>9.6 Future Outlook: The AI Economics of 2026-2030</h3>
                        
                        <div class="mental-model">
                            <strong>Key Trends Shaping the Future:</strong> As AI matures, economic dynamics will shift from growth-at-all-costs to sustainable profitability and efficiency.
                        </div>

                        <h4>Predicted Shifts</h4>
                        <table>
                            <tr>
                                <th>Metric</th>
                                <th>2024-2025</th>
                                <th>2028-2030 Forecast</th>
                            </tr>
                            <tr>
                                <td><strong>Training Costs</strong></td>
                                <td>$100-500M per model</td>
                                <td>$50-200M (algorithmic efficiency)</td>
                            </tr>
                            <tr>
                                <td><strong>Inference Costs</strong></td>
                                <td>$3-15/1M tokens (flagship)</td>
                                <td>$0.50-3/1M tokens (50-80% reduction)</td>
                            </tr>
                            <tr>
                                <td><strong>GPU Prices</strong></td>
                                <td>H100: $30-40K</td>
                                <td>Next-gen: $20-30K (competition + scale)</td>
                            </tr>
                            <tr>
                                <td><strong>Model Quality</strong></td>
                                <td>GPT-4 level = frontier</td>
                                <td>GPT-4 level = commodity, GPT-5+ = frontier</td>
                            </tr>
                            <tr>
                                <td><strong>Open vs Closed Gap</strong></td>
                                <td>6-12 month lag</td>
                                <td>3-6 month lag (faster iteration)</td>
                            </tr>
                            <tr>
                                <td><strong>Energy Efficiency</strong></td>
                                <td>1-2 tokens/Wh</td>
                                <td>5-10 tokens/Wh (better chips, algorithms)</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>Scenarios for Market Consolidation</strong>
                            <pre><strong>Scenario A: "Platform Winners" (60% probability)</strong>
â€¢ 3-4 major players dominate (OpenAI, Google, Anthropic + Microsoft/Amazon)
â€¢ High barriers to entry (compute, data, talent)
â€¢ Open-source models stay 6-12 months behind
â€¢ Pricing remains stable ($1-5/1M tokens for mid-tier)

<strong>Scenario B: "Commoditization" (30% probability)</strong>
â€¢ Open models catch up fully (Llama 4, Mixtral, etc.)
â€¢ Inference costs drop 90%+ (efficient algorithms, chips)
â€¢ Differentiation shifts to applications, integrations
â€¢ Pricing drops to $0.10-0.50/1M tokens

<strong>Scenario C: "Disruption" (10% probability)</strong>
â€¢ New architecture breakthrough (beyond transformers)
â€¢ 10-100x efficiency gains
â€¢ New entrants leapfrog incumbents
â€¢ Unpredictable market reshuffling</pre>
                        </div>

                        <h4>Investment Areas (2025-2030)</h4>
                        <ul>
                            <li><strong>Inference Optimization:</strong> vLLM, TensorRT-LLM, speculative decoding - every 2x speedup = huge savings</li>
                            <li><strong>Custom Silicon:</strong> Google TPUs, AWS Trainium, Microsoft Maia - reduce NVIDIA dependence</li>
                            <li><strong>Energy Efficiency:</strong> Green data centers, better cooling, renewable energy</li>
                            <li><strong>Model Distillation:</strong> Compress GPT-4 quality into GPT-3.5 size (10x cost savings)</li>
                            <li><strong>Multi-Modal:</strong> Vision, audio, video models - new revenue streams</li>
                            <li><strong>Edge AI:</strong> On-device models (smartphones, IoT) - new market segment</li>
                        </ul>

                        <div class="faq-box">
                            <strong>Q: Will AI become a commodity like cloud storage?</strong><br>
                            A: Partially, but not entirely:
                            <ul>
                                <li><strong>Commoditized:</strong> Basic text generation, simple chat, standard embeddings</li>
                                <li><strong>Differentiated:</strong> Complex reasoning, domain expertise (medical, legal), cutting-edge capabilities</li>
                                <li><strong>Analogy:</strong> Similar to cloud - S3 storage is commodity, but specialized services (ML, analytics) command premium</li>
                            </ul>
                            <p><em>Expect a barbell market: cheap commodity models + expensive specialized/frontier models</em></p>
                        </div>
                    </div>

                    <div class="concept-card">
                        <h3>9.7 Summary: The AI Economic Landscape</h3>
                        
                        <div class="must-know">
                            <strong>Key Takeaways:</strong>
                        </div>

                        <table>
                            <tr>
                                <th>Stakeholder</th>
                                <th>Current State</th>
                                <th>Strategic Focus</th>
                            </tr>
                            <tr>
                                <td><strong>Hyperscalers</strong></td>
                                <td>$200B+/year spending, racing for dominance</td>
                                <td>Scale infrastructure, lock-in users, efficiency</td>
                            </tr>
                            <tr>
                                <td><strong>Model Providers</strong></td>
                                <td>High costs, growing revenue, not yet profitable</td>
                                <td>Enterprise adoption, inference optimization</td>
                            </tr>
                            <tr>
                                <td><strong>Hardware Vendors</strong></td>
                                <td>NVIDIA capturing most value, supply constrained</td>
                                <td>Scale production, defend against competition</td>
                            </tr>
                            <tr>
                                <td><strong>Application Builders</strong></td>
                                <td>High margins, but dependent on model providers</td>
                                <td>Differentiation, cost optimization, self-hosting</td>
                            </tr>
                            <tr>
                                <td><strong>End Users</strong></td>
                                <td>Choosing between API convenience vs self-host savings</td>
                                <td>Optimize cost-quality-speed tradeoffs</td>
                            </tr>
                        </table>

                        <div class="example-box">
                            <strong>The Big Picture:</strong>
                            <ul>
                                <li>ğŸ’° <strong>Massive investments</strong> in AI infrastructure ($200B+/year) drive rapid innovation</li>
                                <li>ğŸ“ˆ <strong>Revenue growth</strong> is strong (50-300%/year) but profitability still 2-3 years out for most</li>
                                <li>âš¡ <strong>Efficiency gains</strong> (2-5x in next 3 years) will make AI economically viable at scale</li>
                                <li>ğŸ† <strong>Winners will be:</strong> Those who achieve lowest cost per quality unit</li>
                                <li>ğŸŒ <strong>Democratization:</strong> Open models and cheaper inference make AI accessible to all</li>
                            </ul>
                        </div>
                    </div>
                </div>
            </main>
        </div>
    </div>

    <div class="back-to-top" id="backToTop" onclick="scrollToTop()">â†‘</div>

    <script>
        // Navigation
        function showSection(sectionId, trigger) {
            // Hide all sections
            document.querySelectorAll('.section').forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            const targetSection = document.getElementById(sectionId);
            if (!targetSection) return;
            targetSection.classList.add('active');
            
            // Resolve click source if provided
            let triggerEl = null;
            if (trigger && trigger.target) {
                triggerEl = trigger.target;
            } else if (trigger && trigger.nodeType === 1) {
                triggerEl = trigger;
            } else {
                const activeEl = document.activeElement;
                if (activeEl && (activeEl.classList?.contains('nav-pill') || activeEl.classList?.contains('sidebar-link'))) {
                    triggerEl = activeEl;
                }
            }

            // Update nav pills
            document.querySelectorAll('.nav-pill').forEach(pill => pill.classList.remove('active'));
            const matchingPill = document.querySelector(`.nav-pill[onclick*="showSection('${sectionId}')"]`);
            (matchingPill || (triggerEl && triggerEl.classList?.contains('nav-pill') ? triggerEl : null))?.classList.add('active');

            // Update sidebar
            document.querySelectorAll('.sidebar-link').forEach(link => link.classList.remove('active'));
            const matchingSidebar = document.querySelector(`.sidebar-link[onclick*="showSection('${sectionId}')"]`);
            (matchingSidebar || (triggerEl && triggerEl.classList?.contains('sidebar-link') ? triggerEl : null))?.classList.add('active');
            
            // Scroll to top
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Subsection navigation - scrolls to a specific subsection within its owning section
        function scrollToSubsection(subsectionId) {
            const element = document.getElementById(subsectionId);
            if (!element) return;

            const owningSection = element.closest('.section');
            if (owningSection?.id) {
                showSection(owningSection.id);
            }

            setTimeout(() => {
                element.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }, 100);
        }

        // Progress bar
        window.addEventListener('scroll', () => {
            const winScroll = document.body.scrollTop || document.documentElement.scrollTop;
            const height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
            const scrolled = (winScroll / height) * 100;
            document.getElementById('progressBar').style.width = scrolled + '%';
            
            // Back to top button
            const backToTop = document.getElementById('backToTop');
            if (winScroll > 300) {
                backToTop.classList.add('visible');
            } else {
                backToTop.classList.remove('visible');
            }
        });

        function scrollToTop() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Collapsible sections
        document.addEventListener('click', (e) => {
            if (e.target.classList.contains('collapsible')) {
                e.target.classList.toggle('expanded');
                const content = e.target.nextElementSibling;
                if (content && content.classList.contains('collapsible-content')) {
                    content.classList.toggle('expanded');
                }
            }
        });

        // Search functionality
        function searchHandbook() {
            const searchInput = document.getElementById('searchInput');
            const searchResults = document.getElementById('searchResults');
            const query = searchInput.value.toLowerCase().trim();
            
            // Clear previous highlights
            document.querySelectorAll('.search-highlight').forEach(el => {
                el.classList.remove('search-highlight');
            });
            
            if (query.length < 2) {
                searchResults.innerHTML = '';
                return;
            }
            
            // Search through all sections
            const sections = document.querySelectorAll('.section');
            let matches = [];
            let totalMatches = 0;
            
            sections.forEach(section => {
                const sectionId = section.id;
                const sectionTitle = section.querySelector('h2')?.textContent || 'Section';
                const content = section.textContent.toLowerCase();
                
                // Count matches in this section
                const regex = new RegExp(query, 'gi');
                const sectionMatches = (content.match(regex) || []).length;
                
                if (sectionMatches > 0) {
                    totalMatches += sectionMatches;
                    matches.push({
                        id: sectionId,
                        title: sectionTitle,
                        count: sectionMatches
                    });
                    
                    // Highlight first few occurrences in visible section
                    if (section.classList.contains('active')) {
                        const headings = section.querySelectorAll('h3, h4, strong');
                        headings.forEach(heading => {
                            if (heading.textContent.toLowerCase().includes(query)) {
                                heading.classList.add('search-highlight');
                            }
                        });
                    }
                }
            });
            
            // Display results
            if (matches.length > 0) {
                let resultsHTML = `<strong style="color: #1e40af;">${totalMatches} matches found in ${matches.length} sections:</strong><br/><div style="margin-top: 8px;">`;
                matches.forEach(match => {
                    resultsHTML += `<div style="margin: 4px 0; padding: 6px 10px; background: rgba(30, 64, 175, 0.05); border-radius: 4px;">
                                     <a href="#" onclick="showSectionById('${match.id}'); return false;" 
                                        style="color: #1e40af; text-decoration: none; font-weight: 500;">
                                        ğŸ“„ ${match.title} <span style="color: #64748b; font-weight: normal;">(${match.count} matches)</span>
                                     </a></div>`;
                });
                resultsHTML += '</div>';
                searchResults.innerHTML = resultsHTML;
            } else {
                searchResults.innerHTML = `<em>No matches found for "${query}"</em>`;
            }
        }
        
        function showSectionById(sectionId) {
            showSection(sectionId);
            
            // Re-run search to highlight in newly visible section
            searchHandbook();
        }

        // Add CSS for search highlighting
        const style = document.createElement('style');
        style.textContent = `
            .search-highlight {
                background: linear-gradient(120deg, #fef3c7 0%, #fde68a 100%);
                color: #1a1a2e !important;
                padding: 2px 4px;
                border-radius: 3px;
                animation: highlightFade 2s ease;
                font-weight: 600;
            }
            @keyframes highlightFade {
                0% { background: #fde047; }
                100% { background: linear-gradient(120deg, #fef3c7 0%, #fde68a 100%); }
            }
        `;
        document.head.appendChild(style);

        // ============================================
        // ANIMATION FUNCTIONS
        // ============================================

        // Animation #1: Matrix Multiplication (2x3 Ã— 3x2 = 2x2)
        function startMatrixMultiplication() {
            // Keep these in sync with the visible matrices in the HTML block
            const matrixA = [[2, 1, 3], [4, 0, 1]];
            const matrixB = [[1, 2], [3, 1], [0, 2]];
            const result = [[0, 0], [0, 0]];
            
            const cellsA = document.querySelectorAll('#matrixA .matrix-cell');
            const cellsB = document.querySelectorAll('#matrixB .matrix-cell');
            const cellsResult = document.querySelectorAll('#matrixResult .matrix-cell');
            const calcDisplay = document.getElementById('calcDisplay');
            
            let step = 0;
            const steps = [
                // Result[0][0] = 2*1 + 1*3 + 3*0 = 5
                { row: 0, col: 0, aIndices: [0,1,2], bIndices: [0,2,4], calc: '2Ã—1 + 1Ã—3 + 3Ã—0 = 5', value: 5 },
                // Result[0][1] = 2*2 + 1*1 + 3*2 = 11
                { row: 0, col: 1, aIndices: [0,1,2], bIndices: [1,3,5], calc: '2Ã—2 + 1Ã—1 + 3Ã—2 = 11', value: 11 },
                // Result[1][0] = 4*1 + 0*3 + 1*0 = 4
                { row: 1, col: 0, aIndices: [3,4,5], bIndices: [0,2,4], calc: '4Ã—1 + 0Ã—3 + 1Ã—0 = 4', value: 4 },
                // Result[1][1] = 4*2 + 0*1 + 1*2 = 10
                { row: 1, col: 1, aIndices: [3,4,5], bIndices: [1,3,5], calc: '4Ã—2 + 0Ã—1 + 1Ã—2 = 10', value: 10 }
            ];
            
            function animate() {
                if (step >= steps.length) {
                    setTimeout(() => {
                        cellsA.forEach(c => c.classList.remove('highlight'));
                        cellsB.forEach(c => c.classList.remove('highlight'));
                        calcDisplay.textContent = 'Matrix multiplication complete!';
                    }, 1000);
                    return;
                }
                
                const current = steps[step];
                
                // Clear previous highlights
                cellsA.forEach(c => c.classList.remove('highlight'));
                cellsB.forEach(c => c.classList.remove('highlight'));
                
                // Highlight current calculation cells
                current.aIndices.forEach(i => cellsA[i].classList.add('highlight'));
                current.bIndices.forEach(i => cellsB[i].classList.add('highlight'));
                
                // Update calculation display
                calcDisplay.textContent = current.calc;
                
                // Update result cell
                const resultIndex = current.row * 2 + current.col;
                cellsResult[resultIndex].textContent = current.value;
                cellsResult[resultIndex].classList.add('highlight');
                
                step++;
                setTimeout(animate, 1500);
            }
            
            // Reset and start
            cellsResult.forEach(c => {
                c.textContent = '?';
                c.classList.remove('highlight');
            });
            calcDisplay.textContent = 'Starting calculation...';
            setTimeout(animate, 500);
        }

        // Animation #2: Embedding Space 3D Scatter Plot
        let embeddingAnimationId = null;
        function animateEmbeddings() {
            const canvas = document.getElementById('embeddingCanvas');
            if (!canvas) return;
            
            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 400;
            
            // Sample word embeddings (3D projected to 2D)
            const words = [
                { word: 'king', x: 450, y: 100, z: 0.8, color: '#3b82f6' },
                { word: 'queen', x: 470, y: 150, z: 0.75, color: '#3b82f6' },
                { word: 'man', x: 200, y: 120, z: 0.6, color: '#10b981' },
                { word: 'woman', x: 220, y: 170, z: 0.55, color: '#10b981' },
                { word: 'prince', x: 430, y: 180, z: 0.7, color: '#3b82f6' },
                { word: 'princess', x: 450, y: 230, z: 0.65, color: '#3b82f6' },
                { word: 'boy', x: 180, y: 200, z: 0.5, color: '#10b981' },
                { word: 'girl', x: 200, y: 250, z: 0.45, color: '#10b981' },
                { word: 'apple', x: 100, y: 300, z: 0.3, color: '#f59e0b' },
                { word: 'orange', x: 130, y: 320, z: 0.35, color: '#f59e0b' },
                { word: 'banana', x: 90, y: 350, z: 0.25, color: '#f59e0b' }
            ];
            
            let rotation = 0;
            
            function draw() {
                ctx.fillStyle = '#1a1a2e';
                ctx.fillRect(0, 0, canvas.width, canvas.height);
                
                // Rotate words slightly
                rotation += 0.005;
                
                // Draw lines between similar words
                ctx.strokeStyle = 'rgba(100, 100, 100, 0.2)';
                ctx.lineWidth = 1;
                ctx.beginPath();
                ctx.moveTo(words[0].x, words[0].y); ctx.lineTo(words[1].x, words[1].y); // king-queen
                ctx.moveTo(words[2].x, words[2].y); ctx.lineTo(words[3].x, words[3].y); // man-woman
                ctx.moveTo(words[8].x, words[8].y); ctx.lineTo(words[9].x, words[9].y); // apple-orange
                ctx.stroke();
                
                // Sort by z-depth for proper layering
                const sorted = [...words].sort((a, b) => a.z - b.z);
                
                // Draw points and labels
                sorted.forEach((w, i) => {
                    const size = 3 + w.z * 5;
                    const offsetX = Math.cos(rotation + i) * 10;
                    const offsetY = Math.sin(rotation + i) * 5;
                    
                    // Draw point
                    ctx.fillStyle = w.color;
                    ctx.beginPath();
                    ctx.arc(w.x + offsetX, w.y + offsetY, size, 0, Math.PI * 2);
                    ctx.fill();
                    
                    // Draw label
                    ctx.fillStyle = '#ffffff';
                    ctx.font = '12px monospace';
                    ctx.fillText(w.word, w.x + offsetX + 8, w.y + offsetY + 4);
                });
                
                // Draw axes
                ctx.strokeStyle = 'rgba(255, 255, 255, 0.3)';
                ctx.lineWidth = 1;
                ctx.beginPath();
                ctx.moveTo(50, 380); ctx.lineTo(550, 380); // X-axis
                ctx.moveTo(50, 380); ctx.lineTo(50, 50);    // Y-axis
                ctx.stroke();
                
                ctx.fillStyle = 'rgba(255, 255, 255, 0.5)';
                ctx.font = '11px monospace';
                ctx.fillText('Gender â†’', 520, 395);
                ctx.fillText('Royalty â†‘', 10, 60);
                
                embeddingAnimationId = requestAnimationFrame(draw);
            }
            
            // Cancel previous animation if exists
            if (embeddingAnimationId) cancelAnimationFrame(embeddingAnimationId);
            draw();
        }

        // Animation #3: Attention Heatmap
        let attentionStep = 0;
        let attentionInterval = null;
        function showAttentionHeatmap() {
            const grid = document.getElementById('attentionGrid');
            if (!grid) return;
            
            // Clear any existing interval
            if (attentionInterval) clearInterval(attentionInterval);
            attentionStep = 0;
            
            const tokens = ['The', 'cat', 'sat', 'on', 'the', 'mat'];
            const attention = [
                [0.8, 0.1, 0.05, 0.02, 0.02, 0.01],
                [0.1, 0.7, 0.15, 0.02, 0.02, 0.01],
                [0.05, 0.65, 0.15, 0.1, 0.03, 0.02],
                [0.03, 0.05, 0.2, 0.5, 0.15, 0.07],
                [0.8, 0.05, 0.03, 0.02, 0.05, 0.05],
                [0.02, 0.1, 0.2, 0.1, 0.05, 0.53]
            ];
            
            // Create compact grid with proper heatmap colors
            let html = '<div style="display: inline-block; background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">';
            html += '<div style="display: grid; grid-template-columns: 60px repeat(6, 50px); gap: 3px; font-size: 11px;">';
            
            // Header row
            html += '<div style="font-weight: bold; text-align: center; color: #1e40af;"></div>';
            tokens.forEach(t => {
                html += `<div style="font-weight: bold; text-align: center; color: #1e40af; padding: 5px;">${t}</div>`;
            });
            
            // Data rows with proper heatmap colors (red=high, yellow=medium, blue=low)
            tokens.forEach((rowToken, i) => {
                html += `<div style="font-weight: bold; text-align: right; padding: 8px 5px; color: #1e40af;">${rowToken}</div>`;
                attention[i].forEach((score, j) => {
                    // Proper heatmap: blue (low) -> green (medium) -> yellow -> red (high)
                    let color;
                    if (score >= 0.6) {
                        // High attention: yellow to red (0.6-1.0)
                        const intensity = (score - 0.6) / 0.4;  // 0 to 1
                        const red = 255;
                        const green = Math.floor(255 * (1 - intensity));
                        const blue = 0;
                        color = `rgb(${red}, ${green}, ${blue})`;
                    } else if (score >= 0.3) {
                        // Medium attention: green to yellow (0.3-0.6)
                        const intensity = (score - 0.3) / 0.3;  // 0 to 1
                        const red = Math.floor(255 * intensity);
                        const green = 200;
                        const blue = 0;
                        color = `rgb(${red}, ${green}, ${blue})`;
                    } else {
                        // Low attention: blue to green (0-0.3)
                        const intensity = score / 0.3;  // 0 to 1
                        const red = 0;
                        const green = Math.floor(150 * intensity);
                        const blue = Math.floor(255 * (1 - intensity * 0.6));
                        color = `rgb(${red}, ${green}, ${blue})`;
                    }
                    html += `<div class="attention-cell" data-row="${i}" data-col="${j}" style="background: ${color}; width: 50px; height: 40px; display: flex; align-items: center; justify-content: center; border-radius: 4px; font-weight: bold; color: ${score > 0.5 ? 'white' : '#1e293b'}; font-size: 10px; text-shadow: ${score > 0.5 ? '0 1px 2px rgba(0,0,0,0.5)' : 'none'}; transition: all 0.3s ease; cursor: pointer;" title="${rowToken} â†’ ${tokens[j]}: ${score.toFixed(2)}">${score.toFixed(2)}</div>`;
                });
            });
            
            html += '</div>';
            html += '<div style="margin-top: 10px; text-align: center; font-size: 11px; color: #64748b;">ğŸ”´ High Attention &nbsp;&nbsp; ğŸŸ¡ Medium &nbsp;&nbsp; ğŸ”µ Low</div>';
            html += '</div>';
            grid.innerHTML = html;
            
            // Animate highlighting rows
            function highlightRow() {
                const cells = document.querySelectorAll('.attention-cell');
                cells.forEach(c => {
                    c.style.transform = 'scale(1)';
                    c.style.boxShadow = 'none';
                });
                
                const row = attentionStep % 6;
                for (let col = 0; col < 6; col++) {
                    const cell = document.querySelector(`.attention-cell[data-row="${row}"][data-col="${col}"]`);
                    if (cell) {
                        cell.style.transform = 'scale(1.15)';
                        cell.style.boxShadow = '0 4px 12px rgba(0,0,0,0.4)';
                    }
                }
                
                attentionStep++;
            }
            
            highlightRow();
            attentionInterval = setInterval(highlightRow, 1500);
        }

        // Animation #4: LoRA Injection
        function animateLoRA() {
            const container = document.getElementById('loraContainer');
            if (!container) return;
            
            container.innerHTML = `
                <div style="text-align: center; margin-bottom: 15px; color: #e2e8f0; font-weight: 600;">
                    <strong>Fine-Tuning with LoRA</strong>
                </div>
                <div style="display: flex; align-items: center; justify-content: center; gap: 20px; flex-wrap: wrap;">
                    <div class="lora-base" style="width: 140px; height: 140px; background: linear-gradient(135deg, #3b82f6, #2563eb); border-radius: 12px; display: flex; align-items: center; justify-content: center; font-weight: bold; color: white; font-size: 14px; text-align: center; box-shadow: 0 4px 16px rgba(59, 130, 246, 0.3);">
                        Base Model<br/><span style="font-size: 18px;">175B</span><br/>params
                    </div>
                    <div style="font-size: 32px; color: #e2e8f0; font-weight: bold;">+</div>
                    <div class="lora-adapter" style="width: 80px; height: 80px; background: linear-gradient(135deg, #f59e0b, #d97706); border-radius: 10px; display: flex; align-items: center; justify-content: center; font-weight: bold; color: white; font-size: 12px; text-align: center; opacity: 0; animation: loraFadeIn 1s 0.5s forwards; box-shadow: 0 4px 16px rgba(245, 158, 11, 0.4);">
                        LoRA<br/><span style="font-size: 16px;">0.5B</span>
                    </div>
                    <div style="font-size: 32px; color: #e2e8f0; font-weight: bold;">=</div>
                    <div class="lora-result" style="width: 140px; height: 140px; background: linear-gradient(135deg, #10b981, #059669); border-radius: 12px; display: flex; align-items: center; justify-content: center; font-weight: bold; color: white; font-size: 13px; text-align: center; opacity: 0; animation: loraFadeIn 1s 1.5s forwards; box-shadow: 0 4px 16px rgba(16, 185, 129, 0.4);">
                        Custom Model<br/><span style="font-size: 18px;">175.5B</span><br/>params
                    </div>
                </div>
                <div style="text-align: center; margin-top: 20px; padding: 10px 20px; background: linear-gradient(135deg, #8b5cf6, #7c3aed); color: white; border-radius: 8px; display: inline-block; font-size: 13px; font-weight: 600;">
                    Only 0.3% of parameters trained!
                </div>
            `;
            
            const styleSheet = document.styleSheets[0];
            styleSheet.insertRule(`
                @keyframes loraFadeIn {
                    from { opacity: 0; transform: scale(0.8); }
                    to { opacity: 1; transform: scale(1); }
                }
            `, styleSheet.cssRules.length);
        }

        // Animation #5: RAG Pipeline Flow
        function animateRAG() {
            const pipeline = document.getElementById('ragPipeline');
            if (!pipeline) return;
            
            const steps = [
                { id: 'ragQuery', label: '1. User Query', content: '"What is LoRA?"', delay: 0 },
                { id: 'ragEmbed', label: '2. Embed Query', content: '[0.23, -0.45, ...]', delay: 1000 },
                { id: 'ragSearch', label: '3. Vector Search', content: 'Pinecone/Chroma', delay: 2000 },
                { id: 'ragDocs', label: '4. Retrieved Docs', content: '3 relevant chunks', delay: 3000 },
                { id: 'ragLLM', label: '5. LLM + Context', content: 'GPT-4 generation', delay: 4000 },
                { id: 'ragResponse', label: '6. Response', content: '"LoRA is..."', delay: 5000 }
            ];
            
            const colors = [
                'linear-gradient(135deg, #3b82f6, #2563eb)',  // Blue
                'linear-gradient(135deg, #8b5cf6, #7c3aed)',  // Purple
                'linear-gradient(135deg, #ec4899, #db2777)',  // Pink
                'linear-gradient(135deg, #f59e0b, #d97706)',  // Orange
                'linear-gradient(135deg, #10b981, #059669)',  // Green
                'linear-gradient(135deg, #06b6d4, #0891b2)'   // Cyan
            ];
            
            pipeline.innerHTML = steps.map((s, idx) => `
                <div id="${s.id}" class="rag-step" style="opacity: 0; background: ${colors[idx]}; color: white; padding: 15px 20px; border-radius: 8px; min-width: 140px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.2); transform: translateY(20px); transition: all 0.5s ease;">
                    <strong style="font-size: 13px;">${s.label}</strong><br/>
                    <span style="font-size: 11px; opacity: 0.9;">${s.content}</span>
                </div>
            `).join('');
            
            steps.forEach((step, i) => {
                setTimeout(() => {
                    const el = document.getElementById(step.id);
                    el.style.opacity = '1';
                    el.style.transform = 'translateY(0)';
                    
                    // Add arrow after each step except last
                    if (i < steps.length - 1) {
                        const arrow = document.createElement('div');
                        arrow.className = 'rag-arrow';
                        arrow.innerHTML = 'â†’';
                        arrow.style.fontSize = '28px';
                        arrow.style.fontWeight = 'bold';
                        arrow.style.color = '#64748b';
                        arrow.style.opacity = '0';
                        arrow.style.transition = 'all 0.3s ease';
                        pipeline.insertBefore(arrow, el.nextSibling);
                        setTimeout(() => {
                            arrow.style.opacity = '1';
                            arrow.style.color = '#06b6d4';
                        }, 200);
                    }
                }, step.delay);
            });
        }

        // Animation #6: Quantization Comparison
        function showQuantization() {
            const container = document.getElementById('quantContainer');
            if (!container) return;
            
            container.innerHTML = `
                <div class="quant-model" style="background: linear-gradient(135deg, #ef4444, #dc2626);">
                    <strong>FP32 Model</strong><br/>
                    <span style="font-size: 24px; font-weight: bold;">28 GB</span><br/>
                    <span style="font-size: 11px;">Full Precision</span>
                </div>
                <div style="font-size: 32px; color: #94a3b8; animation: arrowPulse 2s infinite;">â†’</div>
                <div class="quant-model" style="background: linear-gradient(135deg, #10b981, #059669); animation: quantTransform 2s ease-in-out;">
                    <strong>INT4 Model</strong><br/>
                    <span style="font-size: 24px; font-weight: bold;">3.5 GB</span><br/>
                    <span style="font-size: 11px;">~8Ã— smaller</span>
                </div>
            `;
        }

        // Animation #7: Token Embedding Lookup - Complete Pipeline with Matrix Visualization
        let tokenLookupStep = 0;
        function animateTokenLookup() {
            const container = document.getElementById('tokenLookupContainer');
            if (!container) return;
            
            const examples = [
                { token: 'cat', subwords: ['cat'], id: 2543, vector: '[0.23, -0.45, 0.67, ..., 0.12]', dim: 768 },
                { token: 'running', subwords: ['run', '##ning'], id: '3721+89', vector: '[0.21, -0.43, 0.69, ..., 0.15]', dim: 768 },
                { token: 'king', subwords: ['king'], id: 1829, vector: '[0.56, 0.12, -0.34, ..., 0.78]', dim: 768 }
            ];
            
            function showToken() {
                const current = examples[tokenLookupStep % 3];
                const cleanId = current.id.toString().split('+')[0];
                
                container.innerHTML = `
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 1: Raw Input Text</div>
                        <div style="display: inline-block; padding: 10px 20px; background: linear-gradient(135deg, #3b82f6, #2563eb); color: white; border-radius: 8px; font-size: 16px; font-weight: bold; box-shadow: 0 4px 12px rgba(59, 130, 246, 0.4);">
                            "${current.token}"
                        </div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 2: Tokenization (BPE/WordPiece)</div>
                        <div style="display: inline-flex; gap: 5px; justify-content: center; flex-wrap: wrap;">
                            ${current.subwords.map(sw => `<div style="padding: 6px 14px; background: linear-gradient(135deg, #8b5cf6, #7c3aed); color: white; border-radius: 6px; font-family: monospace; font-size: 13px; font-weight: bold; box-shadow: 0 4px 12px rgba(139, 92, 246, 0.4);">${sw}</div>`).join('')}
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">Break into subwords</div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 3: Vocabulary Lookup â†’ Token ID</div>
                        <div style="display: inline-block; padding: 8px 18px; background: linear-gradient(135deg, #ec4899, #db2777); color: white; border-radius: 6px; font-family: monospace; font-size: 15px; font-weight: bold; box-shadow: 0 4px 12px rgba(236, 72, 153, 0.4);">
                            ID: ${cleanId}
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">Integer from vocabulary (0-50,000)</div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center; margin-bottom: 12px;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 4: Embedding Matrix Lookup</div>
                        <div style="display: inline-block; padding: 10px; background: linear-gradient(135deg, #f59e0b, #d97706); color: white; border-radius: 8px; box-shadow: 0 4px 12px rgba(245, 158, 11, 0.4);">
                            <div style="font-family: monospace; font-size: 11px; text-align: left; line-height: 1.4;">
                                <div style="opacity: 0.6;">Row 0:    [0.12, -0.45, 0.78, ...]</div>
                                <div style="opacity: 0.6;">Row 1:    [0.34,  0.12, -0.67, ...]</div>
                                <div style="opacity: 0.6;">...</div>
                                <div style="background: rgba(255,255,255,0.2); padding: 3px 6px; border-radius: 4px; font-weight: bold;">Row ${cleanId}: [${current.vector}] â† COPY THIS!</div>
                                <div style="opacity: 0.6;">...</div>
                                <div style="opacity: 0.6;">Row 50000: [...]</div>
                            </div>
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">Pre-trained matrix (50K rows Ã— 768 cols)</div>
                    </div>
                    <div style="text-align: center; font-size: 18px; color: #06b6d4; margin: 8px 0;">â†“</div>
                    <div style="text-align: center;">
                        <div style="font-size: 13px; color: #cbd5e1; margin-bottom: 8px; font-weight: 600;">Step 5: Dense Vector (768 Decimals)</div>
                        <div style="display: inline-block; padding: 10px 18px; background: linear-gradient(135deg, #10b981, #059669); color: white; border-radius: 6px; font-family: monospace; font-size: 12px; max-width: 300px; overflow: hidden; text-overflow: ellipsis; box-shadow: 0 4px 12px rgba(16, 185, 129, 0.4);">
                            ${current.vector}
                        </div>
                        <div style="font-size: 10px; color: #94a3b8; margin-top: 4px; font-style: italic;">âœ“ Ready for transformer! (captures word meaning)</div>
                    </div>
                    <div style="margin-top: 12px; padding: 8px 12px; background: rgba(6, 182, 212, 0.1); border-left: 3px solid #06b6d4; border-radius: 4px; text-align: left; font-size: 11px; color: #cbd5e1;">
                        <strong style="color: #06b6d4;">ğŸ’¡ Key Insight:</strong> Token ID ${cleanId} is just an index! The embedding matrix is a <strong>lookup table of pre-trained decimal values</strong>. We simply copy row ${cleanId} to get our vector.
                    </div>
                `;
                
                tokenLookupStep++;
                setTimeout(showToken, 5000);
            }
            
            showToken();
        }

        // Animation #8: Neural Flow with Toggle (Canvas particle system)
        let neuralFlowEnabled = localStorage.getItem('neuralFlowEnabled') !== 'false';
        let neuralFlowAnimationId = null;
        
        function toggleNeuralFlow(enabled) {
            neuralFlowEnabled = enabled;
            localStorage.setItem('neuralFlowEnabled', enabled);
            
            // Update toggle switch visual state
            const toggleSwitch = document.querySelector('.toggle-switch');
            if (toggleSwitch) {
                if (enabled) {
                    toggleSwitch.classList.add('active');
                } else {
                    toggleSwitch.classList.remove('active');
                }
            }
            
            if (enabled) {
                startNeuralFlow();
            } else {
                if (neuralFlowAnimationId) {
                    cancelAnimationFrame(neuralFlowAnimationId);
                    neuralFlowAnimationId = null;
                }
                const canvas = document.getElementById('neuralFlowCanvas');
                if (canvas) {
                    const ctx = canvas.getContext('2d');
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    ctx.fillStyle = '#1a1a2e';
                    ctx.fillRect(0, 0, canvas.width, canvas.height);
                    ctx.fillStyle = '#94a3b8';
                    ctx.font = '14px monospace';
                    ctx.textAlign = 'center';
                    ctx.fillText('Animation Paused', canvas.width / 2, canvas.height / 2);
                }
            }
        }
        
        function startNeuralFlow() {
            const canvas = document.getElementById('neuralFlowCanvas');
            if (!canvas || !neuralFlowEnabled) return;
            
            const ctx = canvas.getContext('2d');
            canvas.width = 600;
            canvas.height = 400;
            
            // Particle system
            const layers = [
                { x: 100, y: 200, nodes: 8 },
                { x: 250, y: 200, nodes: 16 },
                { x: 400, y: 200, nodes: 16 },
                { x: 550, y: 200, nodes: 8 }
            ];
            
            const particles = [];
            for (let i = 0; i < 20; i++) {
                particles.push({
                    x: 50,
                    y: 100 + Math.random() * 200,
                    vx: 1 + Math.random(),
                    vy: (Math.random() - 0.5) * 0.5,
                    life: Math.random()
                });
            }
            
            function draw() {
                if (!neuralFlowEnabled) return;
                
                ctx.fillStyle = 'rgba(26, 26, 46, 0.1)';
                ctx.fillRect(0, 0, canvas.width, canvas.height);
                
                // Draw layer nodes
                layers.forEach((layer, layerIdx) => {
                    const spacing = 300 / layer.nodes;
                    for (let i = 0; i < layer.nodes; i++) {
                        const y = 50 + i * spacing;
                        ctx.fillStyle = `rgba(59, 130, 246, ${0.3 + layerIdx * 0.1})`;
                        ctx.beginPath();
                        ctx.arc(layer.x, y, 4, 0, Math.PI * 2);
                        ctx.fill();
                    }
                });
                
                // Update and draw particles
                particles.forEach(p => {
                    p.x += p.vx;
                    p.y += p.vy;
                    p.life += 0.01;
                    
                    if (p.x > canvas.width) {
                        p.x = 50;
                        p.y = 100 + Math.random() * 200;
                        p.life = 0;
                    }
                    
                    const alpha = Math.sin(p.life * Math.PI) * 0.7;
                    ctx.fillStyle = `rgba(6, 182, 212, ${alpha})`;
                    ctx.beginPath();
                    ctx.arc(p.x, p.y, 2, 0, Math.PI * 2);
                    ctx.fill();
                });
                
                // Draw connections
                ctx.strokeStyle = 'rgba(100, 100, 100, 0.1)';
                ctx.lineWidth = 1;
                for (let i = 0; i < layers.length - 1; i++) {
                    ctx.beginPath();
                    ctx.moveTo(layers[i].x, 200);
                    ctx.lineTo(layers[i + 1].x, 200);
                    ctx.stroke();
                }
                
                neuralFlowAnimationId = requestAnimationFrame(draw);
            }
            
            if (neuralFlowAnimationId) cancelAnimationFrame(neuralFlowAnimationId);
            draw();
        }

        // Animation #9: LangChain Pipeline Flow
        function animateLangChain() {
            const pipeline = document.getElementById('langchainPipeline');
            if (!pipeline) return;
            
            const steps = [
                { id: 'lcPrompt', label: '1. PromptTemplate', content: 'Format input', delay: 0, color: 'linear-gradient(135deg, #3b82f6, #2563eb)' },
                { id: 'lcLoader', label: '2. Document Loader', content: 'Load docs', delay: 800, color: 'linear-gradient(135deg, #8b5cf6, #7c3aed)' },
                { id: 'lcSplit', label: '3. Text Splitter', content: 'Chunk text', delay: 1600, color: 'linear-gradient(135deg, #ec4899, #db2777)' },
                { id: 'lcEmbed', label: '4. Embeddings', content: 'Vectorize', delay: 2400, color: 'linear-gradient(135deg, #f59e0b, #d97706)' },
                { id: 'lcVector', label: '5. VectorStore', content: 'Store & search', delay: 3200, color: 'linear-gradient(135deg, #10b981, #059669)' },
                { id: 'lcRetriever', label: '6. Retriever', content: 'Find relevant', delay: 4000, color: 'linear-gradient(135deg, #06b6d4, #0891b2)' },
                { id: 'lcLLM', label: '7. LLM Chain', content: 'Generate answer', delay: 4800, color: 'linear-gradient(135deg, #8b5cf6, #6d28d9)' },
                { id: 'lcOutput', label: '8. Output Parser', content: 'Format result', delay: 5600, color: 'linear-gradient(135deg, #10b981, #047857)' }
            ];
            
            pipeline.innerHTML = steps.map(s => `
                <div id="${s.id}" style="opacity: 0; background: ${s.color}; color: white; padding: 12px 18px; border-radius: 8px; min-width: 130px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.2); transform: translateY(20px); transition: all 0.5s ease;">
                    <strong style="font-size: 12px;">${s.label}</strong><br/>
                    <span style="font-size: 10px; opacity: 0.9;">${s.content}</span>
                </div>
            `).join('');
            
            steps.forEach((step, i) => {
                setTimeout(() => {
                    const el = document.getElementById(step.id);
                    if (el) {
                        el.style.opacity = '1';
                        el.style.transform = 'translateY(0)';
                    }
                    
                    // Add arrow after each step except last
                    if (i < steps.length - 1) {
                        const arrow = document.createElement('div');
                        arrow.innerHTML = 'â†’';
                        arrow.style.fontSize = '24px';
                        arrow.style.fontWeight = 'bold';
                        arrow.style.color = '#64748b';
                        arrow.style.opacity = '0';
                        arrow.style.transition = 'all 0.3s ease';
                        pipeline.insertBefore(arrow, el.nextSibling);
                        setTimeout(() => {
                            arrow.style.opacity = '1';
                            arrow.style.color = '#06b6d4';
                        }, 200);
                    }
                }, step.delay);
            });
        }

        // Animation #10: W&B Live Experiment Tracking
        function animateWandB() {
            const dashboard = document.getElementById('wandbDashboard');
            if (!dashboard) return;
            
            // Initialize dashboard with 3 experiments
            dashboard.innerHTML = `
                <div style="margin-bottom: 20px;">
                    <h4 style="color: #92400e; margin-bottom: 10px; text-align: center;">Experiment Dashboard: Llama-3-8B Fine-Tuning</h4>
                    <div style="display: flex; gap: 10px; justify-content: center; margin-bottom: 15px;">
                        <div style="padding: 6px 12px; background: rgba(59, 130, 246, 0.2); border-left: 3px solid #3b82f6; border-radius: 4px; font-size: 11px;">
                            <strong>Exp-1:</strong> lr=1e-5, r=8
                        </div>
                        <div style="padding: 6px 12px; background: rgba(16, 185, 129, 0.2); border-left: 3px solid #10b981; border-radius: 4px; font-size: 11px;">
                            <strong>Exp-2:</strong> lr=2e-5, r=16
                        </div>
                        <div style="padding: 6px 12px; background: rgba(245, 158, 11, 0.2); border-left: 3px solid #f59e0b; border-radius: 4px; font-size: 11px;">
                            <strong>Exp-3:</strong> lr=5e-5, r=32
                        </div>
                    </div>
                </div>
                <div style="display: flex; gap: 15px; flex-wrap: wrap; justify-content: center;">
                    <div style="flex: 1; min-width: 250px; background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <div style="font-weight: 600; color: #92400e; margin-bottom: 10px; font-size: 13px;">ğŸ“‰ Training Loss</div>
                        <canvas id="lossChart" width="250" height="150"></canvas>
                        <div id="lossValue" style="margin-top: 8px; text-align: center; font-size: 11px; color: #78716c;">Epoch: 0 / 10</div>
                    </div>
                    <div style="flex: 1; min-width: 250px; background: white; padding: 15px; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
                        <div style="font-weight: 600; color: #92400e; margin-bottom: 10px; font-size: 13px;">ğŸ“Š GPU Usage</div>
                        <canvas id="gpuChart" width="250" height="150"></canvas>
                        <div id="gpuValue" style="margin-top: 8px; text-align: center; font-size: 11px; color: #78716c;">GPU: 0%</div>
                    </div>
                </div>
                <div id="finalSelection" style="margin-top: 20px; padding: 15px; background: rgba(16, 185, 129, 0.1); border-left: 4px solid #10b981; border-radius: 8px; opacity: 0; transition: opacity 0.5s;">
                    <strong style="color: #065f46;">ğŸ† Best Model Selected:</strong> Experiment-2 (lr=2e-5, r=16)<br>
                    <span style="font-size: 12px; color: #047857;">Final Loss: 0.42 | Training Time: 2.1 hrs | Saved to Model Registry âœ“</span>
                </div>
            `;
            
            // Setup canvases
            const lossCanvas = document.getElementById('lossChart');
            const gpuCanvas = document.getElementById('gpuChart');
            const lossCtx = lossCanvas.getContext('2d');
            const gpuCtx = gpuCanvas.getContext('2d');
            
            // Training data simulation
            const epochs = 10;
            const exp1Loss = [2.5, 1.8, 1.4, 1.1, 0.9, 0.75, 0.65, 0.58, 0.52, 0.48];
            const exp2Loss = [2.5, 1.6, 1.1, 0.8, 0.6, 0.5, 0.45, 0.43, 0.42, 0.42]; // Best
            const exp3Loss = [2.5, 1.5, 0.9, 0.7, 0.6, 0.58, 0.57, 0.57, 0.58, 0.59]; // Overfits
            
            const exp1GPU = [75, 78, 80, 82, 81, 79, 78, 77, 76, 75];
            const exp2GPU = [85, 87, 88, 89, 88, 87, 86, 85, 84, 83];
            const exp3GPU = [92, 94, 95, 96, 96, 95, 94, 93, 92, 91];
            
            let currentEpoch = 0;
            
            function drawLossChart(epoch) {
                lossCtx.clearRect(0, 0, 250, 150);
                
                // Draw grid
                lossCtx.strokeStyle = '#e5e7eb';
                lossCtx.lineWidth = 1;
                for (let i = 0; i <= 4; i++) {
                    lossCtx.beginPath();
                    lossCtx.moveTo(0, i * 37.5);
                    lossCtx.lineTo(250, i * 37.5);
                    lossCtx.stroke();
                }
                
                // Draw lines
                const drawLine = (data, color, width) => {
                    lossCtx.strokeStyle = color;
                    lossCtx.lineWidth = width;
                    lossCtx.beginPath();
                    for (let i = 0; i <= epoch; i++) {
                        const x = (i / 9) * 230 + 10;
                        const y = 140 - ((data[i] / 2.5) * 130);
                        if (i === 0) lossCtx.moveTo(x, y);
                        else lossCtx.lineTo(x, y);
                    }
                    lossCtx.stroke();
                    
                    // Draw point at current position
                    if (epoch < 10) {
                        const x = (epoch / 9) * 230 + 10;
                        const y = 140 - ((data[epoch] / 2.5) * 130);
                        lossCtx.fillStyle = color;
                        lossCtx.beginPath();
                        lossCtx.arc(x, y, 4, 0, 2 * Math.PI);
                        lossCtx.fill();
                    }
                };
                
                drawLine(exp1Loss, '#3b82f6', 2);
                drawLine(exp2Loss, '#10b981', 3); // Thicker = winner
                drawLine(exp3Loss, '#f59e0b', 2);
                
                // Labels
                lossCtx.fillStyle = '#92400e';
                lossCtx.font = '10px sans-serif';
                lossCtx.fillText('2.5', 2, 12);
                lossCtx.fillText('0.0', 2, 145);
            }
            
            function drawGPUChart(epoch) {
                gpuCtx.clearRect(0, 0, 250, 150);
                
                // Draw grid
                gpuCtx.strokeStyle = '#e5e7eb';
                gpuCtx.lineWidth = 1;
                for (let i = 0; i <= 4; i++) {
                    gpuCtx.beginPath();
                    gpuCtx.moveTo(0, i * 37.5);
                    gpuCtx.lineTo(250, i * 37.5);
                    gpuCtx.stroke();
                }
                
                // Draw bars
                const drawBars = (data, colors) => {
                    const barWidth = 20;
                    const gap = 5;
                    for (let i = 0; i <= epoch && i < 10; i++) {
                        const x = (i / 9) * 200 + 25;
                        colors.forEach((color, j) => {
                            const height = (data[j][i] / 100) * 140;
                            const barX = x + (j - 1) * (barWidth + gap);
                            gpuCtx.fillStyle = color;
                            gpuCtx.fillRect(barX, 145 - height, barWidth, height);
                        });
                    }
                };
                
                drawBars([exp1GPU, exp2GPU, exp3GPU], ['#3b82f680', '#10b98180', '#f59e0b80']);
                
                // Labels
                gpuCtx.fillStyle = '#92400e';
                gpuCtx.font = '10px sans-serif';
                gpuCtx.fillText('100%', 2, 12);
                gpuCtx.fillText('0%', 2, 145);
            }
            
            // Animate training
            const interval = setInterval(() => {
                if (currentEpoch < epochs) {
                    drawLossChart(currentEpoch);
                    drawGPUChart(currentEpoch);
                    
                    document.getElementById('lossValue').textContent = 
                        `Epoch: ${currentEpoch + 1} / ${epochs} | Exp-2 Loss: ${exp2Loss[currentEpoch].toFixed(2)}`;
                    document.getElementById('gpuValue').textContent = 
                        `GPU: Exp-1: ${exp1GPU[currentEpoch]}% | Exp-2: ${exp2GPU[currentEpoch]}% | Exp-3: ${exp3GPU[currentEpoch]}%`;
                    
                    currentEpoch++;
                } else {
                    clearInterval(interval);
                    // Show final selection
                    setTimeout(() => {
                        document.getElementById('finalSelection').style.opacity = '1';
                    }, 500);
                }
            }, 600);
        }

        // Animation #11: Unsloth Fine-Tuning Speed Race
        function animateUnsloth() {
            const race = document.getElementById('unslothRace');
            if (!race) return;
            
            race.innerHTML = `
                <div style="margin-bottom: 20px;">
                    <h4 style="color: #581c87; margin-bottom: 15px; text-align: center;">Fine-Tuning Llama-3-8B: Speed Comparison</h4>
                </div>
                
                <div style="margin-bottom: 25px;">
                    <div style="margin-bottom: 15px;">
                        <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                            <span style="font-weight: 600; color: #be123c; font-size: 13px;">ğŸ¢ Standard Training (10 hours)</span>
                            <span id="standardTime" style="font-size: 12px; color: #78716c;">0:00 / 10:00</span>
                        </div>
                        <div style="background: #fecdd3; border-radius: 8px; height: 30px; position: relative; overflow: hidden;">
                            <div id="standardBar" style="background: linear-gradient(135deg, #be123c, #e11d48); height: 100%; width: 0%; transition: width 0.3s; display: flex; align-items: center; padding-left: 10px;">
                                <span style="color: white; font-size: 11px; font-weight: 600;"></span>
                            </div>
                        </div>
                        <div style="display: flex; justify-content: space-between; margin-top: 8px; font-size: 11px; color: #57534e;">
                            <div>ğŸ’¾ Memory: <span id="standardMem">120GB</span></div>
                            <div>ğŸ’° Cost: <span id="standardCost">$0</span></div>
                            <div>âš¡ Speed: <span id="standardSpeed">1x</span></div>
                        </div>
                    </div>
                    
                    <div style="margin-bottom: 15px;">
                        <div style="display: flex; justify-content: space-between; margin-bottom: 5px;">
                            <span style="font-weight: 600; color: #7c3aed; font-size: 13px;">âš¡ Unsloth + QLoRA (2 hours)</span>
                            <span id="unslothTime" style="font-size: 12px; color: #78716c;">0:00 / 2:00</span>
                        </div>
                        <div style="background: #e9d5ff; border-radius: 8px; height: 30px; position: relative; overflow: hidden;">
                            <div id="unslothBar" style="background: linear-gradient(135deg, #8b5cf6, #7c3aed); height: 100%; width: 0%; transition: width 0.3s; display: flex; align-items: center; padding-left: 10px;">
                                <span style="color: white; font-size: 11px; font-weight: 600;"></span>
                            </div>
                        </div>
                        <div style="display: flex; justify-content: space-between; margin-top: 8px; font-size: 11px; color: #57534e;">
                            <div>ğŸ’¾ Memory: <span id="unslothMem">16GB</span></div>
                            <div>ğŸ’° Cost: <span id="unslothCost">$0</span></div>
                            <div>âš¡ Speed: <span id="unslothSpeed">5x</span></div>
                        </div>
                    </div>
                </div>
                
                <div style="background: white; padding: 15px; border-radius: 8px; margin-bottom: 15px;">
                    <div style="font-weight: 600; color: #581c87; margin-bottom: 10px; font-size: 13px;">ğŸ“ˆ Training Metrics</div>
                    <div style="display: flex; gap: 20px; flex-wrap: wrap; font-size: 12px;">
                        <div>
                            <div style="color: #78716c;">Loss</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="currentLoss">2.500</div>
                        </div>
                        <div>
                            <div style="color: #78716c;">Samples/sec</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="throughput">0</div>
                        </div>
                        <div>
                            <div style="color: #78716c;">GPU Temp</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="gpuTemp">45Â°C</div>
                        </div>
                        <div>
                            <div style="color: #78716c;">Progress</div>
                            <div style="font-weight: 600; color: #7c3aed;" id="progress">0%</div>
                        </div>
                    </div>
                </div>
                
                <div id="raceWinner" style="padding: 15px; background: rgba(139, 92, 246, 0.1); border-left: 4px solid #7c3aed; border-radius: 8px; opacity: 0; transition: opacity 0.5s;">
                    <strong style="color: #581c87;">ğŸ† Unsloth Wins!</strong> 5x faster, 7.5x less memory, same quality.<br>
                    <span style="font-size: 12px; color: #6b21a8;">Standard: 10 hrs on 8Ã—A100 ($360) | Unsloth: 2 hrs on RTX 4090 ($0) ğŸ’°</span>
                </div>
            `;
            
            // Simulate training race
            let elapsedSec = 0;
            const totalStandard = 600; // 10 hours = 600 units
            const totalUnsloth = 120;  // 2 hours = 120 units
            const updateInterval = 50; // Update every 50ms for smooth animation
            
            const losses = [2.5, 2.1, 1.8, 1.5, 1.2, 1.0, 0.8, 0.65, 0.52, 0.45, 0.42];
            
            const interval = setInterval(() => {
                elapsedSec++;
                
                // Update progress bars
                const standardProgress = Math.min((elapsedSec / totalStandard) * 100, 100);
                const unslothProgress = Math.min((elapsedSec / totalUnsloth) * 100, 100);
                
                document.getElementById('standardBar').style.width = standardProgress + '%';
                document.getElementById('unslothBar').style.width = unslothProgress + '%';
                
                // Update times
                const standardHours = Math.floor((elapsedSec / 60));
                const standardMins = Math.floor((elapsedSec % 60));
                document.getElementById('standardTime').textContent = 
                    `${standardHours}:${standardMins.toString().padStart(2, '0')} / 10:00`;
                
                const unslothMins = Math.floor((elapsedSec / 60) * 5);
                const unslothSecs = Math.floor(((elapsedSec / 60) * 5 % 1) * 60);
                if (unslothMins < 2 || (unslothMins === 2 && unslothSecs === 0)) {
                    document.getElementById('unslothTime').textContent = 
                        `${unslothMins}:${unslothSecs.toString().padStart(2, '0')} / 2:00`;
                } else {
                    document.getElementById('unslothTime').textContent = '2:00 / 2:00 âœ“';
                }
                
                // Update metrics
                const lossIndex = Math.min(Math.floor((elapsedSec / totalUnsloth) * 10), 10);
                document.getElementById('currentLoss').textContent = losses[lossIndex].toFixed(3);
                document.getElementById('throughput').textContent = (250 + Math.random() * 50).toFixed(0);
                document.getElementById('gpuTemp').textContent = (65 + Math.random() * 15).toFixed(0) + 'Â°C';
                document.getElementById('progress').textContent = Math.floor(unslothProgress) + '%';
                
                // Update costs (AWS A100 = $3.60/hr)
                document.getElementById('standardCost').textContent = '$' + ((elapsedSec / 60) * 8 * 3.60).toFixed(2);
                document.getElementById('unslothCost').textContent = '$0.00'; // Local GPU
                
                // Unsloth finishes first
                if (elapsedSec >= totalUnsloth) {
                    document.getElementById('unslothBar').innerHTML = '<span style="color: white; font-size: 11px; font-weight: 600; padding-left: 10px;">âœ“ COMPLETE</span>';
                    document.getElementById('raceWinner').style.opacity = '1';
                }
                
                // Stop when standard finishes
                if (elapsedSec >= totalStandard) {
                    document.getElementById('standardBar').innerHTML = '<span style="color: white; font-size: 11px; font-weight: 600; padding-left: 10px;">âœ“ COMPLETE</span>';
                    clearInterval(interval);
                }
            }, updateInterval);
        }
        
        // Set initial toggle state
        window.addEventListener('DOMContentLoaded', () => {
            const toggle = document.getElementById('neuralFlowToggle');
            const toggleSwitch = document.querySelector('.toggle-switch');
            
            if (toggle) {
                toggle.checked = neuralFlowEnabled;
                
                // Set visual state
                if (toggleSwitch) {
                    if (neuralFlowEnabled) {
                        toggleSwitch.classList.add('active');
                    } else {
                        toggleSwitch.classList.remove('active');
                    }
                }
                
                if (neuralFlowEnabled) {
                    setTimeout(startNeuralFlow, 500);
                }
            }
        });
    </script>
</body>
</html>